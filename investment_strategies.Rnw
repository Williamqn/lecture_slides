% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='scriptsize', fig.width=4, fig.height=4)
options(width=60, dev='pdf')
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
% bbm and bbold packages for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{bbold}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}



% Title page setup
\title[Investment Strategies]{Investment Strategies}
\subtitle{FRE6871 \& FRE7241, Spring 2020}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{\today}



%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Evaluating Manager Skill}


%%%%%%%%%%%%%%%
\subsection{Tests for Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{market timing} skill can be measured by performing a \emph{linear regression} of a strategy's returns against a strategy with perfect \emph{market timing} skill.
      \vskip1ex
      The \emph{Merton-Henriksson} market timing test uses a linear \emph{market timing} term:
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + \gamma \max{(0, R_m - R_f)} + {\varepsilon}
      \end{displaymath}
      Where $R$ are the strategy returns, $R_m$ are the market returns, and $R_f$ are the risk-free returns.
      \vskip1ex
      If the coefficient $\gamma$ is statistically significant, then it's very likely due to \emph{market timing} skill.
      \vskip1ex
      The \emph{market timing} regression is a generalization of the \emph{Capital Asset Pricing Model}.
      \vskip1ex
      The \emph{Treynor-Mazuy} test uses a quadratic term, which makes it more sensitive to the magnitude of returns:
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + \gamma (R_m - R_f)^2 + {\varepsilon}
      \end{displaymath}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ief_vti_timing.png}
    \vspace{-2em}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=6, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 3, 1), oma=c(0, 0, 0, 0))
# Test if IEF can time VTI
re_turns <- na.omit(rutils::etf_env$re_turns[, c("IEF", "VTI")])
vt_i <- re_turns[, "VTI"]
re_turns <- cbind(re_turns, 0.5*(vt_i+abs(vt_i)), vt_i^2)
colnames(re_turns)[3:4] <- c("merton", "treynor")
# Merton-Henriksson test
mod_el <- lm(IEF ~ VTI + merton, data=re_turns); summary(mod_el)
# Treynor-Mazuy test
mod_el <- lm(IEF ~ VTI + treynor, data=re_turns); summary(mod_el)
# Plot scatterplot
with(re_turns, plot.default(x=VTI, y=IEF-mod_el$coefficients[2]*VTI, xlab="VTI", ylab="IEF"))
title(main="Treynor-Mazuy market timing test\n for IEF vs VTI", line=0.5)
# Plot fitted (predicted) response values
with(re_turns, points.default(x=VTI, y=mod_el$fitted.values-mod_el$coefficients[2]*VTI, pch=16, col="red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Identifying Managers With Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Adapt from RFinance\_2017.Rmd and from scratch.R.
      \vskip1ex
      \vskip1ex
      Consider a binary investment (gamble) with the probability of winning equal to $p$, the winning amount (gain) equal to $a$, and the loss equal to $b$.
      \vskip1ex
      The investor makes no up-front payments, and either wins an amount $a$, or loses an amount $b$.
      \vskip1ex
      Assuming that an investor makes decisions exclusively on the basis of the expected value of future wealth, then they would choose to invest all their wealth on the gamble if its expected value is positive, and choose not to invest at all if its expected value is negative.
    \column{0.5\textwidth}
      <<results='asis',echo=FALSE,eval=TRUE>>=
library(xtable)
gambl_e <- data.frame(win=c("p", "a"), lose=c("q = 1 - p", "-b"))
rownames(gambl_e) <- c("probability", "payout")
# print(xtable(gambl_e), comment=FALSE, size="tiny")
print(xtable(gambl_e), comment=FALSE)
      @
      The expected value of the gamble is equal to: $m = p \, a - q \, b$.
      \vskip1ex
      The variance of the gamble is equal to: $var=p \, q \, (a + b)^2$.
      \vskip1ex
      Without loss of generality we can assume that $p=q = \frac{1}{2}$,\\
      $m = 0.5 \, (b - a)$,\\
      $var=0.25 \, (a + b)^2$.
      \vskip1ex
      The \emph{Sharpe ratio} of the gamble is then equal to:
      \begin{displaymath}
        S_r = \frac{m}{sqrt(var)} = \frac{(b - a)}{(a + b)}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Investor Risk Preferences and Portfolio Selection}


%%%%%%%%%%%%%%%
\subsection{Single Period Binary Gamble}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Consider a single investment (gamble) with a binary outcome: \\
      The investor makes no up-front payments, and either wins an amount $a$ (with probability $p$), or loses an amount $b$ (with probability $q = 1-p$).
      <<results='asis',echo=FALSE,eval=TRUE>>=
library(xtable)
gambl_e <- data.frame(win=c("p", "a", "1 + a"), lose=c("q = 1 - p", "-b", "1 - b"))
rownames(gambl_e) <- c("probability", "payout", "terminal wealth")
# print(xtable(gambl_e), comment=FALSE, size="tiny")
print(xtable(gambl_e), comment=FALSE)
      @
      The initial wealth is equal to $1$ dollar, and the terminal wealth after the gamble is either $1 + a$ (with probability $p$), or $1 - b$ (with probability $q = 1-p$).
      \vskip1ex
      The amounts $a$ and $b$ are expressed as percentages of the wealth risked in the gamble, and the ratio $a / b$ is called the \emph{betting odds}.
      \vskip1ex
      The expected value of the gamble (called the \emph{edge}) is equal to: $\mu = p \, a - q \, b$, \\
      and its variance to: $\sigma^2 = p \, q \, (a + b)^2$.
    \column{0.5\textwidth}
      If the investor chooses to risk only a fraction $\kappa$ of wealth, then the wealth after the gamble is either $1 + \kappa a$ (with probability $p$), or $1 - \kappa b$ (with probability $q = 1-p$).
      \vskip1ex
      And the expected value of the wealth after the gamble is equal to: $p \, (1 + \kappa a) + q \, (1 - \kappa b) = 1 + \kappa \, \mu$.
      \vskip1ex
      The fraction $\kappa$ can be greater than $1$ (leveraged investing), or it can be negative (shorting).
      \vskip1ex
      If an investor makes decisions exclusively based on the expected value of wealth, then they would invest all their wealth on the single period gamble, if its expected value $\mu$ is positive, and choose not to invest at all if its expected value is negative.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Multiperiod Binary Gambles}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.9\textwidth}
      The terminal wealth after $n$ repeated gambles with $k$ wins is equal to: $(1 + \kappa a)^k (1 - \kappa b)^{n-k}$.
      \vskip1ex
      And the expected value of the wealth is equal to: 
      \begin{displaymath}
        w(\kappa) = \sum_{k=0}^n \binom{n}{k} p^k q^{n-k} {(1 + \kappa a)^k (1 - \kappa b)^{n-k}}
      \end{displaymath}
      We can then find the fraction $\kappa$ which maximizes the expected wealth $w(\kappa)$:
      \begin{flalign*}
        \frac{\mathrm{d} w(\kappa)}{\mathrm{d} \kappa} = \sum_{k=0}^n \binom{n}{k} p^k q^{n-k} {(1 + \kappa a)^k (1 - \kappa b)^{n-k}} (\frac{a k}{1 + \kappa a} - \frac{b (n-k)}{1 - \kappa b}) = \\
        \frac{a}{1 + \kappa a} \sum_{k=0}^n \binom{n}{k} p^k q^{n-k} {k} - \\
        \sum_{k=0}^n \binom{n}{k} p^k q^{n-k} {(1 + \kappa a)^k (1 - \kappa b)^{n-k}} (\frac{a k}{1 + \kappa a} - \frac{b (n-k)}{1 - \kappa b}) \\
      \end{flalign*}
      If the investor chooses to risk only a fraction $\kappa$ of wealth, then the wealth after the gamble is either $1 + \kappa a$ (with probability $p$), or $1 - \kappa b$ (with probability $q = 1-p$).
      \vskip1ex
      (with probability $p$), or $1 - b$ (with probability $q = 1-p$).
      initial wealth is equal to $1$, and the 
      The \emph{Kelly fraction} for multiperiod betting can be found by maximizing the expected \emph{utility} of the final wealth distribution:
      \begin{flalign*}
        u(\kappa) &= \sum_{k=0}^n \binom{n}{k} p^k q^{n-k} \log((1 + \kappa a)^k (1 - \kappa b)^{n-k}) &\\
        &= \log(1 + \kappa a) \sum_{k=0}^n {\binom{n}{k} p^k q^{n-k} k} + &\\
        & \log(1 - \kappa b) \sum_{k=0}^n {\binom{n}{k} p^k q^{n-k} (n-k)} &\\
        &= n \, p \, \log(1 + \kappa a) + n \, q \, \log(1 - \kappa b)
      \end{flalign*}
      The above is just the single period \emph{utility} multiplied by the number of rounds of betting $n$.
      \vskip1ex
      The \emph{Kelly fraction} $\kappa$ for multiperiod betting is the same as for single period betting:
      \begin{displaymath}
        \kappa = \frac{p}{b}-\frac{q}{a}
      \end{displaymath}
    \column{0.1\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Multiperiod Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In multiperiod betting the investor participates in $n$ rounds of gambles, and in each round they risk a fixed fraction $\kappa$ of their current outstanding wealth.
      \vskip1ex
      In each round the wealth is multiplied by either $(1 + \kappa a)$ (win) or $(1 - \kappa b)$ (loss), so that the current outstanding wealth changes over time.
      \vskip1ex
      The terminal wealth after $n$ rounds with $k$ wins is equal to: $w(\kappa) = (1 + \kappa a)^k (1 - \kappa b)^{n-k}$.
      \vskip1ex
      If the number of rounds $n$ is very large, then the number of wins is almost always equal to $k = n \, p$, and the terminal wealth is equal to: $w(\kappa) = (1 + \kappa a)^{np} (1 - \kappa b)^{nq}$.
    \column{0.5\textwidth}
      Without loss of generality we can assume that $p = q = \frac{1}{2}$.
      \vskip1ex
      And then $\mu = 0.5 \, (a - b)$, and $\sigma^2 = 0.25 \, (a + b)^2$.
      \vskip1ex
      The \emph{Sharpe ratio} of the gamble is then equal to:
      \begin{displaymath}
        S_r = \frac{\mu}{\sigma} = \frac{(a - b)}{(a + b)}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Multiperiod Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The betting fraction $\kappa$ that maximizes the terminal wealth is found by setting the derivative of $w(\kappa)$ to zero:
      \begin{flalign*}
        & \frac{\mathrm{d} w(\kappa)}{\mathrm{d} \kappa} = n p a (1 + \kappa a)^{np-1} (1 - \kappa b)^{nq} & \\
        & - n q b (1 + \kappa a)^{np} (1 - \kappa b)^{nq-1} & \\
        & = (\frac{n p a}{1 + \kappa a} - \frac{n q b}{1 - \kappa b}) (1 + \kappa a)^{np} (1 - \kappa b)^{nq} = 0 &
      \end{flalign*}
      We can then solve for the optimal betting fraction $\kappa$:
      \begin{flalign*}
        \frac{p a}{1 + \kappa a} - \frac{q b}{1 - \kappa b} = 0 \\
        p a (1 - \kappa b) - q b (1 + \kappa a) = 0 \\
        p a - q b - \kappa a b = 0 \\
        \kappa = \frac{p a - q b}{a b} = \frac{p}{b} - \frac{q}{a}
      \end{flalign*}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/kelly_multi.png}
      \vspace{-2em}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Wealth of multiperiod binary betting
weal_th <- function(f, a=0.8, b=0.1, n=1e3, i=150) {
  (1+f*a)^i * (1-f*b)^(n-i)
}  # end weal_th
curve(expr=weal_th, xlim=c(0, 1), 
      xlab="betting fraction", 
      ylab="wealth", main="", lwd=2)
title(main="Wealth of Multiperiod Betting", line=0.1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investor Utility and Fractional Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{expected utility} hypothesis states that investors try to maximize the expected value of the \emph{utility} of wealth, not the expected value of the wealth.
      \vskip1ex
      In 1738 Daniel Bernoulli introduced the concept of \emph{logarithmic utility} in his work \emph{"Specimen Theoriae Novae de Mensura Sortis"} (New Theory of the Measurement of Risk).
      \vskip1ex
      The \emph{logarithmic utility} function is defined as the logarithm of wealth: $u(w) = \log(w)$.
      \vskip1ex
      Under \emph{logarithmic utility} investor preferences depend on the percentage change of wealth, instead of the absolute change of wealth: $\mathrm{d} u(w) = \frac{\mathrm{d}w}{w}$.
      \vskip1ex
      An investor with \emph{logarithmic utility} invests only a fraction $\kappa$ of their wealth in a gamble, depending on the risk-return of the gamble.
      \vskip1ex
      If the initial wealth is equal to $1$, then the expected value of \emph{logarithmic utility} for the binary gamble is equal to: $u(\kappa) = p \, \log(1 + \kappa a) + q \, \log(1 - \kappa b)$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/util_log.png}
      \vspace{-2em}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Define logarithmic utility
utili_ty <- function(frac, p=0.3, a=20, b=1) {
  p*log(1+frac*a) + (1-p)*log(1-frac*b)
}  # end utili_ty
# Plot utility
curve(expr=utili_ty, xlim=c(0, 1), 
      ylim=c(-0.5, 0.4), xlab="betting fraction", 
      ylab="utility", main="", lwd=2)
title(main="Logarithmic Utility", line=0.5)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Fractional Betting Under Logarithmic Utility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The betting fraction that maximizes the \emph{utility} can be found by equating the derivative of \emph{utility} to zero:
      \begin{displaymath}
        \frac{\mathrm{d} u(\kappa)}{\mathrm{d} \kappa} = \frac{p \, a}{1 + \kappa a} - \frac{q \, b}{1 - \kappa b} = 0
      \end{displaymath}
      \begin{displaymath}
        \kappa = \frac{p}{b}-\frac{q}{a} = \frac{p \, a - q \, b}{b \, a} = \frac{\mu}{b \, a}
      \end{displaymath}
      The optimal $\kappa$ is called the \emph{Kelly fraction}, and it depends on the parameters of the gamble.
      \vskip1ex
      The \emph{Kelly fraction} can be greater than $1$ (leveraged investing), or it can be negative (shorting).
      \vskip1ex
      If we assume that $b=1$, then the betting odds are equal to $a$ and the \emph{Kelly fraction} is: $\kappa = \frac{p (a + 1) - 1}{a}$
      \vskip1ex
      The \emph{Kelly fraction} is then equal to the expected payout divided by the betting odds.
      \vskip1ex
      If the expected payout of the gamble is not positive, then an investor with logarithmic utility should not allocate any capital to the gamble.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/kelly_fraction.png}
      \vspace{-2em}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Define and plot Kelly fraction
kelly_frac <- function(a, p=0.5, b=1) {
  p/b - (1-p)/a
}  # end kelly_frac
curve(expr=kelly_frac, xlim=c(0, 5), 
      ylim=c(-2, 1), xlab="betting odds", 
      ylab="kelly fraction", main="", lwd=2)
abline(h=0.5, lwd=2, col="red")
text(x=1.5, y=0.5, pos=3, cex=0.8, labels="max Kelly fraction=0.5")
title(main="Kelly fraction", line=-0.8)
      @
\end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Kelly Criterion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kelly criterion} states that investors should bet the optimal \emph{Kelly fraction} of their capital in a gamble.
      \vskip1ex
      Investors with concave utility functions (for example logarithmic utility) are sensitive to the risk of ruin (losing all their capital).
      \vskip1ex
      Applying the \emph{Kelly criterion} and betting only a fraction of their capital reduces the risk of ruin (but it doesn't eliminate the risk if prices drop suddenly).
      \vskip1ex
      The loss amount $b$ determines the risk of ruin, with larger values of $b$ increasing the risk of ruin.
      \vskip1ex
      Therefore investors will choose a smaller betting fraction $\kappa$ for larger values of $b$.
      \vskip1ex
      This means that even for huge odds in their favor, investors may not choose to invest all their capital, because of the risk of ruin.
      \vskip1ex
      For example, if the betting odds are very large $a \to \infty$, then the \emph{Kelly fraction}: $\kappa = \frac{p}{b}$.
    \column{0.5\textwidth}
    \vspace{-1em}
    \includegraphics[width=0.5\paperwidth]{figure/kelly_fraction_max.png}
    \vspace{-2em}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Plot several Kelly curves
curve(expr=kelly_frac(x, b=1), xlim=c(0, 5), 
      ylim=c(-1, 1.5), xlab="betting odds", 
      ylab="kelly fraction", main="", lwd=2)
abline(h=0.5, lwd=2, col="red")
text(x=1.5, y=0.5, pos=3, cex=0.8, labels="b=1.0; max fraction=0.5")
curve(expr=kelly_frac(x, b=0.5), add=TRUE, main="", lwd=2)
abline(h=1.0, lwd=2, col="red")
text(x=1.5, y=1.0, pos=3, cex=0.8, labels="b=0.5; max fraction=1.0")
title(main="Kelly fraction", line=-0.8)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Utility of Multiperiod Binary Gambles}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.7\textwidth}
      The \emph{Kelly fraction} for multiperiod betting can be found by maximizing the expected \emph{utility} of the final wealth distribution:
      \begin{flalign*}
        u(\kappa) &= \sum_{k=0}^n \binom{n}{k} p^k q^{n-k} \log((1 + \kappa a)^k (1 - \kappa b)^{n-k}) &\\
        &= \log(1 + \kappa a) \sum_{k=0}^n {\binom{n}{k} p^k q^{n-k} k} + &\\
        & \log(1 - \kappa b) \sum_{k=0}^n {\binom{n}{k} p^k q^{n-k} (n-k)} &\\
        &= n \, p \, \log(1 + \kappa a) + n \, q \, \log(1 - \kappa b)
      \end{flalign*}
      The above is just the single period \emph{utility} multiplied by the number of rounds of betting $n$.
      \vskip1ex
      The \emph{Kelly fraction} $\kappa$ for multiperiod betting is the same as for single period betting:
      \begin{displaymath}
        \kappa = \frac{p}{b}-\frac{q}{a}
      \end{displaymath}
    \column{0.3\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investing With Fixed Margin}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $r_i$ be the percentage returns on a \emph{risky asset}, so that the asset price $p_t$ at time $t$ is given by: 
      \begin{displaymath}
        p_t = p_0 \prod_{i=1}^t {(1 + r_i)}
      \end{displaymath}
      The initial investor wealth at time $t=0$ is equal to $1$ dollar, and they also borrow on margin $m$ dollars to invest in the risky asset.
      \vskip1ex
      The investor's \emph{wealth} at time $t$ is equal to (the margin borrowing rate is assumed to be zero): 
      \begin{displaymath}
        w_t = 1 + m \, \frac{p_t - p_0}{p_0}
      \end{displaymath}
      The \emph{leverage} $\kappa$ is equal to the margin debt $m$ divided by the total wealth $w_t$: $\kappa = m / w_t$.
      \vskip1ex
      If the asset price drops then the \emph{leverage} increases, because the margin debt is fixed while the wealth drops.
      \vskip1ex
      If the asset price drops enough so that the wealth reaches zero, then the investment is liquidated and the investor is ruined.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/simu_prices.png}
      \vspace{-2em}
      <<echo=(-(1:5)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
set.seed(1121)  # Reset random number generator
# Simulate asset prices
calc_prices <- function(x) cumprod(1 + rnorm(1e3, sd=0.01))
price_paths <- sapply(1:3, calc_prices)
plot(price_paths[, 1], type="l", lwd=3, 
     main="Simulated Asset Prices", 
     ylim=range(price_paths),
     lty="solid", xlab="time", ylab="price")
lines(price_paths[, 2], col="blue", lwd=3)
lines(price_paths[, 3], col="orange", lwd=3)
abline(h=0.5, col="red", lwd=3)
text(x=200, y=0.5, pos=3, labels="liquidation threshold")
      @
  \end{columns}
\end{block}


\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investing With Fixed Leverage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In order to avoid ruin, the investor may choose to maintain a fixed \emph{leverage ratio} equal to $\kappa$, so that the amount invested in the risky asset is proportional to the \emph{wealth}: $m_t = \kappa \, w_t$.
      \vskip1ex
      This requires buying the \emph{risky asset} when its price increases, and selling it when it drops.
      \vskip1ex
      The return on the risky asset in a single period is equal to: $\kappa \, w_t \, r_t$, so the \emph{terminal wealth} at time $t$ is equal to the compounded returns: 
      \begin{displaymath}
        w_t = (1 + \kappa \, r_1) \ldots (1 + \kappa \, r_t) = \prod_{i=1}^t {(1 + \kappa \, r_i)}
      \end{displaymath}
      The utility of the \emph{terminal wealth} is equal to the sum of the utilities of single periods: 
      \begin{flalign*}
        & \mathbbm{E}[\log{w_t}] = \mathbbm{E}[\log((1 + \kappa \, r_1) \ldots (1 + \kappa \, r_t))] &\\
        & = \sum_{i=1}^t {\mathbbm{E}[\log{(1 + \kappa \, r_i)}]} = t \, \mathbbm{E}[\log{(1 + \kappa \, r)}]
      \end{flalign*}
      The last equality holds because all the utilities of single periods are the same.
    \column{0.5\textwidth}
      Let the returns over a short time period be equal to $r$, with probability distribution $p(r)$.
      \vskip1ex
      The mean return $\bar{r}$, and variance $\sigma^2$ are:
      \begin{displaymath}
        \bar{r} = \int {r \, p(r) \, \mathrm{d}r} \; ; \quad
        \sigma^2 = \int {(r - \bar{r})^2 \, p(r) \, \mathrm{d}r}
      \end{displaymath}
      Since the returns are over a short time period, we have: $r \ll 1$ and $\bar{r} \ll \sigma$, so that we can replace $r - \bar{r}$ with $r$ as follows:
      \begin{displaymath}
        \int {(r - \bar{r})^2 \, p(r) \, \mathrm{d}r} \approx \int {r^2 \, p(r) \, \mathrm{d}r}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Utility of Leveraged Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      So the utility of the \emph{terminal wealth} $u_t$ is equal to the utility of a single period times the number of periods:
      \begin{displaymath}
        u_t = \mathbbm{E}[\log{w_t}] = t \, \mathbbm{E}[\log{(1 + \kappa \, r)}] = t \, u_r
      \end{displaymath}
      The utility of the asset returns $u_r$ is equal to:
      \begin{displaymath}
        u_r = \mathbbm{E}[\log{(1 + \kappa \, r)}] = \int {\log(1 + \kappa \, r) \, p(r) \, \mathrm{d}r}
      \end{displaymath}
      The leverage $\kappa$ is limited so that $(1 + \kappa \, r) > 0$ for all return values $r$.
      \vskip1ex
      If the mean returns are positive, then at first the utility increases with leverage, but only up to a point.
      \vskip1ex
      With higher leverage, the negative utility of time periods with negative returns becomes significant, forcing the aggregate utility to drop.
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)
# Calculate the VTI returns
re_turns <- rutils::etf_env$re_turns[, "VTI"]
re_turns <- na.omit(re_turns)
c(mean=mean(re_turns), std=sd(re_turns))
range(re_turns)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/util_rets.png}
      \vspace{-2em}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Define vectorized logarithmic utility function
utili_ty <- function(kell_y, re_turns) {
  sapply(kell_y, function(x) 
    sum(log(1 + x*re_turns)))
}  # end utili_ty
utili_ty(1, re_turns)
utili_ty(c(1, 4), re_turns)
# Plot the logarithmic utility
curve(expr=utili_ty(x, re_turns=re_turns), 
      xlim=c(0.1, 5), xlab="leverage", ylab="utility", 
      main="Utility of Asset Returns", lwd=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Criterion for Optimal Leverage of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{logarithmic utility}  $u_r$ can be expanded in the moments of the return distribution:
      \begin{flalign*}
        & u_r = \mathbbm{E}[\log{(1 + \kappa \, r)}] = \int {\log(1 + \kappa \, r) \, p(r) \, \mathrm{d}r} & \\
        & = \int {(\kappa \, r - \frac{(\kappa \, r)^2}{2} + \frac{(\kappa \, r)^3}{3} - \frac{(\kappa \, r)^4}{4}) \, p(r) \, \mathrm{d}r} & \\
        & = \kappa \bar{r} - \frac{\kappa^2 \sigma^2}{2} + \frac{\kappa^3 \sigma^3 \hat{s}}{3} - \frac{\kappa^4 \sigma^4 \hat{k}}{4}
      \end{flalign*}
      Where $\hat{s} = \frac{1}{\sigma^3} \int {r^3 \, p(r) \, \mathrm{d}r}$ is the \emph{skewness}, and $\hat{k} = \frac{1}{\sigma^4} \int {r^4 \, p(r) \, \mathrm{d}r}$ is the \emph{kurtosis}.
      \vskip1ex
      The \emph{Kelly leverage} which maximizes the \emph{utility} is found by equating the derivative of \emph{utility} to zero:
      \begin{displaymath}
        \frac{\mathrm{d} u_r}{\mathrm{d}\kappa} = \bar{r} - \kappa \sigma^2 + \kappa^2 \sigma^3 \hat{s} - \kappa^3 \sigma^4 \hat{k} = 0
      \end{displaymath}
    \column{0.5\textwidth}
      Assuming that the third and fourth moments $\sigma^4 \hat{s}$ and $\sigma^4 \hat{k}$ are small and can be neglected, we get:
      \begin{displaymath}
        \kappa = \frac{\bar{r}}{\sigma^2} = \frac{S_r}{\sigma} \; ; \quad u_r = \frac{1}{2} \frac{{\bar{r}}^2}{\sigma^2} = \frac{1}{2} S_r^2
      \end{displaymath}
      The \emph{Kelly leverage} is \emph{approximately} equal to the \emph{Sharpe ratio} divided by the \emph{standard deviation}.
      \vskip1ex
      The optimal utility $u_r$ is \emph{approximately} equal to half the \emph{Sharpe ratio} $S_r$ squared.
      \vskip1ex
      The \emph{standard deviation} and \emph{Sharpe ratio} are calculated over the same time interval as the returns (not annualized).
      <<echo=TRUE,eval=FALSE>>=
# Approximate Kelly leverage
mean(re_turns)/var(re_turns)
PerformanceAnalytics::KellyRatio(R=re_turns, method="full")
# Kelly leverage
unlist(optimize(
  f=function(x) -utili_ty(x, re_turns), 
  interval=c(1, 4)))
      @
    \vspace{-1em}
%    \vspace{-1em}
%    \includegraphics[width=0.5\paperwidth]{figure/kelly_returns-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Strategy Wealth Path}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The wealth of a Kelly Strategy with a fixed leverage ratio $\kappa$ is equal to:
      \begin{displaymath}
        w_t = \prod_{i=1}^t {(1 + \kappa \, r_i)}
      \end{displaymath}
      The \emph{Kelly ratio} $\kappa$ provides the optimal leverage to maximize the utility of wealth, by balancing the benefit of leveraging higher positive returns, with the risk of ruin due to excessive leverage.
      \vskip1ex
      If the mean asset returns are positive, then a higher leverage ratio provides higher returns.
      \vskip1ex
      But if the leverage is too high, then the losses in periods with negative returns wipe out most of the wealth, so then it's slow to recover.
      <<echo=TRUE,eval=FALSE>>=
# Calculate wealth paths
kelly_ratio <- drop(mean(re_turns)/var(re_turns))
kelly_wealth <- cumprod(1 + kelly_ratio*re_turns)
hyper_kelly <- cumprod(1 + (kelly_ratio+2)*re_turns)
sub_kelly <- cumprod(1 + (kelly_ratio-2)*re_turns)
kelly_paths <- cbind(kelly_wealth, hyper_kelly, sub_kelly)
colnames(kelly_paths) <- c("kelly", "hyper-kelly", "sub-kelly")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/kelly_wealth.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot wealth paths
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "orange", "blue")
chart_Series(kelly_paths, theme=plot_theme, 
             name="Wealth Paths")
legend("topleft", legend=colnames(kelly_paths), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Half-Kelly Criterion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In reality investors don't know the probability of winning or the odds of the gamble, so they can't accurately calculate the optimal \emph{Kelly fraction}.
      \vskip1ex
      The \emph{Kelly fraction}: $\kappa = \frac{\bar{r}}{\sigma^2}$ is especially sensitive to the uncertainty of the expected returns $\bar{r}$.
      \vskip1ex
      If the expected returns are over-estimated, then it can produce an inflated value of the \emph{Kelly fraction}, leading to ruin.
      \vskip1ex
      The risk of applying too much leverage (over-betting) is much greater than the risk of applying too little leverage (under-betting).
      \vskip1ex
      Too much leverage (over-betting) not only reduces returns, but it increases the risk of ruin.
      \vskip1ex
      So in practice many investors apply only half the theoretical \emph{Kelly fraction} (the Half-Kelly), to reduce the risk of ruin.
    \column{0.5\textwidth}
      Perform bootstrap simulation to obtain the standard error of the \emph{Kelly fraction}.
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Plot several Kelly curves
curve(expr=kelly_frac(x, b=1), xlim=c(0, 5), 
      ylim=c(-1, 1.5), xlab="betting odds", 
      ylab="kelly fraction", main="", lwd=2)
abline(h=0.5, lwd=2, col="red")
text(x=1.5, y=0.5, pos=3, cex=0.8, labels="b=1.0; max fraction=0.5")
curve(expr=kelly_frac(x, b=0.5), add=TRUE, main="", lwd=2)
abline(h=1.0, lwd=2, col="red")
text(x=1.5, y=1.0, pos=3, cex=0.8, labels="b=0.5; max fraction=1.0")
title(main="Kelly fraction", line=-0.8)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk Aversion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Risk aversion is the investor preference to avoid losses more than to seek similar percentage gains in wealth.
      \vskip1ex
      For example, for a risk averse investor, a 10\% loss of wealth is more important than a 10\% gain.
      \vskip1ex
      Risk aversion is associated with the \emph{diminishing marginal utility} of the percentage change in wealth $\Delta w$.
      \vskip1ex
      This manifests itself as a concave utility function, with a negative second derivative $u''(w) < 0$.
      \vskip1ex
      For example, the \emph{logarithmic utility} function is concave.
      \vskip1ex
      The Arrow-Pratt coefficient of relative risk aversion is proportional to the convexity $u''(w)$ of the utility, and is defined as: $\eta = - \frac{w \, u''(w)}{u'(w)}$.
      \vskip1ex
      The relative risk aversion of \emph{logarithmic utility} is equal to one: $\eta = 1$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/util_log2.png}
      \vspace{-2em}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Plot logarithmic utility function
curve(expr=log, lwd=3, col="blue", xlim=c(0.5, 5), 
      xlab="wealth", ylab="utility", 
      main="Logarithmic Utility")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Constant Relative Risk Aversion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      It's not a given that all investors have a risk aversion coefficient equal to $1$, and other \emph{utility functions} are possible.
      \vskip1ex
      The Constant Relative Risk Aversion (\emph{CRRA}) utility function is a generalization of logarithmic utility:
      \begin{displaymath}
        u(w) = \frac{w^{1 - \eta} - 1}{1 - \eta}
      \end{displaymath}
      Where $\eta$ is the risk aversion parameter.
      \vskip1ex
      The relative risk aversion of the \emph{CRRA} utility function is constant and equal to $\eta$.
      \vskip1ex
      When the risk aversion parameter is equal to one $\eta = 1$, then the \emph{CRRA} utility function is equal to the logarithmic utility.
      \vskip1ex
      In practice, the risk aversion parameter $\eta$ is not known, and must be estimated through empirical studies.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/util_crra.png}
      \vspace{-2em}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Define CRRA utility
cr_ra <- function(w, ra) {
  (w^(1-ra) - 1)/(1-ra)
}  # end cr_ra
# Plot utility functions
curve(expr=cr_ra(x, ra=0.7), xlim=c(0.5, 5), lwd=3, 
      xlab="wealth", ylab="utility", main="", col="blue")
curve(expr=log, add=TRUE, lwd=3)
curve(expr=cr_ra(x, ra=1.3), add=TRUE, lwd=3, col="red")
# Add title and legend
title(main="CRRA Utility", line=0.5)
legend(x="topleft", legend=c("risk seeking", "logarithmic", "risk averse"),
       title="Risk Aversion", inset=0.05, cex=0.8, bg="white",
       lwd=6, lty=1, bty="n", col=c("blue", "black", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Utility of Lottery Tickets}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Lottery tickets are equivalent to binary gambles with a very small probability of winning $p$, but a very large winning amount $a$, and a small loss amount $b$ equal to the ticket price.
      \vskip1ex
      The expected payout $\mu = p \, a - q \, b$ of most lottery tickets is negative.
      \vskip1ex
      So under \emph{logarithmic utility}, the Kelly fraction $\kappa$ for most lottery tickets is also negative, meaning that investors should not be expected to buy these lottery tickets.
      \vskip1ex
      But in reality many people do buy lottery tickets with negative expected payouts, which means that their utility functions are not logarithmic.
      \vskip1ex
      The demand for lottery tickets can be explained by assuming a strong demand for positive \emph{skewness}, which exceeds the demand for a positive payout.
      \vskip1ex
      People buy lottery tickets because they want a small chance of a very large payout, even if the average payout is negative.
    \column{0.5\textwidth}
      Without loss of generality we can assume that the lottery ticket price is one dollar $b = 1$, that it pays out $a$ dollars, and that the expected payout is equal to zero: $\mu = p \, a - q \, b = 0$.
      \vskip1ex
      Then the probabilities of winning and losing are equal to: 
      $p = \frac{1}{a + 1}$ and $q = \frac{a}{a + 1}$.
      \vskip1ex
      The variance is equal to: $\sigma^2 = p \, q \, (a + 1)^2 = a$.
      \vskip1ex
      And the \emph{skewness} is equal to: 
      $\hat{s} = \frac{1}{\sigma^3} (\frac{a^3}{a + 1} - \frac{a}{a + 1}) = \frac{a - 1}{\sqrt{a}}$.
      \vskip1ex
      So the positive \emph{skewness} of a lottery ticket increases as the square root of the \emph{betting odds} $a$, and it can become very large for large \emph{betting odds}.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investor \protect\emph{Risk Aversion}, \protect\emph{Prudence} and \protect\emph{Temperance}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Investor risk and return preferences depend on the signs of the derivatives of their \emph{utility} function.
      \vskip1ex
      Investors with \emph{logarithmic utility} have positive \emph{odd} derivatives ($u'(w) > 0$ and $u'''(w) > 0$) and negative \emph{even} derivatives ($u''(w) < 0$ and $u''''(w) < 0$), which is typical for most other investors as well.
      \vskip1ex
      \emph{Risk averse} investors have a negative second derivative of utility $u''(w) < 0$.
      \vskip1ex
      The demand for lottery tickets shows that investors' utility typically has a positive third derivative $u'''(w) > 0$.
      \vskip1ex
      Positive \emph{odd} derivatives imply a preference for larger \emph{odd moments} of the change of the wealth distribution (mean, skewness).
      \vskip1ex
      Negative \emph{even} derivatives imply a preference for smaller \emph{even moments} (variance, kurtosis).
      \vskip1ex
      The preference for smaller \emph{variance} is called \emph{risk aversion}, for larger \emph{skewness} is called \emph{prudence}, and for smaller \emph{kurtosis} is called \emph{temperance}.
    \column{0.5\textwidth}
      The expected change of the \emph{utility} of wealth $\mathbb{E}[\Delta u(w)]$ can be expanded in the moments of the wealth distribution $\Delta w$:
      \begin{flalign*}
        \mathbb{E}[\Delta u(w)] &= u'(w) \mathbb{E}[\Delta w] + \frac{u''(w)}{2} \sigma^2 &\\
        & + \frac{u'''(w)}{3!} \mu_3 + \frac{u''''(w)}{4!} \mu_3
      \end{flalign*}
      Where $\mathbb{E}[\Delta w]$ is the expected change of wealth, $\sigma^2 = \int {{\Delta w}^2 \, p(w) \, \mathrm{d}w}$ is the \emph{variance} of the change of wealth, and $\mu_3 = \int {{\Delta w}^3 \, p(w) \, \mathrm{d}w} = \sigma^3 \hat{s}$ and $\mu_4 = \int {{\Delta w}^4 \, p(w) \, \mathrm{d}w} = \sigma^4 \hat{k}$ are the third and fourth moments, proportional to the \emph{skewness} $\hat{s}$ and the \emph{kurtosis} $\hat{k}$.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investor Preferences and Empirical Return Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The investor preference for higher \emph{returns} and for lower \emph{volatility} is expressed by maximizing the \emph{Sharpe ratio}.
      \vskip1ex
      The third and fourth moments of asset returns are usually much smaller than the \emph{variance}, so they typically have a smaller effect on the investor risk and return preferences. 
      \vskip1ex
      Nevertheless, there is evidence that investors also have significant preferences for positive \emph{skewness} and lower \emph{kurtosis}.
      \vskip1ex
      But stock returns typically have negative \emph{skewness} and excess \emph{kurtosis}, the opposite of what investors prefer.
      \vskip1ex
      Many investors may prefer positive \emph{skewness}, even at the expense of lower \emph{returns}, similar to the buyers of lottery tickets.
      \vskip1ex
      A paper by Amaya asks if the 
      \href{http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1898735}{\emph{Realized Skewness Predicts the Cross-Section of Equity Returns?}}
      \vskip1ex
      But higher moments are hard to estimate accurately from low frequency (daily) returns, which makes empirical investigations more difficult.
    \column{0.5\textwidth}
      <<echo=TRUE,eval=FALSE>>=
# Calculate higher moments of VTI returns
c(mean=sum(re_turns), 
  variance=sum(re_turns^2),
  mom3=sum(re_turns^3),
  mom4=sum(re_turns^4))/NROW(re_turns)
# Calculate higher moments of minutely SPY returns
re_turns <- HighFreq::SPY[, 4]
re_turns <- na.omit(re_turns)
re_turns <- HighFreq::diff_it(log(re_turns))
c(mean=sum(re_turns), 
  variance=sum(re_turns^2),
  mom3=sum(re_turns^3),
  mom4=sum(re_turns^4))/NROW(re_turns)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Active Investment Strategies}


%%%%%%%%%%%%%%%
\subsection{Static Stock and Bond Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The returns of stocks and bonds are usually negatively correlated, so they are natural hedges for each other.
      \vskip1ex
      Static portfolios consisting of stocks and bonds provide a much better risk versus return tradeoff than either of the assets separately.
      \vskip1ex
      The static weights depend on the investment horizon, with a greater allocation to bonds for a shorter investment horizon.
      \vskip1ex
      Active investment strategies are expected to outperform static stock and bond portfolios. 
      <<echo=(-(1:1)),eval=FALSE>>=
library(HighFreq)  # Load package HighFreq
# Calculate ETF returns
re_turns <- na.omit(
  rutils::etf_env$re_turns[, c("VTI", "IEF")])
re_turns <- cbind(re_turns, 
  0.6*re_turns[, "IEF"]+0.4*re_turns[, "VTI"])
colnames(re_turns)[3] <- "combined"
# Calculate compounded wealth from returns
weal_th <- lapply(re_turns, 
  function(x) cumprod(1 + x))
weal_th <- do.call(cbind, weal_th)
# Plot compounded wealth
dygraphs::dygraph(weal_th, main="Stock and Bond Portfolio") %>% 
  dyOptions(colors=c("green","blue","green")) %>%
  dySeries("combined", color="red", strokeWidth=2) %>%
  dyLegend(show="always")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/stocks_bonds_static.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate correlations
cor(re_turns)
# Calculate Sharpe ratios
sqrt(252)*sapply(re_turns, function(x) mean(x)/sd(x))
# Calculate standard deviations
sapply(re_turns, sd)
# Calculate standardized returns
re_turns <- lapply(re_turns, function(x) {
  x <- (x - mean(x))
  x/sd(x)})
re_turns <- do.call(cbind, re_turns)
sapply(re_turns, sd)
# Calculate skewness and kurtosis
t(sapply(re_turns, function(x) {
  c(skew=mean(x^3), kurt=mean(x^4))
}))
# Or
sapply(c(skew=3, kurt=4), function(x) 
  moments::moment(re_turns, order=x, central=TRUE))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Utility of Stock and Bond Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The utility $u$ of the stock and bond portfolio with weights $w_s, w_b$ is equal to:
      \begin{displaymath}
        u = \sum_{i=1}^n {\log(1 + w_s \, r^s_i + w_b \, r^b_i)}
      \end{displaymath}
      Where $r^s_i, r^b_i$ are the stock and bond returns.
      <<echo=TRUE,eval=FALSE>>=
re_turns <- na.omit(
  rutils::etf_env$re_turns[, c("VTI", "IEF")])
# Logarithmic utility of stock and bond portfolio
utili_ty <- function(w_s, w_b) {
  -sum(log(1 + w_s*re_turns[, "VTI"] + w_b*re_turns[, "IEF"]))
}  # end utili_ty
# Draw 3d surface plot of utility
library(rgl)  # Load rgl
w_s <- seq(from=3, to=8, by=0.2)
w_b <- seq(from=10, to=20, by=0.2)
utility_mat <- sapply(w_b, function(y) sapply(w_s, 
  function(x) utili_ty(x, y)))
rgl::persp3d(w_s, w_b, utility_mat, col="green",
        xlab="stocks", ylab="bonds", zlab="utility")
rgl::rgl.snapshot("utility_surface.png")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/utility_surface.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Optimal Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Kelly optimal stock and bond portfolio weights $w_s, w_b$ can be calculated by maximizing the utility $u$.
      <<echo=TRUE,eval=FALSE>>=
# Approximate Kelly weights
weight_s <- sapply(re_turns, 
      function(x) mean(x)/var(x))
# Kelly weight for stocks
unlist(optimize(f=function(x) 
  utili_ty(x, w_b=0), interval=c(1, 4)))
# Kelly weight for bonds
unlist(optimize(f=function(x) 
  utili_ty(x, w_s=0), interval=c(1, 14)))
# Vectorized utility of stock and bond portfolio
utility_vec <- function(weight_s) {
  utili_ty(weight_s[1], weight_s[2])
}  # end utility_vec
# Optimize with respect to vector argument
op_tim <- optim(fn=utility_vec, par=c(3, 10), 
                method="L-BFGS-B",
                upper=c(8, 20),
                lower=c(2, 5))
# Exact Kelly weights
op_tim$par
      @
    \column{0.5\textwidth}
      The Kelly optimal weights can be calculated approximately by first calculating the individual stock and bond weights, and then multiplying them by the Kelly weight of the combined portfolio.
      <<echo=TRUE,eval=FALSE>>=
# Approximate Kelly weights
p_rets <- (re_turns %*% weight_s)
drop(mean(p_rets)/var(p_rets))*weight_s
# Exact Kelly weights
op_tim$par
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Optimal Stock and Bond Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In practice, the Kelly optimal weights under logarithmic utility are too aggressive and they require very active trading, so half-Kelly or even quarter-Kelly weights are used instead.
      <<echo=TRUE,eval=FALSE>>=
# Quarter-Kelly sub-optimal weights
weight_s <- op_tim$par/4
# Plot Kelly optimal portfolio
re_turns <- cbind(re_turns, 
  weight_s[1]*re_turns[, "VTI"] + 
    weight_s[2]*re_turns[, "IEF"])
colnames(re_turns)[3] <- "Kelly_sub_optimal"
# Calculate compounded wealth from returns
weal_th <- lapply(re_turns, 
  function(x) cumprod(1 + x))
weal_th <- do.call(cbind, weal_th)
# Plot compounded wealth
dygraphs::dygraph(weal_th, main="Stock and Bond Portfolio") %>% 
  dyOptions(colors=c("green","blue","green")) %>%
  dySeries("Kelly_sub_optimal", color="red", strokeWidth=2) %>%
  dyLegend(show="always")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/stocks_bonds_kelly.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Rolling Kelly Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Kelly weights are calculated daily over a rolling look-back interval.
      \vskip1ex
      The distribution of Kelly weights depends on the rolling returns and variance.
      <<echo=TRUE,eval=FALSE>>=
re_turns <- na.omit(
  rutils::etf_env$re_turns[, c("VTI", "IEF")])
# Calculate rolling returns and variance
look_back <- 200
var_rolling <- roll::roll_var(re_turns, width=look_back)
weight_s <- roll::roll_sum(re_turns, width=look_back)/look_back
weight_s <- weight_s/var_rolling
weight_s <- zoo::na.locf(weight_s, fromLast=TRUE)
sum(is.na(weight_s))
range(weight_s)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/kelly_distr.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot the weight_s
x11(width=6, height=4)
par(mar=c(4, 4, 3, 1), oma=c(0, 0, 0, 0))
plot(density(weight_s[, 2]), t="l", lwd=3, col="red", 
     xlab="weights", ylab="density", 
     ylim=c(0, max(density(weight_s[, 1])$y)), 
     main="Kelly Weight Distributions")
lines(density(weight_s[, 1]), t="l", col="blue", lwd=3)
legend("topright", legend=c("VTI", "IEF"), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=c("blue", "red"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Rolling Kelly Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Kelly weights are calculated daily over a rolling look-back interval.
      \vskip1ex
      The distribution of Kelly weights depends on the rolling returns and variance.
      <<echo=TRUE,eval=FALSE>>=
# Scale and lag the Kelly weights
weight_s <- 10*weight_s/sum(abs(range(weight_s)))
weight_s <- HighFreq::lag_it(weight_s)
# Calculate the compounded Kelly wealth and VTI
weal_th <- cbind(cumprod(1 + weight_s[, 1]*re_turns$VTI), 
                 cumprod(1 + re_turns$VTI))
colnames(weal_th) <- c("Kelly Strategy", "VTI")
dygraphs::dygraph(weal_th, main="VTI Strategy Using Rolling Kelly Weight") %>%
  dyAxis("y", label="Kelly Strategy", independentTicks=TRUE) %>%
  dyAxis("y2", label="VTI", independentTicks=TRUE) %>%
  dySeries(name="Kelly Strategy", axis="y", label="Kelly Strategy", strokeWidth=1, col="red") %>%
  dySeries(name="VTI", axis="y2", label="VTI", strokeWidth=1, col="blue")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/kelly_strat.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trading Transaction Costs Under Fixed Leverage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The margin amount $m$ is equal to the dollar amount borrowed to purchase the risky asset.
      \vskip1ex
      If the leverage $\kappa$ is fixed, then the margin amount must change over time together with the wealth: $m = \kappa w_t$.
      \vskip1ex
      The wealth at time $t+1$ is equal to: $w_{t+1} = w_t (1 + \kappa \, r_{i+1})$, so the leverage changes to: $\frac{\kappa}{1 + \kappa \, r_{i+1}}$.
      \vskip1ex
      To maintain fixed leverage the investor must trade an amount of the risky asset equal to:
      \begin{displaymath}
        \kappa - \frac{\kappa}{1 + \kappa \, r_{i+1}} = \frac{\kappa^2 \, r_{i+1}}{1 + \kappa \, r_{i+1}}
      \end{displaymath}
    \column{0.5\textwidth}
      The \emph{bid-offer spread} can be expressed as a percentage, equal to the difference between the \emph{offer} price minus the \emph{bid}, divided by the \emph{mid} price.
      \vskip1ex
      For institutional investors the \emph{bid-offer spread} for stocks is often estimated to be about \texttt{10} basis points (bps).
      \vskip1ex
      In reality the \emph{bid-offer spread} is not static and depends on many factors, such as market liquidity (trading volume), volatility, and time of day.
      \vskip1ex
      Broker commissions are an additional trading cost, but they depend on the size of the trades and on the type of investors, with institutional investors usually enjoying smaller commissions.
      <<echo=TRUE,eval=FALSE>>=
# bid_offer is equal to 10 bps for liquid ETFs
bid_offer <- 0.001
# Calculate open and lagged prices
op_en <- Op(oh_lc)
close_lag <- rutils::lag_it(cl_ose)
pos_lagged <- rutils::lag_it(posi_tion)
# Calculate the transaction cost for one share
cost_s <- 0.0*posi_tion
cost_s[trade_dates] <- 
  0.5*bid_offer*abs(pos_lagged[trade_dates] - 
  posi_tion[trade_dates])*op_en[trade_dates]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Rolling Kelly Portfolio Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a trend following \emph{Moving Average Crossover} strategy, when the current price crosses above the \emph{VWAP}, then the strategy switches its position to long risk, and vice versa.
      \vskip1ex
      A single-period time lag is applied to the \emph{VWAP indicator}, so that the strategy trades immediately after the \emph{VWAP indicator} is evaluated at the end of the day
      \vskip1ex
      This assumption may be too optimistic because in practice it's difficult to trade immediately just before the close of markets.
      <<echo=TRUE,eval=FALSE>>=
look_back <- 200
var_rolling <- roll::roll_var(re_turns, width=look_back)
weight_s <- roll::roll_sum(re_turns, width=look_back)/look_back
weight_s <- weight_s/var_rolling
weight_s <- zoo::na.locf(weight_s, fromLast=TRUE)
sum(is.na(weight_s))
range(weight_s)
# Plot the weight_s
x11(width=6, height=4)
par(mar=c(4, 4, 3, 1), oma=c(0, 0, 0, 0))
plot(density(weight_s[, 2]), t="l", ylim=c(0, max(density(weight_s[, 1])$y)), lwd=3, main="Kelly Weight Distribution")
lines(density(weight_s[, 1]), t="l", col="blue", lwd=3)

# Calculate compounded wealth from returns
weight_s <- HighFreq::lag_it(weight_s)
# weight_s <- 10*weight_s/sum(abs(range(weight_s)))
weight_s <- apply(weight_s, 2, function(x) 10*x/sum(abs(range(x))))

weal_th <- cumprod(1 + rowSums(weight_s*re_turns))
weal_th <- xts(weal_th, index(re_turns))
chart_Series(weal_th, name="VTI Strategy Using Rolling Kelly Weight")


# Calculate VWAP indicator
indica_tor <- sign(cl_ose - v_wap)
# Calculate positions as lagged indicator
posi_tion <- rutils::lag_it(indica_tor)
# Calculate simple dollar VTI returns
re_turns <- rutils::diff_it(cl_ose)
# Calculate daily profits and losses of strategy
pnl_s <- re_turns*posi_tion
cum_pnls <- star_t + cumsum(pnl_s)
# Annualized Sharpe ratio of VWAP strategy
sqrt(252)*sum(pnl_s)/sd(pnl_s)/NROW(pnl_s)
# Annualized Sharpe ratio of VTI
sqrt(252)*sum(re_turns)/sd(re_turns)/NROW(pnl_s)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vwap_strat.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot prices and VWAP
chart_Series(x=cl_ose, name="VWAP Crossover Strategy for VTI", col="orange")
add_TA(cum_pnls, on=1, lwd=2, col="blue")
add_TA(posi_tion > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(posi_tion < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=c("VTI", "VWAP strategy"), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=c("orange", "blue"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Distributions of Terminal Wealth}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Perform a bootsrap to get the Distributions of Terminal Wealth
      \vskip1ex
      In the past, the returns of stocks and bonds have usually been negatively correlated.
      \vskip1ex
      Static portfolios consisting of stocks and bonds provide a much better risk versus return tradeoff than either of the assets separately.
      \vskip1ex
      The static weights depend on the investment horizon, with a greater allocation to bonds for a shorter investment horizon.
      \vskip1ex
      Active investment strategies are expected to outperform static stock and bond portfolios. 
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)  # Load package rutils
# Calculate ETF returns
re_turns <- na.omit(
  rutils::etf_env$re_turns[, c("IEF", "VTI")])
re_turns <- cbind(re_turns, 
  0.6*re_turns[, "IEF"]+0.4*re_turns[, "VTI"])
colnames(re_turns)[3] <- "combined"
# Calculate correlations
cor(re_turns)
# Calculate Sharpe ratios
sqrt(252)*sapply(re_turns, function(x) mean(x)/sd(x))
# Calculate skewness and kurtosis
sapply(re_turns, sd)
# Calculate skewness and kurtosis
t(sapply(c(skew=3, kurt=4), function(x) 
  moments::moment(re_turns, order=x, central=TRUE)))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/stocks_bonds_static.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate prices from returns
price_s <- lapply(re_turns, 
  function(x) exp(cumsum(x)))
price_s <- rutils::do_call(cbind, price_s)
# Plot prices
dygraphs::dygraph(price_s, main="Stock and Bond Portfolio") %>% 
  dyOptions(colors=c("green","blue","green")) %>%
  dySeries("combined", color="red", strokeWidth=2) %>%
  dyLegend(show="always")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: \protect\emph{RSI} Price Technical Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Relative Strength Index} (\emph{RSI}) is defined as the weighted average of prices over a rolling interval:
      \begin{displaymath}
        P_i^{RSI} = (1-\exp(-\lambda)) \sum_{j=0}^{\infty} \exp(-\lambda j) P_{i-j}
      \end{displaymath}
      Where the decay parameter $\lambda$ determines the rate of decay of the \emph{RSI} weights, with larger values of $\lambda$ producing faster decay, giving more weight to recent prices, and vice versa, 
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Get close prices and calculate close-to-close returns
# price_s <- quantmod::Cl(rutils::etf_env$VTI)
price_s <- quantmod::Cl(HighFreq::SPY)
colnames(price_s) <- rutils::get_name(colnames(price_s))
re_turns <- TTR::ROC(price_s)
re_turns[1] <- 0
# Calculate the RSI indicator
r_si <- TTR::RSI(price_s, 2)
# Calculate the long (up) and short (dn) signals
sig_up <- ifelse(r_si < 10, 1, 0)
sig_dn <- ifelse(r_si > 90, -1, 0)
# Lag signals by one period
sig_up <- rutils::lag_it(sig_up, 1)
sig_dn <- rutils::lag_it(sig_dn, 1)
# Replace NA signals with zero position
sig_up[is.na(sig_up)] <- 0
sig_dn[is.na(sig_dn)] <- 0
# Combine up and down signals into one
sig_nals <- sig_up + sig_dn
# Calculate cumulative returns
eq_up <- exp(cumsum(sig_up*re_turns))
eq_dn <- exp(cumsum(-1*sig_dn*re_turns))
eq_all <- exp(cumsum(sig_nals*re_turns))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/rsi_indic.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot daily cumulative returns in panels
end_points <- endpoints(re_turns, on="days")
plot.zoo(cbind(eq_all, eq_up, eq_dn)[end_points], lwd=c(2, 2, 2), 
  ylab=c("Total","Long","Short"), col=c("red","green","blue"),
  main=paste("RSI(2) strategy for", colnames(price_s), "from", 
             format(start(re_turns), "%B %Y"), "to", 
             format(end(re_turns), "%B %Y")))
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Moving Average Technical Indicators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Volume-Weighted Average Price (\emph{VWAP}) is defined as the sum of prices multiplied by trading volumes, divided by the sum of volumes.
      \vskip1ex
      Moving averages (such as \emph{VWAP}) are often used to define technical indicators (trading signals).
      <<echo=TRUE,eval=FALSE>>=
# Calculate open, close, and lagged prices
oh_lc <- rutils::etf_env$VTI
op_en <- quantmod::Op(oh_lc)
cl_ose <- quantmod::Cl(oh_lc)
star_t <- as.numeric(cl_ose[1])
# Define aggregation interval and calculate VWAP
look_back <- 150
v_wap <- HighFreq::roll_vwap(oh_lc, 
              look_back=look_back)
# Plot prices and VWAP
chart_Series(x=cl_ose, 
  name="VTI prices and VWAP", col="orange")
add_TA(v_wap, on=1, lwd=2, col="blue")
legend("top", legend=c("VTI", "VWAP"), 
  bg="white", lty=1, lwd=6, 
  col=c("orange", "blue"), bty="n")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vwap_indic.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Moving Average Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a trend following \emph{Moving Average Crossover} strategy, when the current price crosses above the \emph{VWAP}, then the strategy switches its position to long risk, and vice versa.
      \vskip1ex
      A single-period time lag is applied to the \emph{VWAP indicator}, so that the strategy trades immediately after the \emph{VWAP indicator} is evaluated at the end of the day
      \vskip1ex
      This assumption may be too optimistic because in practice it's difficult to trade immediately just before the close of markets.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VWAP indicator
indica_tor <- sign(cl_ose - v_wap)
# Calculate positions as lagged indicator
posi_tion <- rutils::lag_it(indica_tor)
# Calculate simple dollar VTI returns
re_turns <- rutils::diff_it(cl_ose)
# Calculate daily profits and losses of strategy
pnl_s <- re_turns*posi_tion
cum_pnls <- star_t + cumsum(pnl_s)
# Annualized Sharpe ratio of VWAP strategy
sqrt(252)*sum(pnl_s)/sd(pnl_s)/NROW(pnl_s)
# Annualized Sharpe ratio of VTI
sqrt(252)*sum(re_turns)/sd(re_turns)/NROW(pnl_s)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vwap_strat.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot prices and VWAP
chart_Series(x=cl_ose, name="VWAP Crossover Strategy for VTI", col="orange")
add_TA(cum_pnls, on=1, lwd=2, col="blue")
add_TA(posi_tion > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(posi_tion < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=c("VTI", "VWAP strategy"), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=c("orange", "blue"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trading at the \protect\emph{Open} Price}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A more realistic assumption is that the strategy trades at the \emph{Open} price next period.
      <<echo=TRUE,eval=FALSE>>=
# Determine dates right after VWAP has crossed prices
trade_dates <- (rutils::diff_it(indica_tor) != 0)
trade_dates <- which(trade_dates) + 1
# Calculate positions, either: -1, 0, or 1
posi_tion <- rep(NA_integer_, NROW(oh_lc))
posi_tion[1] <- 0
posi_tion[trade_dates] <- indica_tor[trade_dates-1]
posi_tion <- na.locf(posi_tion)
posi_tion <- xts(posi_tion, order.by=index(oh_lc))
pos_lagged <- rutils::lag_it(posi_tion)
# Calculate pnl for days without trade
pnl_s <- re_turns*posi_tion
# Calculate realized pnl for days with trade
close_lag <- rutils::lag_it(cl_ose)
pnl_s[trade_dates] <- pos_lagged[trade_dates] * 
  (op_en[trade_dates] - close_lag[trade_dates])
# Calculate unrealized pnl for days with trade
pnl_s[trade_dates] <- pnl_s[trade_dates] + 
  posi_tion[trade_dates] * 
  (cl_ose[trade_dates] - op_en[trade_dates])
cum_pnls <- star_t + cumsum(pnl_s)
# Annualized Sharpe ratio of VWAP strategy
sqrt(252)*sum(pnl_s)/sd(pnl_s)/NROW(pnl_s)
# Annualized Sharpe ratio of VTI
sqrt(252)*sum(re_turns)/sd(re_turns)/NROW(pnl_s)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vwap_strat_pnl_open.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot prices and VWAP
chart_Series(x=cl_ose, name="VWAP Crossover Strategy for VTI Trade at Open Price", col="orange")
add_TA(cum_pnls, on=1, lwd=2, col="blue")
add_TA(posi_tion > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(posi_tion < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=c("VTI", "VWAP strategy"), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=c("orange", "blue"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{EWMA} Price Technical Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Exponentially Weighted Moving Average Price} (\emph{EWMA}) is defined as the weighted average of prices over a rolling interval:
      \begin{displaymath}
        P_i^{EWMA} = (1-\exp(-\lambda)) \sum_{j=0}^{\infty} \exp(-\lambda j) P_{i-j}
      \end{displaymath}
      Where the decay parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with larger values of $\lambda$ producing faster decay, giving more weight to recent prices, and vice versa.
      <<echo=TRUE,eval=FALSE>>=
# Define length for weights and decay parameter
wid_th <- 352
lamb_da <- 0.01
# Calculate EWMA prices
weight_s <- exp(-lamb_da*1:wid_th)
weight_s <- weight_s/sum(weight_s)
ew_ma <- stats::filter(cl_ose, filter=weight_s, sides=1)
ew_ma[1:(wid_th-1)] <- ew_ma[wid_th]
ew_ma <- xts(cbind(cl_ose, ew_ma), order.by=index(oh_lc))
colnames(ew_ma) <- c("VTI", "VTI EWMA")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_indic.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA prices with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(ew_ma["2007/2010"], theme=plot_theme, 
             name="EWMA prices")
legend("bottomleft", legend=colnames(ew_ma), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating The \protect\emph{EWMA} Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a trend following \emph{EWMA Crossover} strategy, the risk position switches depending if the current price is above or below the \emph{EWMA}.
      \vskip1ex
      If the current price crosses above the \emph{EWMA}, then the strategy switches its risk position to a fixed unit of long risk, and if it crosses below, to a fixed unit of short risk.
      \vskip1ex
      The strategy holds the same position until the \emph{EWMA} crosses over the current price (either from above or below), and then it switches its position.
      \vskip1ex
      The strategy is therefore always either in a long risk, or in a short risk position.
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Determine dates right after VWAP has crossed prices
indica_tor <- sign(cl_ose - ew_ma[, 2])
trade_dates <- (rutils::diff_it(indica_tor) != 0)
trade_dates <- which(trade_dates) + 1
# Calculate positions, either: -1, 0, or 1
posi_tion <- rep(NA_integer_, NROW(oh_lc))
posi_tion[1] <- 0
posi_tion[trade_dates] <- indica_tor[trade_dates-1]
posi_tion <- na.locf(posi_tion)
posi_tion <- xts(posi_tion, order.by=index(oh_lc))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_strat.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA prices with position shading
chart_Series(ew_ma["2007/2010"], theme=plot_theme, 
             name="EWMA prices")
add_TA(posi_tion > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(posi_tion < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("bottomleft", legend=colnames(ew_ma), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating the Transaction Costs of Trading}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bid-offer spread} can be expressed as a percentage, equal to the difference between the \emph{offer} price minus the \emph{bid}, divided by the \emph{mid} price.
      \vskip1ex
      For institutional investors the \emph{bid-offer spread} for stocks is often estimated to be about \texttt{10} basis points (bps).
      \vskip1ex
      In reality the \emph{bid-offer spread} is not static and depends on many factors, such as market liquidity (trading volume), volatility, and time of day.
      \vskip1ex
      Broker commissions are an additional trading cost, but they depend on the size of the trades and on the type of investors, with institutional investors usually enjoying smaller commissions.
    \column{0.5\textwidth}
      <<echo=TRUE,eval=FALSE>>=
# bid_offer is equal to 10 bps for liquid ETFs
bid_offer <- 0.001
# Calculate open and lagged prices
op_en <- Op(oh_lc)
close_lag <- rutils::lag_it(cl_ose)
pos_lagged <- rutils::lag_it(posi_tion)
# Calculate the transaction cost for one share
cost_s <- 0.0*posi_tion
cost_s[trade_dates] <- 
  0.5*bid_offer*abs(pos_lagged[trade_dates] - 
  posi_tion[trade_dates])*op_en[trade_dates]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of \protect\emph{EWMA} Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The strategy trades at the \emph{Open} price on the next day after prices cross the \emph{EWMA}, since in practice it may not be possible to trade immediately.
      \vskip1ex
      The Profit and Loss (\emph{PnL}) on a trade date is the sum of the realized \emph{PnL} from closing the old position, plus the unrealized \emph{PnL} after opening the new position.
      <<echo=TRUE,eval=FALSE>>=
# Calculate daily profits and losses
# Calculate pnl for days without trade
pnl_s <- re_turns*posi_tion
# Calculate realized pnl for days with trade
close_lag <- rutils::lag_it(cl_ose)
pnl_s[trade_dates] <- pos_lagged[trade_dates] * 
  (op_en[trade_dates] - close_lag[trade_dates])
# Calculate unrealized pnl for days with trade
pnl_s[trade_dates] <- pnl_s[trade_dates] + 
  posi_tion[trade_dates] * 
  (cl_ose[trade_dates] - op_en[trade_dates])
# Annualized Sharpe ratio of EWMA strategy
sqrt(252)*sum(pnl_s)/sd(pnl_s)/NROW(pnl_s)
# Cumulative pnls
pnl_s <- star_t + cumsum(pnl_s)
pnl_s <- cbind(cl_ose, pnl_s)
colnames(pnl_s) <- c("VTI", "EWMA PnL")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_strat_pnl.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA PnL with position shading
chart_Series(pnl_s, theme=plot_theme, 
             name="Performance of EWMA Strategy")
add_TA(posi_tion > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(posi_tion < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=colnames(pnl_s), 
       inset=0.05, bg="white", lty=1, lwd=6, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Function for \protect\emph{EWMA} Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The \emph{EWMA} strategy can be simulated by a single function, which allows the analysis of its performance depending on its parameters.
      \vskip1ex
      The function \texttt{simu\_ewma()} performs a simulation of the \emph{EWMA} strategy, given an \emph{OHLC} time series of prices, and a decay parameter $\lambda$.
      \vskip1ex
      The function \texttt{simu\_ewma()} returns the \emph{EWMA} strategy positions and returns, in a two-column \emph{xts} time series.
    \column{0.6\textwidth}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
simu_ewma <- function(oh_lc, lamb_da=0.01, wid_th=251, bid_offer=0.001, tre_nd=1) {
  # Calculate EWMA prices
  weight_s <- exp(-lamb_da*1:wid_th)
  weight_s <- weight_s/sum(weight_s)
  cl_ose <- quantmod::Cl(oh_lc)
  ew_ma <- stats::filter(as.numeric(cl_ose), filter=weight_s, sides=1)
  ew_ma[1:(wid_th-1)] <- ew_ma[wid_th]
  # Determine dates right after EWMA has crossed prices
  indica_tor <- tre_nd*xts::xts(sign(as.numeric(cl_ose) - ew_ma), order.by=index(oh_lc))
  indicator_lag <- rutils::lag_it(indica_tor)
  trade_dates <- (rutils::diff_it(indica_tor) != 0)
  trade_dates <- which(trade_dates) + 1
  trade_dates <- trade_dates[trade_dates<NROW(oh_lc)]
  # Calculate positions, either: -1, 0, or 1
  posi_tion <- rep(NA_integer_, NROW(oh_lc))
  posi_tion[1] <- 0
  posi_tion[trade_dates] <- indicator_lag[trade_dates]
  posi_tion <- na.locf(posi_tion)
  posi_tion <- xts(posi_tion, order.by=index(oh_lc))
  op_en <- quantmod::Op(oh_lc)
  close_lag <- rutils::lag_it(cl_ose)
  pos_lagged <- rutils::lag_it(posi_tion)
  # Calculate transaction costs
  cost_s <- 0.0*posi_tion
  cost_s[trade_dates] <- 0.5*bid_offer*abs(pos_lagged[trade_dates] - posi_tion[trade_dates])*op_en[trade_dates]
  # Calculate daily profits and losses
  re_turns <- pos_lagged*(cl_ose - close_lag)
  re_turns[trade_dates] <- pos_lagged[trade_dates] * (op_en[trade_dates] - close_lag[trade_dates]) + posi_tion[trade_dates] * (cl_ose[trade_dates] - op_en[trade_dates]) - cost_s
  # Calculate strategy returns
  out_put <- cbind(posi_tion, re_turns)
  colnames(out_put) <- c("positions", "returns")
  out_put
}  # end simu_ewma
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Multiple Trend-following \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{EWMA} strategies can be simulated by calling the function \texttt{simu\_ewma()} in a loop over a vector of $\lambda$ parameters.
      \vskip1ex
      But \texttt{simu\_ewma()} returns an \emph{xts} time series, and \texttt{sapply()} cannot merge \emph{xts} time series together.
      \vskip1ex
      So instead the loop is performed using \texttt{lapply()} which returns a list of \emph{xts}, and the list is merged into a single \emph{xts} using functions \texttt{rutils::do\_call()} and \texttt{cbind()}.
      <<echo=TRUE,eval=FALSE>>=
source("C:/Develop/lecture_slides/scripts/ewma_model.R")
lamb_das <- seq(0.0001, 0.05, 0.005)
# Perform lapply() loop over lamb_das
pnl_s <- lapply(lamb_das, function(lamb_da) {
  # Simulate EWMA strategy and calculate re_turns
  star_t + cumsum(simu_ewma(oh_lc=oh_lc, 
    lamb_da=lamb_da, wid_th=wid_th)[, "returns"])
})  # end lapply
pnl_s <- rutils::do_call(cbind, pnl_s)
colnames(pnl_s) <- paste0("lambda=", lamb_das)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_trend_returns.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA strategies with custom line colors
column_s <- seq(1, NCOL(pnl_s), by=3)
plot_theme <- chart_theme()
plot_theme$col$line.col <- 
  colorRampPalette(c("blue", "red"))(NROW(column_s))
chart_Series(pnl_s[, column_s], 
  theme=plot_theme, name="Cumulative Returns of EWMA Strategies")
legend("topleft", legend=colnames(pnl_s[, column_s]), 
  inset=0.1, bg="white", cex=0.8, lwd=rep(6, NCOL(pnl_s)), 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating \protect\emph{EWMA} Strategies Using Parallel Computing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Simulating \emph{EWMA} strategies naturally lends itself to parallel computing, since the simulations are independent from each other.
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs apply loops under \emph{Windows}, using parallel computing on several CPU cores.
      \vskip1ex
      The resulting list of time series can then be collapsed into a single \emph{xts} series using the functions \texttt{rutils::do\_call()} and \texttt{cbind()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# initialize compute cluster under Windows
library(parallel)
clus_ter <- makeCluster(detectCores()-1)
clusterExport(clus_ter, 
  varlist=c("oh_lc", "wid_th", "simu_ewma"))
# Perform parallel loop over lamb_das under Windows
pnl_s <- parLapply(clus_ter, lamb_das, function(lamb_da) {
  library(quantmod)
  # Simulate EWMA strategy and calculate re_turns
  star_t + cumsum(simu_ewma(oh_lc=oh_lc, 
    lamb_da=lamb_da, wid_th=wid_th)[, "returns"])
})  # end parLapply
# Perform parallel loop over lamb_das under Mac-OSX or Linux
re_turns <- mclapply(lamb_das, function(lamb_da) {
  library(quantmod)
  # Simulate EWMA strategy and calculate re_turns
  star_t + cumsum(simu_ewma(oh_lc=oh_lc, 
    lamb_da=lamb_da, wid_th=wid_th)[, "returns"])
})  # end mclapply
stopCluster(clus_ter)  # Stop R processes over cluster under Windows
pnl_s <- rutils::do_call(cbind, pnl_s)
colnames(pnl_s) <- paste0("lambda=", lamb_das)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of Trend-following \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sharpe ratios} of \emph{EWMA} strategies with different $\lambda$ parameters can be calculated by performing an \texttt{sapply()} loop over the \emph{columns} of returns.
      \vskip1ex
      \texttt{sapply()} treats the columns of \emph{xts} time series as list elements, and loops over the columns.
      \vskip1ex
      Performing loops in \texttt{R} over the \emph{columns} of returns is acceptable, but \texttt{R} loops over the \emph{rows} of returns should be avoided.
      \vskip1ex
      The performance of trend following \emph{EWMA} strategies depends on the $\lambda$ parameter, with larger $\lambda$ parameters performing worse than smaller ones.
      <<echo=TRUE,eval=FALSE>>=
sharpe_ratios <- sqrt(252)*sapply(pnl_s, function(x_ts) {
  # Calculate annualized Sharpe ratio of strategy returns
  x_ts <- rutils::diff_it(log(x_ts))
  sum(x_ts)/sd(x_ts)
})/NROW(pnl_s)  # end sapply
plot(x=lamb_das, y=sharpe_ratios, t="l", 
     main="Performance of EWMA trend following strategies 
     as function of the decay parameter lambda")
trend_returns <- rutils::diff_it(log(pnl_s))
trend_sharpe <- sharpe_ratios
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_trend_performance.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Trend-following \protect\emph{EWMA} Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The best performing trend following \emph{EWMA} strategy has a relatively small $\lambda$ parameter, corresponding to slower weight decay (giving more weight to past prices), and producing less frequent trading.
      <<echo=TRUE,eval=FALSE>>=
# Simulate best performing strategy
ewma_trend <- simu_ewma(oh_lc=oh_lc, 
  lamb_da=lamb_das[which.max(sharpe_ratios)], 
  wid_th=wid_th)
posi_tion <- ewma_trend[, "positions"]
pnl_s <- star_t + cumsum(ewma_trend[, "returns"])
pnl_s <- cbind(cl_ose, pnl_s)
colnames(pnl_s) <- c("VTI", "EWMA PnL")
# Plot EWMA PnL with position shading
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(pnl_s, theme=plot_theme, 
             name="Performance of Trend-following EWMA Strategy")
add_TA(posi_tion > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(posi_tion < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=colnames(pnl_s), 
  inset=0.05, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_trend_best.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Multiple Mean reverting \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{EWMA} strategies can be backtested by calling the function \texttt{simu\_ewma()} in a loop over a vector of $\lambda$ parameters.
      \vskip1ex
      But \texttt{simu\_ewma()} returns an \emph{xts} time series, and \texttt{sapply()} cannot merge \emph{xts} time series together.
      \vskip1ex
      So instead the loop is performed using \texttt{lapply()} which returns a list of \emph{xts}, and the list is merged into a single \emph{xts} using functions \texttt{rutils::do\_call()} and \texttt{cbind()}.
      <<echo=TRUE,eval=FALSE>>=
source("C:/Develop/lecture_slides/scripts/ewma_model.R")
lamb_das <- seq(0.05, 1.0, 0.05)
# Perform lapply() loop over lamb_das
pnl_s <- lapply(lamb_das, function(lamb_da) {
  # backtest EWMA strategy and calculate re_turns
  star_t + cumsum(simu_ewma(
    oh_lc=oh_lc, lamb_da=lamb_da, wid_th=wid_th, tre_nd=(-1))[, "returns"])
})  # end lapply
pnl_s <- rutils::do_call(cbind, pnl_s)
colnames(pnl_s) <- paste0("lambda=", lamb_das)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_revert_returns.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA strategies with custom line colors
column_s <- seq(1, NCOL(pnl_s), by=4)
plot_theme <- chart_theme()
plot_theme$col$line.col <- 
  colorRampPalette(c("blue", "red"))(NROW(column_s))
chart_Series(pnl_s[, column_s], 
  theme=plot_theme, name="Cumulative Returns of Mean reverting EWMA Strategies")
legend("topleft", legend=colnames(pnl_s[, column_s]), 
  inset=0.1, bg="white", cex=0.8, lwd=rep(6, NCOL(pnl_s)), 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of Mean reverting \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sharpe ratios} of \emph{EWMA} strategies with different $\lambda$ parameters can be calculated by performing an \texttt{sapply()} loop over the \emph{columns} of returns.
      \vskip1ex
      \texttt{sapply()} treats the columns of \emph{xts} time series as list elements, and loops over the columns.
      \vskip1ex
      Performing loops in \texttt{R} over the \emph{columns} of returns is acceptable, but \texttt{R} loops over the \emph{rows} of returns should be avoided.
      \vskip1ex
      The performance of mean reverting \emph{EWMA} strategies depends on the $\lambda$ parameter, with performance decreasing for very small or very large $\lambda$ parameters.
      \vskip1ex
      For too large $\lambda$ parameters, the trading frequency is too high, causing high transaction costs.
      \vskip1ex
      For too small $\lambda$ parameters, the trading frequency is too low, causing the strategy to miss profitable trades.
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_revert_performance.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
sharpe_ratios <- sqrt(252)*sapply(pnl_s, function(x_ts) {
  # Calculate annualized Sharpe ratio of strategy returns
  x_ts <- rutils::diff_it(log(x_ts))
  sum(x_ts)/sd(x_ts)
})/NROW(pnl_s)  # end sapply
plot(x=lamb_das, y=sharpe_ratios, t="l", 
     main="Performance of EWMA mean reverting strategies 
     as function of the decay parameter lambda")
revert_returns <- rutils::diff_it(log(pnl_s))
revert_sharpe <- sharpe_ratios
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Mean reverting \protect\emph{EWMA} Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Reverting the rules of the trend following \emph{EWMA} strategy creates a mean reverting strategy.
      \vskip1ex
      The best performing mean reverting \emph{EWMA} strategy has a relatively large $\lambda$ parameter, corresponding to faster weight decay (giving more weight to recent prices), and producing more frequent trading.
      \vskip1ex
      But a too large $\lambda$ parameter also causes very high trading frequency, and high transaction costs.
      <<echo=TRUE,eval=FALSE>>=
# backtest best performing strategy
ewma_revert <- simu_ewma(oh_lc=oh_lc, 
  lamb_da=lamb_das[which.max(sharpe_ratios)],
  wid_th=wid_th, tre_nd=(-1))
posi_tion <- ewma_revert[, "positions"]
pnl_s <- star_t + cumsum(ewma_revert[, "returns"])
pnl_s <- cbind(cl_ose, pnl_s)
colnames(pnl_s) <- c("VTI", "EWMA PnL")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_revert_best.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA PnL with position shading
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(pnl_s, theme=plot_theme, 
             name="Performance of Mean reverting EWMA Strategy")
add_TA(posi_tion > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(posi_tion < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=colnames(pnl_s), 
  inset=0.05, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Combining Trend-following and Mean reverting Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The returns of trend following and mean reverting strategies are usually negatively correlated to each other, so combining them can achieve significant diversification of risk.
      <<echo=TRUE,eval=FALSE>>=
# Calculate correlation between trend following and mean reverting strategies
trend_ing <- ewma_trend[, "returns"]
colnames(trend_ing) <- "trend"
revert_ing <- ewma_revert[, "returns"]
colnames(revert_ing) <- "revert"
close_rets <- rutils::diff_it(log(cl_ose))
cor(cbind(trend_ing, revert_ing, close_rets))
# Calculate combined strategy
com_bined <- trend_ing + revert_ing
colnames(com_bined) <- "combined"
# Calculate annualized Sharpe ratio of strategy returns
re_turns <- cbind(close_rets, trend_ing, revert_ing, com_bined)
sqrt(252)*sapply(re_turns, function(x_ts) 
  sum(x_ts)/sd(x_ts))/NROW(com_bined)
pnl_s <- lapply(re_turns, function(x_ts) {star_t + cumsum(x_ts)})
pnl_s <- rutils::do_call(cbind, pnl_s)
colnames(pnl_s) <- c("VTI", "trending", "reverting", "EWMA combined PnL")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_combined.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue", "green", "magenta2")
chart_Series(pnl_s, theme=plot_theme, 
             name="Performance of Combined EWMA Strategies")
legend("topleft", legend=colnames(pnl_s),
       inset=0.05, bg="white", lty=1, lwd=6, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Ensemble of \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Instead of selecting the best performing \emph{EWMA} strategy, one can choose a weighted average of strategies (ensemble), which corresponds to allocating positions according to the weights.
      \vskip1ex
      The weights can be chosen to be proportional to the Sharpe ratios of the \emph{EWMA} strategies.
      <<echo=TRUE,eval=FALSE>>=
sharpe_ratios <- c(trend_sharpe, revert_sharpe)
weight_s <- sharpe_ratios
weight_s[weight_s<0] <- 0
weight_s <- weight_s/sum(weight_s)
re_turns <- cbind(trend_returns, revert_returns)
avg_returns <- re_turns %*% weight_s
avg_returns <- xts(avg_returns, order.by=index(re_turns))
pnl_s <- (star_t + cumsum(avg_returns))
pnl_s <- cbind(cl_ose, pnl_s)
colnames(pnl_s) <- c("VTI", "EWMA PnL")
# Plot EWMA PnL without position shading
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(pnl_s, theme=plot_theme, 
  name="Performance of Ensemble EWMA Strategy")
legend("top", legend=colnames(pnl_s), 
  inset=0.05, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_ensemble.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Backtesting Active Investment Strategies}


%%%%%%%%%%%%%%%
\subsection{Aggregations Over Look-back and Look-forward Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An example of data aggregations are the past cumulative returns of the \emph{EWMA} strategies calculated over a vector of \emph{end points}.
      \vskip1ex
      Overlapping aggregations can be specified by a vector of \emph{look-back} intervals attached at \emph{end points}.
      \vskip1ex
      For example, we may specify aggregations at monthly \emph{end points}, over overlapping 12-month \emph{look-back} intervals.
      \vskip1ex
      The variable \texttt{look\_back} is equal to the number of \emph{end points} in the \emph{look-back} interval.
      \vskip1ex
      The \emph{start points} are the \emph{end points} lagged by the length of the \emph{look-back} interval.
      \vskip1ex
      The \emph{look-back} intervals are spanned by the vectors of \emph{start points} and \emph{end points}.
      \vskip1ex
      Non-overlapping aggregations can also be calculated over a list of \emph{look-forward} intervals (\texttt{look\_fwds}).
      \vskip1ex
      The \emph{look-back} intervals should not overlap with the \emph{look-forward} intervals, in order to avoid data snooping.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define end of month end_points
end_points <- rutils::calc_endpoints(re_turns, 
                inter_val="months")
n_rows <- NROW(end_points)
# Start_points equal end_points lagged by 12-month look-back interval
look_back <- 12
start_points <- c(rep_len(1, look_back-1), 
  end_points[1:(n_rows-look_back+1)])
# Calculate past performance over end_points 
perform_ance <-
  function(re_turns) sum(re_turns)/sd(re_turns)
past_perf <- sapply(1:(n_rows-1), function(it_er) {
  sapply(re_turns[start_points[it_er]:end_points[it_er]], perform_ance)
})  # end sapply
past_perf <- t(past_perf)
fut_rets <- sapply(1:(n_rows-1), function(it_er) {
  sapply(re_turns[(end_points[it_er]+1):end_points[it_er+1]], sum)
})  # end sapply
fut_rets <- t(fut_rets)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Momentum Portfolio Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In \emph{momentum} strategies, the portfolio weights are adjusted over time to be proportional to the past performance of the assets.
      \vskip1ex
      This way \emph{momentum} strategies switch their weights to the best performing assets.
      \vskip1ex
      The weights are scaled to limit the portfolio \emph{leverage} and its market \emph{beta}.
      \vskip1ex
      The portfolio weights of \emph{momentum} strategies can be scaled in several different ways.
      \vskip1ex
      To limit the portfolio leverage, the weights can be scaled so that the sum of their absolute values (or their squares) is equal to $1$: $\sum_{i=1}^n {w_i^2} = 1$
      \vskip1ex
      The weights can also be de-meaned, so that their sum is equal to zero, to create long-short portfolios with small betas.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate weight_s proportional to past_perf
weight_s <- past_perf
weight_s[weight_s<0] <- 0
# Scale weight_s so their sum is equal to 1
weight_s <- weight_s/rowSums(weight_s)
# Set NA values to zero
weight_s[is.na(weight_s)] <- 0
sum(is.na(weight_s))


# Calculate ETF prices and simple returns
sym_bols <- c("VTI", "IEF", "DBC")
price_s <- rutils::etf_env$price_s[, sym_bols]
price_s <- zoo::na.locf(price_s)
price_s <- na.omit(price_s)
re_turns <- rutils::diff_it(price_s)
# Define look-back and look-forward intervals
end_points <- rutils::calc_endpoints(re_turns, 
  inter_val="months")
n_cols <- NCOL(re_turns)
n_rows <- NROW(end_points)
look_back <- 12
start_points <- c(rep_len(1, look_back-1), 
  end_points[1:(n_rows-look_back+1)])
# Calculate past performance over end_points
perform_ance <- 
  function(re_turns) sum(re_turns)/sd(re_turns)
agg_s <- sapply(1:(n_rows-1), function(it_er) {
  c(past_perf=sapply(re_turns[start_points[it_er]:end_points[it_er]], perform_ance),
    fut_rets=sapply(re_turns[(end_points[it_er]+1):end_points[it_er+1]], sum))
})  # end sapply
agg_s <- t(agg_s)
# Select look-back and look-forward aggregations
past_perf <- agg_s[, 1:n_cols]
fut_rets <- agg_s[, n_cols+1:n_cols]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In \emph{momentum} strategies, the portfolio weights are adjusted over time to be proportional to the past performance of the assets.
      \vskip1ex
      This way \emph{momentum} strategies switch their weights to the best performing assets.
      \vskip1ex
      The weights are scaled to limit the portfolio \emph{leverage} and its market \emph{beta}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate weight_s proportional to past_perf
weight_s <- past_perf
weight_s[weight_s<0] <- 0
# Scale weight_s so their sum is equal to 1
weight_s <- weight_s/rowSums(weight_s)
# Set NA values to zero
weight_s[is.na(weight_s)] <- 0
sum(is.na(weight_s))
in_dex <- index(re_turns[end_points[-n_rows]])
trend_weights <- rowMeans(weight_s[, 1:NCOL(trend_returns)])
revert_weights <- rowMeans(weight_s[, -(1:NCOL(trend_returns))])
diff_weights <- xts(trend_weights-revert_weights, order.by=in_dex)
# Find best and worst EWMA Strategies in each period
bes_t <- apply(weight_s, 1, which.max)
wors_t <- apply(weight_s, 1, which.min)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_diff_weights.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot the mean weights of EWMA Strategies
zoo::plot.zoo(cbind(diff_weights, 
  cl_ose[end_points[-n_rows]]), 
  oma = c(3, 0, 3, 0), mar = c(0, 4, 0, 1), 
  xlab=NULL, ylab=c("diff weights", "VTI"), 
  main="Trend minus Revert Weights of EWMA strategies")
best_worst <- xts(cbind(bes_t, wors_t), order.by=in_dex)
zoo::plot.zoo(best_worst,
  oma = c(3, 0, 3, 0), mar = c(0, 4, 0, 1), 
  xlab=NULL, ylab=c("best EWMA", "worst EWMA"), 
  main="Best and Worst EWMA strategies")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting the \protect\emph{EWMA} \protect\emph{Momentum} Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Backtesting} is the testing of the accuracy of a forecasting model using simulation on historical data.
      \vskip1ex
      \emph{Backtesting} is a type of \emph{cross-validation} applied to time series data.
      \vskip1ex
      \emph{Backtesting} is performed by \emph{training} the model on past data and \emph{testing} it on future out-of-sample data.
      \vskip1ex
      The \emph{training} data is specified by the \emph{look-back} intervals (\texttt{past\_aggs}), and the model forecasts are applied to the future data defined by the \emph{look-forward} intervals (\texttt{fwd\_rets}).
      \vskip1ex
      The out-of-sample \emph{momentum} strategy returns can be calculated by multiplying the \texttt{fwd\_rets} by the forecast \emph{ETF} portfolio weights.
      <<echo=TRUE,eval=FALSE>>=
# Calculate backtest returns
pnl_s <- rowSums(weight_s*fut_rets)
pnl_s <- xts(pnl_s, order.by=in_dex)
colnames(pnl_s) <- "ewma momentum"
close_rets <- rutils::diff_it(cl_ose[in_dex])
cor(cbind(pnl_s, close_rets))
pnl_s <- star_t + cumsum(pnl_s)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/backtest_ewma.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot the backtest
chart_Series(x=cl_ose[end_points[-n_rows]], 
  name="backtest of EWMA strategies", col="orange")
add_TA(pnl_s, on=1, lwd=2, col="blue")
legend("top", legend=c("VTI", "EWMA"), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=c("orange", "blue"), bty="n")
# shad_e <- xts(index(pnl_s) < as.Date("2008-01-31"), order.by=index(pnl_s))
# add_TA(shad_e, on=-1, col="lightgrey", border="lightgrey")
# text(x=7, y=0, labels="warmup period")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy for an \protect\emph{ETF} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{momentum} strategy can be \emph{backtested} for a portfolio of \emph{ETFs} or stocks over a series of \emph{end points} as follows:
      \setlength{\leftmargini}{1.0em}
      \begin{enumerate}
        \item Trade a single unit (dollar) of capital, 
        \item Allocate capital to the assets in proportion to their \emph{relative} past performance, 
        \item Calculate the portfolio weights as the number of units (shares) of each asset,
        \item At each \emph{end point} repeat the above and rebalance the asset allocations,
        \item Calculate the future out-of-sample portfolio returns in each period (without reinvestment).
        \item Calculate the transaction costs (as the bid-offer spread times the asset prices, times the change in weights), and subtract them from the returns.
      \end{enumerate}
      The above points \texttt{\#1, \#2} and \texttt{\#3} represent the \emph{momentum} strategy, while points \texttt{\#4, \#5} and \texttt{\#6} represent the \emph{backtest} procedures.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate ETF prices and simple returns
sym_bols <- c("VTI", "IEF", "DBC")
price_s <- rutils::etf_env$price_s[, sym_bols]
price_s <- na.omit(zoo::na.locf(price_s))
re_turns <- rutils::diff_it(price_s)
# Define look-back and look-forward intervals
end_points <- rutils::calc_endpoints(re_turns, 
  inter_val="months")
n_cols <- NCOL(re_turns)
n_rows <- NROW(end_points)
look_back <- 12
start_points <- c(rep_len(1, look_back-1), 
  end_points[1:(n_rows-look_back+1)])
# Calculate past performance over end_points
perform_ance <- 
  function(re_turns) sum(re_turns)/sd(re_turns)
agg_s <- sapply(1:(n_rows-1), function(it_er) {
  c(past_perf=sapply(re_turns[start_points[it_er]:end_points[it_er]], perform_ance),
    fut_rets=sapply(re_turns[(end_points[it_er]+1):end_points[it_er+1]], sum))
})  # end sapply
agg_s <- t(agg_s)
# Select look-back and look-forward aggregations
past_perf <- agg_s[, 1:n_cols]
fut_rets <- agg_s[, n_cols+1:n_cols]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting the \protect\emph{Momentum} Strategy for an \protect\emph{ETF} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The hypothetical \emph{momentum} strategy returns can be calculated by multiplying the forecast portfolio weights times the forward (future) out-of-sample returns.
      <<echo=TRUE,eval=FALSE>>=
# Calculate portfolio weights equal to number of shares
end_prices <- price_s[end_points[-n_rows]]
weight_s <- 
  past_perf/rowSums(abs(past_perf))/end_prices
weight_s[is.na(weight_s)] <- 0
colnames(weight_s) <- colnames(re_turns)
# Calculate profits and losses
pnl_s <- rowSums(weight_s*fut_rets)
pnl_s <- xts(pnl_s, index(end_prices))
colnames(pnl_s) <- "pnls"
# Calculate transaction costs
bid_offer <- 0.001
cost_s <- 
  0.5*bid_offer*end_prices*abs(rutils::diff_it(weight_s))
cost_s <- rowSums(cost_s)
pnl_s <- (pnl_s - cost_s)
pnl_s <- cumsum(pnl_s)
# Plot momentum strategy with VTI
cl_ose <- price_s[index(end_prices), "VTI"]
zoo::plot.zoo(cbind(cl_ose, pnl_s, weight_s), 
  oma = c(3, 1, 3, 0), mar = c(0, 4, 0, 1), nc=1,
  xlab=NULL, main="ETF Momentum Strategy")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_weights.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Functional for \protect\emph{Momentum} Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# Define backtest functional
backtest_ep <- function(re_turns, price_s, perform_ance=sum, 
    look_back=12, re_balance="months", bid_offer=0.001,
    end_points=rutils::calc_endpoints(re_turns, inter_val=re_balance), 
    with_weights=FALSE, ...) {
  stopifnot("package:quantmod" %in% search() || require("quantmod", quietly=TRUE))
  # Define look-back and look-forward intervals
  n_cols <- NCOL(re_turns)
  n_rows <- NROW(end_points)
  start_points <- c(rep_len(1, look_back-1), end_points[1:(n_rows-look_back+1)])
  # Calculate past performance over end_points
  agg_s <- sapply(1:(n_rows-1), function(it_er) {
    c(past_perf=sapply(re_turns[start_points[it_er]:end_points[it_er]], perform_ance, ...),  # end sapply
    fut_rets=sapply(re_turns[(end_points[it_er]+1):end_points[it_er+1]], sum))  # end sapply
  })  # end sapply
  agg_s <- t(agg_s)
  # Select look-back and look-forward aggregations
  past_perf <- agg_s[, 1:n_cols]
  fut_rets <- agg_s[, n_cols+1:n_cols]
  # Calculate portfolio weights equal to number of shares
  end_prices <- price_s[end_points[-n_rows]]
  weight_s <- past_perf/rowSums(abs(past_perf))/end_prices
  weight_s[is.na(weight_s)] <- 0
  colnames(weight_s) <- colnames(re_turns)
  # Calculate profits and losses
  pnl_s <- rowSums(weight_s*fut_rets)
  pnl_s <- xts(pnl_s, index(end_prices))
  colnames(pnl_s) <- "pnls"
  # Calculate transaction costs
  cost_s <- 0.5*bid_offer*end_prices*abs(rutils::diff_it(weight_s))
  cost_s <- rowSums(cost_s)
  pnl_s <- (pnl_s - cost_s)
  pnl_s <- cumsum(pnl_s)
  if (with_weights)
    cbind(pnl_s, weight_s)
  else
    pnl_s
}  # end backtest_ep
      @
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimization of \protect\emph{Momentum} Strategy Parameters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The performance of the \emph{momentum} strategy depends on the length of the \emph{look-back interval} used for calculating the past performance.
      \vskip1ex
      Performing a \emph{backtest} allows finding the optimal \emph{momentum} (trading) strategy parameters, such as the \emph{look-back interval}.
      \vskip1ex
      But using a different rebalancing frequency in the \emph{backtest} can produce different values for the optimal trading strategy parameters.
      \vskip1ex
      So \emph{backtesting} just redefines the problem of finding (tuning) the optimal trading strategy parameters, into the problem of finding the optimal \emph{backtest} (meta-model) parameters.
      \vskip1ex
      But the advantage of using the \emph{backtest} meta-model is that it can reduce the number of parameters that need to be optimized.
      \vskip1ex
      Performing many \emph{backtests} on multiple trading strategies risks identifying inherently unprofitable trading strategies as profitable, purely by chance (\emph{p-value hacking}).
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/look_back_profile.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
source("C:/Develop/lecture_slides/scripts/back_test.R")
look_backs <- seq(5, 60, by=5)
perform_ance <- function(re_turns) sum(re_turns)/sd(re_turns)
pro_files <- sapply(look_backs, function(x) {
  last(backtest_ep(re_turns=re_turns, price_s=price_s, 
    re_balance="weeks", look_back=x, perform_ance=perform_ance))
})  # end sapply
plot(x=look_backs, y=pro_files, t="l", 
  main="Strategy PnL as function of look_back", 
  xlab="look_back (weeks)", ylab="pnl")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy Performance}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The hypothetical out-of-sample \emph{momentum} strategy returns can be calculated by multiplying the \texttt{fwd\_rets} by the forecast \emph{ETF} portfolio weights.
      \vskip1ex
      The \emph{training} data is specified by the \emph{look-back} intervals (\texttt{back\_aggs}), and the forecasts are applied to the future data defined by the \emph{look-forward} intervals (\texttt{fwd\_rets}).
      <<echo=TRUE,eval=FALSE>>=
look_back <- look_backs[which.max(pro_files)]
pnl_s <- backtest_ep(re_turns=re_turns, price_s=price_s, 
  re_balance="weeks", look_back=look_back, perform_ance=perform_ance,
  with_weights=TRUE)
cl_ose <- Cl(rutils::etf_env$VTI[index(pnl_s)])
# bind model returns with VTI
da_ta <- star_t
da_ta <- cbind(cl_ose, da_ta*pnl_s[, 1]+da_ta)
colnames(da_ta) <- c("VTI", "momentum")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_backtest.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot momentum strategy with VTI
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(da_ta, theme=plot_theme, lwd=2, 
             name="Momentum PnL")
legend("topleft", legend=colnames(da_ta), 
  inset=0.1, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Combining the \protect\emph{Momentum} and Static Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{momentum} strategy has attractive returns compared to a static buy-and-hold strategy.
      \vskip1ex
      But the \emph{momentum} strategy suffers from draw-downs called \emph{momentum crashes}, especially after the market rallies from a sharp-sell-off.
      \vskip1ex
      This suggests that combining the \emph{momentum} strategy with a static buy-and-hold strategy can achieve significant diversification of risk.
      <<echo=TRUE,eval=FALSE>>=
# Combine momentum strategy with static
da_ta <- cbind(da_ta, 0.5* (da_ta[, "VTI"] + da_ta[, "momentum"]))
colnames(da_ta) <- c("VTI", "momentum", "combined")
# Calculate strategy annualized Sharpe ratios
sapply(da_ta, function(cumu_lative) {
  x_ts <- na.omit(diff(log(cumu_lative)))
  sqrt(52)*sum(x_ts)/sd(x_ts)/NROW(x_ts)
})  # end sapply
# Calculate strategy correlations
cor(na.omit(diff(log(da_ta))))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_combined.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot momentum strategy combined with VTI
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue", "green")
chart_Series(da_ta, theme=plot_theme, 
             name="Momentum strategy combined with VTI")
legend("topleft", legend=colnames(da_ta), 
  inset=0.1, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy Versus the \protect\emph{All-Weather} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{All-Weather} portfolio is a static portfolio of stocks (30\%), bonds (55\%), and commodities and precious metals (15\%) (approximately), and was designed by Bridgewater Associates, the largest hedge fund in the world:\\
      \url{https://www.bridgewater.com/research-library/the-all-weather-strategy/}
      \url{http://www.nasdaq.com/article/remember-the-allweather-portfolio-its-having-a-killer-year-cm685511}
      \vskip1ex
      The three different asset classes (stocks, bonds, commodities) provide positive returns under different economic conditions (recession, expansion, inflation).
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define all-weather symbols and weights
weight_s <- c(0.30, 0.55, 0.15)
all_weather <- (re_turns / price_s) %*% weight_s
all_weather <- cumsum(all_weather)
all_weather <- xts(all_weather, index(re_turns))[index(pnl_s)]
all_weather <- star_t*all_weather + 
  star_t
colnames(all_weather) <- "all_weather"
# Combine momentum strategy with all-weather
da_ta <- cbind(da_ta, all_weather)
# Calculate strategy annualized Sharpe ratios
sapply(da_ta, function(cumu_lative) {
  x_ts <- na.omit(diff(log(cumu_lative)))
  sqrt(52)*sum(x_ts)/sd(x_ts)/NROW(x_ts)
})  # end sapply
# Calculate strategy correlations
cor(na.omit(diff(log(da_ta))))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_all_weather.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot momentum strategy, combined, and all-weather
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue", "green", "violet")
chart_Series(da_ta, theme=plot_theme, lwd=2, name="Momentum PnL")
legend("topleft", legend=colnames(da_ta),
  inset=0.1, bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
      \vspace{-1em}
      The combination of bonds, stocks, and commodities in the \emph{All-Weather} portfolio is designed to provide positive returns under most economic conditions, without the costs of trading.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy Market Beta}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{momentum} strategy market beta can be calculated by multiplying the \emph{ETF} betas by the \emph{ETF} portfolio weights.
      <<echo=TRUE,eval=FALSE>>=
# Calculate betas
beta_s <- c(1, rutils::etf_env$capm_stats[
  match(sym_bols[-1], 
        rownames(rutils::etf_env$capm_stats)), 
  "Beta"])
names(beta_s)[1] <- sym_bols[1]
# Weights times betas
weight_s <- price_s[index(pnl_s)]*pnl_s[, -1]
beta_s <- weight_s %*% beta_s
beta_s <- xts(beta_s, order.by=index(weight_s))
colnames(beta_s) <- "portf_beta"
zoo::plot.zoo(cbind(beta_s, cl_ose),
  oma = c(3, 1, 3, 0), mar = c(0, 4, 0, 1), 
  main="betas & VTI", xlab="")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_betas.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{market timing} skill can be measured by performing a \emph{linear regression} of a strategy's returns against a strategy with perfect \emph{market timing} skill.
      \vskip1ex
      The \emph{Merton-Henriksson} market timing test uses a linear \emph{market timing} term:
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + \gamma \max{(0, R_m - R_f)} + {\varepsilon}
      \end{displaymath}
      Where $R$ are the strategy returns, $R_m$ are the market returns, and $R_f$ are the risk-free returns.
      \vskip1ex
      If the coefficient $\gamma$ is statistically significant, then it's very likely due to \emph{market timing} skill.
      \vskip1ex
      The \emph{market timing} regression is a generalization of the \emph{Capital Asset Pricing Model}.
      \vskip1ex
      The \emph{Treynor-Mazuy} test uses a quadratic term, which makes it more sensitive to the magnitude of returns:
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + \gamma (R_m - R_f)^2 + {\varepsilon}
      \end{displaymath}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_timing.png}
    \vspace{-2em}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=6, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 3, 1), oma=c(0, 0, 0, 0))
momentum_rets <- as.numeric(rutils::diff_it(pnl_s[, 1]))
vti_rets <- as.numeric(rutils::diff_it(cl_ose)/100)
# Merton-Henriksson test
vti_b <- cbind(VTI=vti_rets, skill=0.5*(vti_rets+abs(vti_rets)))
mod_el <- lm(momentum_rets ~ vti_b); summary(mod_el)
# Treynor-Mazuy test
vti_b <- cbind(VTI=vti_rets, skill=vti_rets^2)
mod_el <- lm(momentum_rets ~ vti_b); summary(mod_el)
# Plot scatterplot
plot(x=vti_rets, y=momentum_rets, xlab="VTI", ylab="momentum")
title(main="Treynor-Mazuy market timing test\n for Momentum vs VTI", line=0.5)
# Plot fitted (predicted) response values
points(x=vti_rets, y=mod_el$fitted.values, pch=16, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Skewness of \protect\emph{Momentum} Strategy Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The distribution of returns of the \emph{momentum} strategy has about one half of the negative skewness compared to \emph{VTI}.
      \vskip1ex
      The \emph{momentum} strategy indicates the existence of a market anomaly (outsized profits), because it has a smaller negative skewness than the market, while having comparable returns to the market.
      <<echo=TRUE,eval=FALSE>>=
# Normalize the returns
momentum_rets <- 
  (momentum_rets-mean(momentum_rets))
momentum_rets <- 
  sd(vti_rets)*momentum_rets/sd(momentum_rets)
vti_rets <- (vti_rets-mean(vti_rets))
# Calculate ratios of moments
sapply(2:4, FUN=moments::moment, x=vti_rets)/
  sapply(2:4, FUN=moments::moment, x=momentum_rets)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_distr.png}
    \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot histogram
x_lim <- 4*sd(momentum_rets)
hist(momentum_rets, breaks=30, 
  main="Momentum and VTI Return Distributions", 
  xlim=c(-x_lim, x_lim), 
  xlab="", ylab="", freq=FALSE)
# draw kernel density of histogram
lines(density(momentum_rets), col='red', lwd=2)
lines(density(vti_rets), col='blue', lwd=2)
# add legend
legend("topright", inset=0.05, cex=0.8, title=NULL,
       leg=c("Momentum", "VTI"),
       lwd=6, bg="white", col=c("red", "blue"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{rolling portfolio optimization} strategy consists of rebalancing a portfolio over a vector of end points: 
      \setlength{\leftmargini}{1.0em}
      \begin{enumerate}
        \item Calculate the maximum Sharpe ratio portfolio weights at each end point.
        \item Apply the weights in the next interval and calculate the out-of-sample portfolio returns.
      \end{enumerate}
      The parameters of this strategy are:
      \begin{enumerate}
        \item Rebalancing frequency (annual, monthly, etc.)
        \item Length of look-back interval (sliding or expanding).
        \item Scaling of weights (sum or sum-of-squares).
      \end{enumerate}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# sym_bols contains all the symbols in rutils::etf_env$re_turns except "VXX" and "SVXY"
sym_bols <- colnames(rutils::etf_env$re_turns)
sym_bols <- sym_bols[!((sym_bols == "VXX")|(sym_bols == "SVXY"))]
# Extract columns of rutils::etf_env$re_turns and remove NA values
re_turns <- rutils::etf_env$re_turns[, sym_bols]
re_turns <- na.omit(zoo::na.locf(re_turns))
# Calculate vector of monthly end points and start points
look_back <- 12
end_points <- rutils::calc_endpoints(re_turns, inter_val="months")
end_points[end_points<2*NCOL(re_turns)] <- 2*NCOL(re_turns)
      @
    \column{0.5\textwidth}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
n_rows <- NROW(end_points)
# Sliding window
start_points <- c(rep_len(1, look_back-1), end_points[1:(n_rows-look_back+1)])
# OR expanding window
# Start_points <- rep_len(1, NROW(end_points))
# risk_free is the daily risk-free rate
risk_free <- 0.03/252
# Calculate daily excess returns 
ex_cess <- re_turns - risk_free
# Perform loop over end_points
portf_rets <- lapply(2:NROW(end_points),
  function(i) {
    # Subset the ex_cess returns
    ex_cess <- ex_cess[start_points[i-1]:end_points[i-1], ]
    in_verse <- solve(cov(ex_cess))
    # Calculate the maximum Sharpe ratio portfolio weights
    weight_s <- in_verse %*% colMeans(ex_cess)
    weight_s <- drop(weight_s/sqrt(sum(weight_s^2)))
    # Subset the re_turns
    re_turns <- re_turns[(end_points[i-1]+1):end_points[i], ]
    # Calculate the out-of-sample portfolio returns
    xts(re_turns %*% weight_s, index(re_turns))
  }  # end anonymous function
)  # end lapply
portf_rets <- rutils::do_call(rbind, portf_rets)
colnames(portf_rets) <- "portf_rets"
# Calculate compounded cumulative portfolio returns
portf_rets <- cumsum(portf_rets)
quantmod::chart_Series(portf_rets,
  name="Cumulative Returns of Max Sharpe Portfolio Strategy")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} Stock Index Constituent Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{S\&P500} stock index constituent data is of poor quality before \texttt{2000}, so we'll mostly use the data after \texttt{2000}.
      <<echo=TRUE,eval=FALSE>>=
# Load S&P500 constituent stock prices
load("C:/Develop/lecture_slides/data/sp500.RData")
price_s <- eapply(env_sp500, quantmod::Cl)
price_s <- rutils::do_call(cbind, price_s)
# Carry forward and backward non-NA prices
price_s <- zoo::na.locf(price_s)
price_s <- zoo::na.locf(price_s, fromLast=TRUE)
colnames(price_s) <- sapply(colnames(price_s),
  function(col_name) strsplit(col_name, split="[.]")[[1]][1])
# Calculate percentage returns of the S&P500 constituent stocks
re_turns <- rutils::diff_it(log(price_s))
sam_ple <- sample(NCOL(re_turns), s=100, replace=FALSE)
returns_100 <- re_turns[, sam_ple]
save(price_s, re_turns, returns_100, 
  file="C:/Develop/lecture_slides/data/sp500_prices.RData")
# Calculate number of constituents without prices
da_ta <- rowSums(rutils::roll_sum(re_turns, 4)==0)
da_ta <- xts::xts(da_ta, order.by=index(re_turns))
dygraphs::dygraph(da_ta, main="Number of S&P 500 Constituents Without Prices") %>%
  dyAxis("y", valueRange=c(0, 300))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/sp500_without_prices.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} Stock Portfolio Index}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The price-weighted index of \emph{S\&P500} constituents closely follows the VTI \emph{ETF}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate price weighted index of constituent
n_cols <- NCOL(price_s)
in_dex <- xts(rowSums(price_s)/n_cols, index(price_s))
colnames(in_dex) <- "index"
# Combine index with VTI
da_ta <- cbind(in_dex[index(etf_env$VTI)], etf_env$VTI[, 4])
col_names <- c("index", "VTI")
colnames(da_ta) <- col_names
# Plot index with VTI
dygraphs::dygraph(da_ta, 
  main="S&P 500 Price-weighted Index and VTI") %>%
  dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
  dySeries(name=col_names[1], axis="y", col="red") %>%
  dySeries(name=col_names[2], axis="y2", col="blue")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/sp500_portfolio_index.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy for \protect\emph{S\&P500} Stock Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A very simple \emph{momentum} strategy for the \emph{S\&P500}, is to go long constituents with positive recent performance, and short constituents with negative performance.
      \vskip1ex
      This \emph{momentum} strategy does not perform well and suffers from \emph{momentum crashes} when the market rebounds sharply from a recent lows.
      <<echo=TRUE,eval=FALSE>>=
# Calculate rolling variance of S&P500 portfolio
wid_th <- 252
vari_ance <- roll::roll_var(re_turns, width=wid_th)
vari_ance <- zoo::na.locf(vari_ance)
vari_ance[is.na(vari_ance)] <- 0
# Calculate rolling Sharpe of S&P500 portfolio
returns_width <- rutils::diff_it(log(price_s), lagg=wid_th)
weight_s <- returns_width/sqrt(wid_th*vari_ance)
weight_s[vari_ance==0] <- 0
weight_s[1:wid_th, ] <- 1
weight_s[is.na(weight_s)] <- 0
weight_s <- weight_s/sqrt(rowSums(weight_s^2))
weight_s[is.na(weight_s)] <- 0
weight_s <- rutils::lag_it(weight_s)
sum(is.na(weight_s))
# Calculate portfolio profits and losses
pnl_s <- weight_s*re_turns
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/sp500_momentum.png}
    \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate transaction costs
bid_offer <- 0.001
cost_s <- 0.5*bid_offer*abs(rutils::diff_it(weight_s))
pnl_s <- (pnl_s - cost_s)
pnl_s <- exp(cumsum(pnl_s))
pnl_s <- rowMeans(pnl_s)
pnl_s <- xts(pnl_s, order.by=index(price_s))
pnl_s <- cbind(rutils::etf_env$VTI[, 4], pnl_s)
pnl_s <- na.omit(pnl_s)
colnames(pnl_s) <- c("VTI", "momentum")
col_names <- colnames(pnl_s)
# Plot momentum and VTI
dygraphs::dygraph(pnl_s, main=paste(col_names, collapse=" and ")) %>%
  dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
  dySeries(name=col_names[1], axis="y", col="blue") %>%
  dySeries(name=col_names[2], axis="y2", col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Functional for \protect\emph{S\&P500} Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# Define backtest functional
backtest_rolling <- function(re_turns, price_s, wid_th=252, bid_offer=0.001, tre_nd=1, ...) {
  stopifnot("package:quantmod" %in% search() || require("quantmod", quietly=TRUE))
  # Define look-back and look-forward intervals
  n_cols <- NCOL(re_turns)
  vari_ance <- roll::roll_var(re_turns, width=wid_th)
  vari_ance <- zoo::na.locf(vari_ance)
  vari_ance[is.na(vari_ance)] <- 0
  # Calculate rolling Sharpe of S&P500 portfolio
  returns_width <- rutils::diff_it(log(price_s), lagg=wid_th)
  weight_s <- tre_nd*returns_width/sqrt(wid_th*vari_ance)
  weight_s[vari_ance==0] <- 0
  weight_s[1:wid_th, ] <- 1
  weight_s[is.na(weight_s)] <- 0
  weight_s <- weight_s/sqrt(rowSums(weight_s^2))
  weight_s[is.na(weight_s)] <- 0
  weight_s <- rutils::lag_it(weight_s)
  sum(is.na(weight_s))
  # Calculate portfolio profits and losses
  pnl_s <- weight_s*re_turns
  # Calculate transaction costs
  bid_offer <- 0.001
  cost_s <- 0.5*bid_offer*abs(rutils::diff_it(weight_s))
  pnl_s <- (pnl_s - cost_s)
  pnl_s <- exp(cumsum(pnl_s))
  pnl_s <- rowMeans(pnl_s)
  pnl_s
}  # end backtest_rolling
@
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Multiple \protect\emph{S\&P500} \protect\emph{Momentum} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{S\&P500} \emph{momentum} strategies can be backtested by calling the function \texttt{backtest\_rolling()} in a loop over a vector of \emph{width} parameters.
      \vskip1ex
      The \emph{momentum} strategies do not perform well, especially the ones with a small \emph{width} parameter.
      <<echo=TRUE,eval=FALSE>>=
source("C:/Develop/lecture_slides/scripts/back_test.R")
pnl_s <- backtest_rolling(wid_th=252, re_turns=re_turns, 
  price_s=price_s, bid_offer=bid_offer)
width_s <- seq(50, 300, by=50)
# Perform sapply loop over width_s
pro_files <- sapply(width_s, backtest_rolling, re_turns=re_turns, 
  price_s=price_s, bid_offer=bid_offer)
colnames(pro_files) <- paste0("width=", width_s)
pro_files <- xts(pro_files, index(price_s))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/sp500_momentum_mult.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- 
  colorRampPalette(c("blue", "red"))(NCOL(pro_files))
chart_Series(pro_files, 
  theme=plot_theme, name="Cumulative Returns of S&P500 Momentum Strategies")
legend("bottomleft", legend=colnames(pro_files), 
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(re_turns)), 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Multiple \protect\emph{S\&P500} \protect\emph{Mean reverting} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{S\&P500} \emph{mean reverting} strategies can be backtested by calling the function \texttt{backtest\_rolling()} in a loop over a vector of \emph{width} parameters.
      \vskip1ex
      The \emph{mean reverting} strategies for the \emph{S\&P500} constituents perform the best for short \emph{width} parameters.
      <<echo=TRUE,eval=FALSE>>=
width_s <- seq(5, 50, by=5)
# Perform sapply loop over width_s
pro_files <- sapply(width_s, backtest_rolling, re_turns=re_turns, 
  price_s=price_s, bid_offer=bid_offer, tre_nd=(-1))
colnames(pro_files) <- paste0("width=", width_s)
pro_files <- xts(pro_files, index(price_s))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/sp500_revert_mult.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- 
  colorRampPalette(c("blue", "red"))(NCOL(pro_files))
chart_Series(pro_files, 
  theme=plot_theme, name="Cumulative Returns of S&P500 Mean reverting Strategies")
legend("topleft", legend=colnames(pro_files), 
  inset=0.05, bg="white", cex=0.7, lwd=rep(6, NCOL(re_turns)), 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization Strategy for \protect\emph{S\&P500}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a rolling portfolio optimization strategy the portfolio weights are adjusted to their optimal values at every end point.
      \vskip1ex
      A portfolio optimization is performed using past data, and the optimal portfolio weights are applied out-of-sample in the next interval.
      \vskip1ex
      The weights are scaled to match the volatility of the equally weighted portfolio, and are kept constant until the next end point.
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
load("C:/Develop/lecture_slides/data/sp500_prices.RData")
n_cols <- NCOL(re_turns) ; date_s <- index(re_turns)
# Calculate returns on equal weight portfolio
in_dex <- xts(cumsum(re_turns %*% rep(1/n_cols, n_cols)), index(re_turns))
# Define monthly end points
end_points <- rutils::calc_endpoints(re_turns, inter_val="months")
end_points <- end_points[end_points > (n_cols+1)]
n_rows <- NROW(end_points) ; look_back <- 12
start_points <- c(rep_len(1, look_back-1), end_points[1:(n_rows-look_back+1)])
# Perform backtest
al_pha <- 0.01 ; max_eigen <- 3
pnl_s <- HighFreq::back_test(ex_cess=re_turns, 
                             re_turns=re_turns,
                             start_points=start_points-1,
                             end_points=end_points-1,
                             al_pha=al_pha,
                             max_eigen=max_eigen)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/backtest_sharpe_monthly.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot strategy in log scale
pnl_s <- cumsum(pnl_s)
pnl_s <- cbind(pnl_s, in_dex, (pnl_s+in_dex)/2)
col_names <- c("Strategy", "Index", "Average")
colnames(pnl_s) <- col_names
dygraphs::dygraph(pnl_s[end_points], main="Rolling Portfolio Optimization Strategy") %>%
  dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
  dySeries(name=col_names[1], axis="y", col="red", strokeWidth=1) %>%
  dySeries(name=col_names[2], axis="y2", col="blue", strokeWidth=1) %>%
  dySeries(name=col_names[3], axis="y2", col="green", strokeWidth=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Backtesting Framework with Overlapping Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An interval aggregation can be specified by a vector of look-back \emph{intervals} attached at \emph{end points} spanning fixed time \emph{intervals}.
      \vskip1ex
      For example, we may wish to perform aggregations at weekly \emph{end points}, over overlapping 40-week look-back \emph{intervals}.
      \vskip1ex
      The variable \texttt{look\_back} is equal to the number of end points in the look-back \emph{interval}, while (\texttt{look\_back - 1}) is equal to the number of intervals in the look-back.
      \vskip1ex
      The \emph{startpoints} are the \emph{end points} lagged by the number of interval intervals (number of intervals in the interval).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Define time interval for end points
re_balance <- "weeks"
# Look-back interval is multiple of re_balance
look_back <- 41
# Create index of rebalancing period end points
end_points <- xts::endpoints(rutils::etf_env$re_turns, 
                             on=re_balance)
# Start_points are multi-period lag of end_points
n_rows <- NROW(end_points)
start_points <- c(rep_len(1, look_back-1), 
  end_points[1:(n_rows-look_back+1)])
# Create list of look-back intervals
look_backs <- lapply(2:n_rows, 
    function(it_er) {
      start_points[it_er]:end_points[it_er]
  })  # end lapply
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Performing Overlapping Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An interval aggregation can be specified by a vector of look-back \emph{intervals} attached at \emph{end points} spanning fixed time \emph{intervals}.
      \vskip1ex
      For example, we may wish to perform aggregations at weekly \emph{end points}, over overlapping 40-week look-back \emph{intervals}.
      \vskip1ex
      The variable \texttt{look\_back} is equal to the number of end points in the look-back \emph{interval}, while (\texttt{look\_back-1}) is equal to the number of intervals in that interval.
      \vskip1ex
      The \emph{startpoints} are the \emph{end points} lagged by the number of interval intervals (number of intervals in the interval).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Create vector of symbols for model
sym_bols <- c("VTI", "IEF", "DBC")

# Calculate risk&ret stats for some symbols, over a range of dates
# Perform lapply() loop over look_backs
risk_stats <- lapply(look_backs, 
  function(look_back) {
    x_ts <- 
      rutils::etf_env$re_turns[look_back, sym_bols]
    t(sapply(x_ts, 
      function(col_umn)
        c(return=mean(col_umn), risk=mad(col_umn))
      ))  # end sapply
    })  # end lapply
# rbind list into single xts or matrix
# risk_stats <- rutils::do_call_rbind(risk_stats)
# head(risk_stats)
# Calculate non-overlapping returns in interval
re_turns <-sapply(2:n_rows, 
    function(it_er) {
    sapply(rutils::etf_env$re_turns[
      (end_points[it_er-1]+1):end_points[it_er], 
      sym_bols], sum)
  })  # end sapply
re_turns <- t(re_turns)
      @
  \end{columns}
\end{block}

\end{frame}


\end{document}
