% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='scriptsize', fig.width=4, fig.height=4)
options(width=60, dev='pdf')
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}



% Title page setup
\title[Investment Strategies]{Investment Strategies}
\subtitle{FRE6871 \& FRE7241, Fall 2019}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{\today}



%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Investor Risk Preferences and Portfolio Selection}


%%%%%%%%%%%%%%%
\subsection{Single Period Binary Gamble}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Consider a binary betting game (gamble) with the probability of winning equal to \texttt{p}, the winning amount (gain) equal to \texttt{b}, and the loss equal to \texttt{-a},
      \vskip1ex
      The investor makes no up-front payments, and either wins an amount \texttt{b}, or loses an amount \texttt{-a},
      \vskip1ex
      Assuming an investor makes decisions exclusively on the basis of the expected value (level) of future wealth, then she would choose to bet all her wealth on the gamble if its expected value is positive, and choose not to bet at all if its expected value is negative,
    \column{0.5\textwidth}
      <<results='asis',echo=FALSE,eval=FALSE>>=
library(xtable)
binbet_table <- data.frame(win=c("p", "b"), lose=c("q = 1 - p", "a"))
rownames(binbet_table) <- c("probability", "payout")
# print(xtable(binbet_table), comment=FALSE, size="tiny")
print(xtable(binbet_table), comment=FALSE)
      @
      The expected value of the gamble is equal to: $ev=p \cdot b - q \cdot a$,
      \vskip1ex
      The variance of the gamble is equal to: $var=p \cdot q \cdot (a+b)^2$,
      \vskip1ex
      Without loss of generality we can assume $p=q=\frac{1}{2}$,\\
      $ev=0.5 \cdot (b-a)$,\\
      $var=0.25 \cdot (a+b)^2$,
      \vskip1ex
      The \emph{Sharpe} ratio of the gamble is then equal to:
      \begin{displaymath}
        S_r=\frac{ev}{sqrt(var)}=\frac{(b-a)}{(a+b)}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investor Utility and Fractional Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{expected utility} hypothesis states that investor risk preferences are based on the expected value of the \emph{utility} of their wealth, instead of on the level of wealth,
      \vskip1ex
      In 1738 Daniel Bernoulli introduced the concept of \emph{logarithmic utility} in his work "\emph{Specimen Theoriae Novae de Mensura Sortis}" (New Theory of the Measurement of Risk),
      \vskip1ex
      \emph{Logarithmic utility} is defined as the logarithm of wealth, 
      \vskip1ex
      The expected value of \emph{logarithmic utility} for the binary gamble is equal to: $g(f)=p \cdot log(1+fb) + q \cdot log(1-fa)$,
      \vskip1ex
      Under \emph{logarithmic utility} investor preferences are based on the percentage change of their wealth, instead of the absolute change of their wealth,
      \vskip1ex
      An investor under \emph{logarithmic utility} doesn't bet either all her wealth or nothing at all, but instead bets only a certain fraction \texttt{f} of wealth, depending on the risk/return of the gamble,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<util_plot,eval=FALSE,echo=(-1),fig.show='hide'>>=
par(mar=c(5, 2, 2, 2), mgp=c(1.5, 0.5, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# Define utility
utility <- function(frac, p=0.5, a=1, b=4) {
  p*log(1+frac*b) + (1-p)*log(1-frac*a)
}  # end utility
# Plot utility
curve(expr=utility, xlim=c(0, 1), 
      ylim=c(-0.5, 0.3), xlab="betting fraction", 
      ylab="utility", main="", lwd=2)
title(main="logarithmic utility", line=-0.8)
      @
      \vspace{-2.5em}
    \includegraphics[width=0.5\paperwidth]{figure/util_plot-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Fractional Betting and the Kelly Criterion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Given the odds and probability of winning, there's an optimal fraction \texttt{f} of wealth that the investor should risk on each bet, 
      \vskip1ex
      The betting fraction that maximizes the \emph{utility} can be found by equating the derivative of \emph{utility} to zero:
      \begin{displaymath}
        \frac{\mathrm{d}g(f)}{\mathrm{d}f}=\frac{p \cdot b}{1+fb} - \frac{q \cdot a}{1-fa} = 0
      \end{displaymath}
      \begin{displaymath}
        f=\frac{p}{a}-\frac{q}{b}=\frac{p \cdot b-q \cdot a}{a \cdot b}
      \end{displaymath}
      The optimal \texttt{f} is called the \emph{Kelly} fraction, and it depends on the parameters of the gamble,
      \vskip1ex
      The \emph{Kelly} fraction can be greater than \texttt{1} (leverage), or be negative (shorting),
      \vskip1ex
      If we assume $a=1$, then: $f=\frac{p(b+1)-1}{b}$
      \vskip1ex
      The \emph{Kelly} fraction is equal to the expected net winnings divided by the odds,
    \column{0.5\textwidth}
      \vspace{-1.5em}
      <<kelly_plot,eval=FALSE,echo=(-1),fig.show='hide'>>=
par(mar=c(5, 2, 2, 2), mgp=c(1.5, 0.5, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# Define and plot Kelly fraction
kelly <- function(b, p=0.5, a=1) {
  p/a - (1-p)/b
}  # end kelly
curve(expr=kelly, xlim=c(0, 5), 
      ylim=c(-2, 1), xlab="betting odds", 
      ylab="kelly", main="", lwd=2)
abline(h=0.5, lwd=2, col="red")
text(x=1.5, y=0.5, pos=3, cex=0.8, labels="max Kelly fraction=0.5")
title(main="Kelly fraction", line=-0.8)
      @
      \vspace{-2.5em}
    \includegraphics[width=0.5\paperwidth]{figure/kelly_plot-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Fractional Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Investors under \emph{logarithmic utility} are sensitive to the risk of ruin (losing all their wealth),
      \vskip1ex
      The loss amount \texttt{"a"} determines the risk of ruin, with larger values of \texttt{"a"} increasing the risk of ruin,
      \vskip1ex
      Therefore investors will choose a smaller betting fraction \texttt{f} for larger values of \texttt{"a"},
      \vskip1ex
      This means that even for huge odds in their favor, investors may choose not to bet all their wealth, because of the risk of ruin,
      \vskip1ex
      For example, if the betting odds are very large $b \to \infty$, then the \emph{Kelly} fraction: $f=\frac{p}{a}$,
      \vspace{-1em}
      <<kelly_max_plot,eval=FALSE,echo=(-1),fig.show='hide'>>=
par(mar=c(5, 2, 2, 2), mgp=c(1.5, 0.5, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# Plot several Kelly fractions
curve(expr=kelly, xlim=c(0, 5), 
      ylim=c(-1, 1.5), xlab="betting odds", 
      ylab="kelly", main="", lwd=2)
abline(h=0.5, lwd=2, col="red")
text(x=1.5, y=0.5, pos=3, cex=0.8, labels="a=1.0; max fraction=0.5")
kelly2 <- function(b) {kelly(b=b, a=0.5)}
curve(expr=kelly2, add=TRUE, main="", lwd=2)
abline(h=1.0, lwd=2, col="red")
text(x=1.5, y=1.0, pos=3, cex=0.8, labels="a=0.5; max fraction=1.0")
title(main="Kelly fraction", line=-0.8)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
    \includegraphics[width=0.5\paperwidth]{figure/kelly_max_plot-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Multiperiod Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiperiod betting consists of \texttt{n} rounds of betting, with random returns equal to $r_i$, 
      \vskip1ex
      Then after \texttt{n} rounds the wealth is equal to: $w=(1+r_1) (1+r_2) \ldots (1+r_n)$,
      \vskip1ex
      The expected value of wealth after \texttt{n} rounds is equal to: $E[w]=(1+E[r_i])^n$ (since the $r_i$ are i.i.d. - independent and identically distributed),
      \vskip1ex
      If only a fraction \texttt{f} of wealth is bet each time, then the wealth is equal to: $w=(1+f \cdot r_1) (1+f \cdot r_2) \ldots (1+f \cdot r_n)$, \\
      and the expected wealth is equal to: $E[w]=(1+f \cdot E[r_i])^n$,
      \vskip1ex
      The utility is equal to the logarithm of wealth: $g(w)=log(w)=\sum_{i=1}^n log(1+f \cdot r_i)$, \\
      and the expected utility is equal to: $E[g(w)]=n \cdot E[log(1+f \cdot r_i)]$,
      \vskip1ex
      The expected utility for multiperiod betting is \texttt{n} times the expected utility for single period betting,
      \vskip1ex
      The \emph{Kelly} fraction for multiperiod betting is the same as for single period betting,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<multi_betting,eval=FALSE,echo=(-(1:2)),fig.show='hide'>>=
par(mar=c(5, 2, 2, 2), mgp=c(1.5, 0.5, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
set.seed(1121)  # reset random number generator
# simulated wealth path
wealth_path <- cumprod(1+runif(1000, 
                      min=-0.1, max=0.1))
plot(wealth_path, type="l", 
     lty="solid", xlab="", ylab="")
title(main="wealth path", line=-1)
      @
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/multi_betting-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Wealth Paths}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Consider an investment strategy with a probability of gain equal to \texttt{win\_prob}, and with binary payouts equal to either \texttt{pro\_fit} or \texttt{-lo\_ss}, 
      \vskip1ex
      The investor makes no up-front payments, and either gains an amount equal to \texttt{pro\_fit}, or loses an amount \texttt{-lo\_ss},
      \vskip1ex
      Daily stock returns typically have a slightly greater probability of gaining over losing, and almost equal \texttt{pro\_fit} or \texttt{lo\_ss}, 
      \vskip1ex
      On the other hand returns on lottery tickets typically have very small probabilities of gains, but very large \texttt{pro\_fit} versus small \texttt{lo\_ss}, 
      \vskip1ex
      The investor makes no up-front payments, and either wins an amount equal to \texttt{pro\_fit}, or loses an amount \texttt{-lo\_ss},
      \vskip1ex
      Simulations in \texttt{R} can be accelerated by pre-computing a vector of random numbers, instead of generatng them one at a time in a loop,
      \vskip1ex
      Vectors of random numbers allow using \emph{vectorized} functions, instead of inefficient (slow) \texttt{while()} loops,
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
len_gth <- 1000  # number of simulation steps
n_simu <- 100  # number of simulation paths
# parameters for stock returns
prob_ab <- 0.51
pro_fit <- 0.001
lo_ss <- 0.001
# parameters for lottery ticket returns
prob_ab <- 0.01
pro_fit <- 0.001*51
lo_ss <- 0.001/(99/49)
# simulate random binary wealth paths
set.seed(1121)  # reset random number generator
path_s <- matrix(
  rbinom(n=n_simu*len_gth, size=1, prob=prob_ab), 
  ncol=n_simu)
path_s <- (pro_fit + lo_ss)*path_s - lo_ss
path_s <- matrixStats::colCumsums(path_s)
x11()
ts.plot(path_s, xlab="number of steps", ylab="wealth")
title(main="wealth paths for stocks", line=-1)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/simu_brown_barrier.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Binomial Multiperiod Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kelly} fraction for multiperiod betting can be found by maximizing the expected \emph{utility} of the final wealth distribution:
      \begin{multline*}
        g(f)=\sum_{k=0}^n \binom{n}{k} p^k q^{n-k} log((1+fb)^k (1-fa)^{n-k})\\
        =log(1+fb) \sum_{k=0}^n {\binom{n}{k} p^k q^{n-k} k} + \\
        log(1-fa) \sum_{k=0}^n {\binom{n}{k} p^k q^{n-k} (n-k)} \\
        =n \cdot p \cdot log(1+fb) + n \cdot q \cdot log(1-fa)
      \end{multline*}
      The above is just the single period \emph{utility} multiplied by the number of rounds of betting \texttt{n},
      \vskip1ex
      The \emph{Kelly} fraction \texttt{f} for multiperiod betting is the same as for single period betting:
      \begin{displaymath}
        f=\frac{p}{a}-\frac{q}{b}
      \end{displaymath}
    \column{0.5\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Multiperiod Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In multiperiod betting the investor participates in \texttt{n} rounds of betting,
      \vskip1ex
      Each round of betting multiplies the wealth by either $(1+fb)$ or $(1-fa)$, 
      \vskip1ex
      If there were \texttt{i} wins and \texttt{j} losses, then the final wealth is equal to: 
      \begin{displaymath}
        w=(1+fb)^i \cdot (1-fa)^j
      \end{displaymath}
      The betting fraction that maximizes the final wealth is equal to:
      \begin{displaymath}
        f=\frac{pfreq}{a}-\frac{qfreq}{b}
      \end{displaymath}
      where $pfreq=\frac{i}{n}$ and $qfreq=1-pfreq$,
      \vskip1ex
      The most likely value of \texttt{pfreq} is equal to the probability of winning \texttt{p}, 
      \vskip1ex
      So the \emph{Kelly} fraction is the betting fraction that maximizes the final wealth in multiperiod betting, provided the fraction of wins is equal to the probability of winning \texttt{p},
    \column{0.5\textwidth}
      \vspace{-1em}
      <<kelly_multi,eval=FALSE,echo=(-1),fig.show='hide'>>=
par(mar=c(5, 2, 2, 2), mgp=c(1.5, 0.5, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# wealth of multiperiod binary betting
wealth <- function(f, b=2, a=1, n=100, i=51) {
  (1+f*b)^i * (1-f*a)^(n-i)
}  # end wealth
curve(expr=wealth, xlim=c(0, 1), 
      xlab="betting fraction", 
      ylab="wealth", main="", lwd=2)
title(main="wealth of multiperiod betting", line=0.1)
      @
    \vspace{-2em}
    \includegraphics[width=0.5\paperwidth]{figure/kelly_multi-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Utility Function of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let the returns over a short time period be equal to \texttt{r}, with probability distribution \texttt{p(r)},
      \vskip1ex
      The mean return $\bar{r}$, and variance ${\sigma}^2$ are:
      \begin{displaymath}
        \bar{r} = \int {r \, p(r) \, \mathrm{d}r} \; ; \quad
        {\sigma}^2 = \int {(r - \bar{r})^2 \, p(r) \, \mathrm{d}r}
      \end{displaymath}
      Since the returns are over a short time period, we have: $r \ll 1$ and $\bar{r} \ll \sigma$, so that we can replace $r - \bar{r}$ with \texttt{r} as follows:
      \begin{displaymath}
        \int {(r - \bar{r})^2 \, p(r) \, \mathrm{d}r} \approx \int {r^2 \, p(r) \, \mathrm{d}r}
      \end{displaymath}
      The logarithmic utility $g(w)$ is given by:
      \begin{multline*}
        g(w) = \int {log(1+f \cdot r) \, p(r) \, \mathrm{d}r} \\
        \hspace*{-1em}
        = \int {(f \cdot r - \frac{(f \cdot r)^2}{2} + 
        \frac{(f \cdot r)^3}{3} - \frac{(f \cdot r)^4}{4}) \, p(r) \, \mathrm{d}r}\\
        \hspace*{-2em}
        = f\bar{r} - \frac{f^2 {\sigma}^2}{2} + \frac{f^3 {\sigma}^3 \hat{s}}{3} - \frac{f^4 {\sigma}^4 \hat{k}}{4}
      \end{multline*}
      Where $\hat{s}$ is the skewness, and $\hat{k}$ is the kurtosis,
    \column{0.5\textwidth}
      \vspace{-2em}
      <<utility_returns,eval=FALSE,echo=(-(1:2)),fig.show='hide'>>=
par(mar=c(5, 2, 2, 2), mgp=c(1.5, 0.5, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
library(rutils)
library(PerformanceAnalytics)
ts_rets <- rutils::etf_env$re_turns[, "VTI"]
c(mean(ts_rets), sd(ts_rets))
utility <- function(frac, r=ts_rets) {
sapply(frac, function(fract) sum(log(1+fract*r)))
}  # end utility
curve(expr=utility, 
      xlim=c(0.1, 2*PerformanceAnalytics::KellyRatio(R=ts_rets, method="full")), 
      xlab="kelly", ylab="utility", main="", lwd=2)
title(main="utility", line=-2)
      @
    \vspace{-3em}
    \includegraphics[width=0.5\paperwidth]{figure/utility_returns-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Criterion for Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kelly} fraction can be found by equating the derivative of \emph{utility} to zero:
      \begin{displaymath}
        \frac{\mathrm{d}g(f)}{\mathrm{d}f} = \bar{r} - f{\sigma}^2 + f^2 {\sigma}^3 \hat{s} - f^3 {\sigma}^4 \hat{k} = 0
      \end{displaymath}
      Assuming $\hat{s}$ and $\hat{k}$ are small and can be neglected, we get:
      \begin{displaymath}
        f = \frac{\bar{r}}{{\sigma}^2} = \frac{S_r}{\sigma} \; ; \quad g(w) = \frac{1}{2} \frac{{\bar{r}}^2}{{\sigma}^2} = \frac{1}{2} S_r^2
      \end{displaymath}
      Where $S_r$ is the \emph{Sharpe} ratio, 
      \vskip1ex
      The \emph{Kelly} fraction is equal to the \emph{Sharpe} ratio divided by the \emph{standard deviation},
      \vskip1ex
      The optimal utility $g(w)$ is equal to half the \emph{Sharpe} ratio squared,
      \vskip1ex
      The \emph{standard deviation} and \emph{Sharpe} ratio are calculated over the same time interval as the returns (not annualized),
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=(-(1:3)),eval=FALSE>>=
par(mar=c(5, 2, 2, 2), mgp=c(1.5, 0.5, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
library(PerformanceAnalytics)
ts_rets <- rutils::etf_env$re_turns[, "VTI"]
PerformanceAnalytics::KellyRatio(R=ts_rets, method="full")
      @
%    \vspace{-1em}
%    \includegraphics[width=0.5\paperwidth]{figure/kelly_returns-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Investor \protect\emph{Utility}, \protect\emph{Risk Aversion}, \protect\emph{Prudence} and \protect\emph{Temperance}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{expected utility} hypothesis states that investor risk and return preferences are based on the expected value of the \emph{utility} of their wealth, instead of on the level of wealth, 
      \vskip1ex
      Investor risk and return preferences depend on the signs of the derivatives of the \emph{utility} function, 
      \vskip1ex
      A \emph{utility} function with a positive first derivative implies a preference for higher \emph{returns} (first moment), while a negative second derivative implies risk aversion and a preference for lower \emph{volatility} (second moment), 
      \vskip1ex
      A positive third derivative implies \emph{prudence}, or a preference for higher \emph{skewness} (third moment), while a negative fourth derivative implies \emph{temperance}, or a preference for lower \emph{kurtosis} (fourth moment), 
      \vskip1ex
      Investors with a logarithmic \emph{utility} of wealth base their preferences on the percentage change of wealth, instead of the absolute change, and have a preference for larger odd moments and smaller even moments, 
    \column{0.5\textwidth}
      \begin{multline*}
        u(w) = u(w_0) + \mathrm{d}w \frac{\mathrm{d}u}{\mathrm{d}w} + {\mathrm{d}w}^2 \frac{1}{2} \frac{\mathrm{d}^2u}{\mathrm{d}w^2} + \\
        {\mathrm{d}w}^3 \frac{1}{3!} \frac{\mathrm{d}^3u}{\mathrm{d}w^3} + {\mathrm{d}w}^4 \frac{1}{4!} \frac{\mathrm{d}^4u}{\mathrm{d}w^4} + \ldots
      \end{multline*}
      \begin{multline*}
        \mathrm{d}E[u] = \alpha_1 mean + \alpha_1 variance + \\
        \alpha_1 skew + \alpha_4 kurtosis + \ldots
      \end{multline*}
      mean: $\bar{w}=\frac{1}{k} \sum_{i=1}^{k} w_i$
      \vskip1ex
      variance: $\hat{\sigma}^2=\frac{1}{k-1} \sum_{i=1}^{k} (w_i-\bar{w})^2$
      \vskip1ex
      skew: $\hat{s}=\frac{1}{k-1} \sum_{i=1}^{k} (\frac{w_i-\bar{w}}{\hat{\sigma}})^3$
      \vskip1ex
      kurtosis: $\hat{k}=\frac{1}{k-1} \sum_{i=1}^{k} (\frac{w_i-\bar{w}}{\hat{\sigma}})^4$
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Investor Preferences and Empirical Return Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Investor preference for higher \emph{returns} and for lower \emph{volatility} is expressed by maximizing the \emph{Sharpe} ratio, 
      \vskip1ex
      Stock price \emph{momentum} refers to the fact that stocks with high past returns tend to have high future returns, and vice versa, 
      \vskip1ex
      Stock price \emph{momentum} has been observed, that is driven by approximately one-year of past returns:\\
Eugene Fama and Kenneth French, \href{http://papers.ssrn.com/sol3/papers.cfm?abstract_id=911960}{\emph{Dissecting Anomalies}}\\
      Asness et al., \href{http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2435323}{\emph{Fact, Fiction and Momentum Investing}}
      \vskip1ex
      The question then is can momentum also work on shorter, intraday time scales?
      \vskip1ex
      Also, do higher moments (skew, kurtosis) have predictive power as well?\\
      Amaya et al., \href{http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1898735}{\emph{Does Realized Skewness Predict the Cross-Section of Equity Returns?}}\\
      \vskip1ex
      Stocks typically have negative skew and excess kurtosis, the opposite of what investors prefer, 
      \vskip1ex
      Higher moments are hard to estimate from low frequency (daily) returns, 
    \column{0.5\textwidth}
      \vspace{-3em}
      \includegraphics[width=0.5\paperwidth]{figure/earl_ret_hist-1}
      \vspace{-3em}
      <<echo=(-(1:6))>>=
# load package "HighFreq"
library(HighFreq)
sym_bol <- "SPY"  # define sym_bol
# load OHLC data
re_turns <- calc_rets(xts_data=to.daily(SPY))
len_gth <- nrow(re_turns)  # number of observations
mean_rets <- mean(re_turns[, 1])  # calculate mean
sd_rets <- sd(re_turns[, 1])  # calculate standard deviation
# calculate skew and kurtosis
(sum(((re_turns[, 1] - mean_rets)/sd_rets)^3))/len_gth
(sum(((re_turns[, 1] - mean_rets)/sd_rets)^4))/len_gth
      @
      \vspace{-3em}
      <<earl_ret_hist,echo=TRUE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PerformanceAnalytics)
chart.Histogram(re_turns[, 1], main="", 
  xlim=c(-6e-5, 6e-5), 
  methods = c("add.density", "add.normal"))
# add title
title(main=paste(sym_bol, "density"), line=-1)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Evaluating Manager Skill}


%%%%%%%%%%%%%%%
\subsection{Tests for Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{market timing} skill can be measured by performing a \emph{linear regression} of a strategy's returns against a strategy with perfect \emph{market timing} skill.
      \vskip1ex
      The \emph{Merton-Henriksson} market timing test uses a linear \emph{market timing} term:
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + \gamma \max{(0, R_m - R_f)} + {\varepsilon}
      \end{displaymath}
      Where $R$ are the strategy returns, $R_m$ are the market returns, and $R_f$ are the risk-free returns.
      \vskip1ex
      If the coefficient $\gamma$ is statistically significant, then it's very likely due to \emph{market timing} skill.
      \vskip1ex
      The \emph{market timing} regression is a generalization of the \emph{Capital Asset Pricing Model}.
      \vskip1ex
      The \emph{Treynor-Mazuy} test uses a quadratic term, which makes it more sensitive to the magnitude of returns:
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + \gamma (R_m - R_f)^2 + {\varepsilon}
      \end{displaymath}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ief_vti_timing.png}
    \vspace{-2em}
      <<echo=(-(1:4)),eval=FALSE>>=
# open x11 for plotting
x11(width=6, height=4)
# set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 3, 1), oma=c(0, 0, 0, 0))
# Test if IEF can time VTI
re_turns <- na.omit(rutils::etf_env$re_turns[, c("IEF", "VTI")])
vt_i <- re_turns[, "VTI"]
re_turns <- cbind(re_turns, 0.5*(vt_i+abs(vt_i)), vt_i^2)
colnames(re_turns)[3:4] <- c("merton", "treynor")
# Merton-Henriksson test
mod_el <- lm(IEF ~ VTI + merton, data=re_turns); summary(mod_el)
# Treynor-Mazuy test
mod_el <- lm(IEF ~ VTI + treynor, data=re_turns); summary(mod_el)
# Plot scatterplot
with(re_turns, plot.default(x=VTI, y=IEF-mod_el$coefficients[2]*VTI, xlab="VTI", ylab="IEF"))
title(main="Treynor-Mazuy market timing test\n for IEF vs VTI", line=0.5)
# Plot fitted (predicted) response values
with(re_turns, points.default(x=VTI, y=mod_el$fitted.values-mod_el$coefficients[2]*VTI, pch=16, col="red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Identifying Managers With Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Adapt from RFinance\_2017.Rmd and from scratch.R,
      \vskip1ex
      \vskip1ex
      Consider a binary betting game (gamble) with the probability of winning equal to \texttt{p}, the winning amount (gain) equal to \texttt{b}, and the loss equal to \texttt{-a},
      \vskip1ex
      The investor makes no up-front payments, and either wins an amount \texttt{b}, or loses an amount \texttt{-a},
      \vskip1ex
      Assuming an investor makes decisions exclusively on the basis of the expected value (level) of future wealth, then she would choose to bet all her wealth on the gamble if its expected value is positive, and choose not to bet at all if its expected value is negative,
    \column{0.5\textwidth}
      <<results='asis',echo=FALSE,eval=FALSE>>=
library(xtable)
binbet_table <- data.frame(win=c("p", "b"), lose=c("q = 1 - p", "a"))
rownames(binbet_table) <- c("probability", "payout")
# print(xtable(binbet_table), comment=FALSE, size="tiny")
print(xtable(binbet_table), comment=FALSE)
      @
      The expected value of the gamble is equal to: $ev=p \cdot b - q \cdot a$,
      \vskip1ex
      The variance of the gamble is equal to: $var=p \cdot q \cdot (a+b)^2$,
      \vskip1ex
      Without loss of generality we can assume $p=q=\frac{1}{2}$,\\
      $ev=0.5 \cdot (b-a)$,\\
      $var=0.25 \cdot (a+b)^2$,
      \vskip1ex
      The \emph{Sharpe} ratio of the gamble is then equal to:
      \begin{displaymath}
        S_r=\frac{ev}{sqrt(var)}=\frac{(b-a)}{(a+b)}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Active Investment Strategies}


%%%%%%%%%%%%%%%
\subsection{Static Stock and Bond Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the past, the returns of stocks and bonds have usually been negatively correlated.
      \vskip1ex
      Static portfolios consisting of stocks and bonds provide a much better risk versus return tradeoff than either of the assets separately.
      \vskip1ex
      The static weights depend on the investment horizon, with a greater allocation to bonds for a shorter investment horizon.
      \vskip1ex
      Active investment strategies are expected to outperform static stock and bond portfolios. 
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)  # Load package rutils
# Calculate ETF returns
re_turns <- 
  rutils::etf_env$re_turns[, c("IEF", "VTI")]
re_turns <- na.omit(re_turns)
re_turns <- cbind(re_turns, 
  0.6*re_turns[, "IEF"]+0.4*re_turns[, "VTI"])
colnames(re_turns)[3] <- "combined"
# Calculate correlations
cor(re_turns)
# Calculate Sharpe ratios
sqrt(252)*sapply(re_turns, function(x) mean(x)/sd(x))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/static_stocks_bonds.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate prices from returns
price_s <- lapply(re_turns, 
  function(x) exp(cumsum(x)))
price_s <- rutils::do_call(cbind, price_s)
# Plot prices
dygraphs::dygraph(price_s, main="Stock and Bond Portfolio") %>% 
  dyOptions(colors=c("green","blue","green")) %>%
  dySeries("combined", color="red", strokeWidth=2) %>%
  dyLegend(show="always")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: \protect\emph{RSI} Price Technical Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Relative Strength Index} (\emph{RSI}) is defined as the weighted average of prices over a rolling interval:
      \begin{displaymath}
        P_i^{RSI} = (1-\exp(-\lambda)) \sum_{j=0}^{\infty} \exp(-\lambda j) P_{i-j}
      \end{displaymath}
      Where the decay parameter $\lambda$ determines the rate of decay of the \emph{RSI} weights, with larger values of $\lambda$ producing faster decay, giving more weight to recent prices, and vice versa, 
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Get close prices and calculate close-to-close returns
# price_s <- quantmod::Cl(rutils::etf_env$VTI)
price_s <- quantmod::Cl(HighFreq::SPY)
colnames(price_s) <- rutils::get_name(colnames(price_s))
re_turns <- TTR::ROC(price_s)
re_turns[1] <- 0
# Calculate the RSI indicator
r_si <- TTR::RSI(price_s, 2)
# Calculate the long (up) and short (dn) signals
sig_up <- ifelse(r_si < 10, 1, 0)
sig_dn <- ifelse(r_si > 90, -1, 0)
# Lag signals by one period
sig_up <- rutils::lag_it(sig_up, 1)
sig_dn <- rutils::lag_it(sig_dn, 1)
# Replace NA signals with zero position
sig_up[is.na(sig_up)] <- 0
sig_dn[is.na(sig_dn)] <- 0
# Combine up and down signals into one
sig_nals <- sig_up + sig_dn
# Calculate cumulative returns
eq_up <- exp(cumsum(sig_up*re_turns))
eq_dn <- exp(cumsum(-1*sig_dn*re_turns))
eq_all <- exp(cumsum(sig_nals*re_turns))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/rsi_indic.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot daily cumulative returns in panels
end_points <- endpoints(re_turns, on="days")
plot.zoo(cbind(eq_all, eq_up, eq_dn)[end_points], lwd=c(2, 2, 2), 
  ylab=c("Total","Long","Short"), col=c("red","green","blue"),
  main=paste("RSI(2) strategy for", colnames(price_s), "from", 
             format(start(re_turns), "%B %Y"), "to", 
             format(end(re_turns), "%B %Y")))
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Moving Average Technical Indicators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Volume-Weighted Average Price (\emph{VWAP}) is defined as the sum of prices multiplied by trading volumes, divided by the sum of volumes.
      \vskip1ex
      Moving averages (such as \emph{VWAP}) are often used to define technical indicators (trading signals).
      <<echo=TRUE,eval=FALSE>>=
# Calculate open, close, and lagged prices
oh_lc <- rutils::etf_env$VTI
op_en <- quantmod::Op(oh_lc)
cl_ose <- quantmod::Cl(oh_lc)
star_t <- as.numeric(cl_ose[1])
# Define aggregation interval and calculate VWAP
look_back <- 150
v_wap <- HighFreq::roll_vwap(oh_lc, 
              look_back=look_back)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vwap_indic.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot prices and VWAP
chart_Series(x=cl_ose, 
  name="VTI prices and VWAP", col="orange")
add_TA(v_wap, on=1, lwd=2, col="blue")
legend("top", legend=c("VTI", "VWAP"), 
  bg="white", lty=1, lwd=6, 
  col=c("orange", "blue"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Moving Average Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a trend following \emph{Moving Average Crossover} strategy, when the current price crosses above the \emph{VWAP}, then the strategy switches its position to long risk, and vice versa.
      \vskip1ex
      A single-period time lag is applied to the \emph{VWAP indicator}, so that the strategy trades immediately after the \emph{VWAP indicator} is evaluated at the end of the day
      \vskip1ex
      This assumption may be too optimistic because in practice it's difficult to trade immediately just before the close of markets.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VWAP indicator
indica_tor <- sign(cl_ose - v_wap)
# Calculate positions as lagged indicator
posi_tion <- rutils::lag_it(indica_tor)
# Calculate simple dollar VTI re_turns
re_turns <- rutils::diff_it(cl_ose)
# Calculate daily profits and losses of strategy
pnl_s <- re_turns*posi_tion
cum_pnls <- star_t + cumsum(pnl_s)
# Annualized Sharpe ratio of VWAP strategy
sqrt(252)*sum(pnl_s)/sd(pnl_s)/NROW(pnl_s)
# Annualized Sharpe ratio of VTI
sqrt(252)*sum(re_turns)/sd(re_turns)/NROW(pnl_s)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vwap_strat.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot prices and VWAP
chart_Series(x=cl_ose, name="VWAP Crossover Strategy for VTI", col="orange")
add_TA(cum_pnls, on=1, lwd=2, col="blue")
add_TA(posi_tion > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(posi_tion < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=c("VTI", "VWAP strategy"), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=c("orange", "blue"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trading at the \protect\emph{Open} Price}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A more realistic assumption is that the strategy trades at the \emph{Open} price next period.
      <<echo=TRUE,eval=FALSE>>=
# Determine dates right after VWAP has crossed prices
trade_dates <- (rutils::diff_it(indica_tor) != 0)
trade_dates <- which(trade_dates) + 1
# Calculate positions, either: -1, 0, or 1
posi_tion <- rep(NA_integer_, NROW(oh_lc))
posi_tion[1] <- 0
posi_tion[trade_dates] <- indica_tor[trade_dates-1]
posi_tion <- na.locf(posi_tion)
posi_tion <- xts(posi_tion, order.by=index(oh_lc))
pos_lagged <- rutils::lag_it(posi_tion)
# Calculate pnl for days without trade
pnl_s <- re_turns*posi_tion
# Calculate realized pnl for days with trade
close_lag <- rutils::lag_it(cl_ose)
pnl_s[trade_dates] <- pos_lagged[trade_dates] * 
  (op_en[trade_dates] - close_lag[trade_dates])
# Calculate unrealized pnl for days with trade
pnl_s[trade_dates] <- pnl_s[trade_dates] + 
  posi_tion[trade_dates] * 
  (cl_ose[trade_dates] - op_en[trade_dates])
cum_pnls <- star_t + cumsum(pnl_s)
# Annualized Sharpe ratio of VWAP strategy
sqrt(252)*sum(pnl_s)/sd(pnl_s)/NROW(pnl_s)
# Annualized Sharpe ratio of VTI
sqrt(252)*sum(re_turns)/sd(re_turns)/NROW(pnl_s)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vwap_strat_pnl_open.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot prices and VWAP
chart_Series(x=cl_ose, name="VWAP Crossover Strategy for VTI Trade at Open Price", col="orange")
add_TA(cum_pnls, on=1, lwd=2, col="blue")
add_TA(posi_tion > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(posi_tion < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=c("VTI", "VWAP strategy"), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=c("orange", "blue"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{EWMA} Price Technical Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Exponentially Weighted Moving Average Price} (\emph{EWMA}) is defined as the weighted average of prices over a rolling interval:
      \begin{displaymath}
        P_i^{EWMA} = (1-\exp(-\lambda)) \sum_{j=0}^{\infty} \exp(-\lambda j) P_{i-j}
      \end{displaymath}
      Where the decay parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with larger values of $\lambda$ producing faster decay, giving more weight to recent prices, and vice versa.
      <<echo=TRUE,eval=FALSE>>=
# Define length for weights and decay parameter
wid_th <- 352
lamb_da <- 0.01
# Calculate EWMA prices
weight_s <- exp(-lamb_da*1:wid_th)
weight_s <- weight_s/sum(weight_s)
ew_ma <- stats::filter(cl_ose, filter=weight_s, sides=1)
ew_ma[1:(wid_th-1)] <- ew_ma[wid_th]
ew_ma <- xts(cbind(cl_ose, ew_ma), order.by=index(oh_lc))
colnames(ew_ma) <- c("VTI", "VTI EWMA")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_indic.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA prices with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(ew_ma["2007/2010"], theme=plot_theme, 
             name="EWMA prices")
legend("bottomleft", legend=colnames(ew_ma), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating The \protect\emph{EWMA} Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a trend following \emph{EWMA Crossover} strategy, the risk position switches depending if the current price is above or below the \emph{EWMA}.
      \vskip1ex
      If the current price crosses above the \emph{EWMA}, then the strategy switches its risk position to a fixed unit of long risk, and if it crosses below, to a fixed unit of short risk.
      \vskip1ex
      The strategy holds the same position until the \emph{EWMA} crosses over the current price (either from above or below), and then it switches its position.
      \vskip1ex
      The strategy is therefore always either in a long risk, or in a short risk position.
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Determine dates right after VWAP has crossed prices
indica_tor <- sign(cl_ose - ew_ma[, 2])
trade_dates <- (rutils::diff_it(indica_tor) != 0)
trade_dates <- which(trade_dates) + 1
# Calculate positions, either: -1, 0, or 1
posi_tion <- rep(NA_integer_, NROW(oh_lc))
posi_tion[1] <- 0
posi_tion[trade_dates] <- indica_tor[trade_dates-1]
posi_tion <- na.locf(posi_tion)
posi_tion <- xts(posi_tion, order.by=index(oh_lc))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_strat.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA prices with position shading
chart_Series(ew_ma["2007/2010"], theme=plot_theme, 
             name="EWMA prices")
add_TA(posi_tion > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(posi_tion < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("bottomleft", legend=colnames(ew_ma), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating the Transaction Costs of Trading}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bid-offer spread} can be expressed as a percentage, equal to the difference between the \emph{offer} price minus the \emph{bid}, divided by the \emph{mid} price.
      \vskip1ex
      For institutional investors the \emph{bid-offer spread} is often estimated to be about \texttt{10} basis points (bps).
      \vskip1ex
      In reality the \emph{bid-offer spread} is not static and depends on many factors, such as market liquidity (trading volume), volatility, and time of day.
      \vskip1ex
      Broker commissions are an additional trading cost, but they depend on the size of the trades and on the type of investors, with institutional investors usually enjoying smaller commissions.
    \column{0.5\textwidth}
      <<echo=TRUE,eval=FALSE>>=
# bid_offer is equal to 10 bps for liquid ETFs
bid_offer <- 0.001
# Calculate open and lagged prices
op_en <- Op(oh_lc)
close_lag <- rutils::lag_it(cl_ose)
pos_lagged <- rutils::lag_it(posi_tion)
# Calculate the transaction cost for one share
cost_s <- 0.0*posi_tion
cost_s[trade_dates] <- 
  0.5*bid_offer*abs(pos_lagged[trade_dates] - 
  posi_tion[trade_dates])*op_en[trade_dates]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of \protect\emph{EWMA} Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The strategy trades at the \emph{Open} price on the next day after prices cross the \emph{EWMA}, since in practice it may not be possible to trade immediately.
      \vskip1ex
      The Profit and Loss (\emph{PnL}) on a trade date is the sum of the realized \emph{PnL} from closing the old position, plus the unrealized \emph{PnL} after opening the new position.
      <<echo=TRUE,eval=FALSE>>=
# Calculate daily profits and losses
# Calculate pnl for days without trade
pnl_s <- re_turns*posi_tion
# Calculate realized pnl for days with trade
close_lag <- rutils::lag_it(cl_ose)
pnl_s[trade_dates] <- pos_lagged[trade_dates] * 
  (op_en[trade_dates] - close_lag[trade_dates])
# Calculate unrealized pnl for days with trade
pnl_s[trade_dates] <- pnl_s[trade_dates] + 
  posi_tion[trade_dates] * 
  (cl_ose[trade_dates] - op_en[trade_dates])
# Annualized Sharpe ratio of EWMA strategy
sqrt(252)*sum(pnl_s)/sd(pnl_s)/NROW(pnl_s)
# Cumulative pnls
pnl_s <- star_t + cumsum(pnl_s)
pnl_s <- cbind(cl_ose, pnl_s)
colnames(pnl_s) <- c("VTI", "EWMA PnL")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_strat_pnl.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA PnL with position shading
chart_Series(pnl_s, theme=plot_theme, 
             name="Performance of EWMA Strategy")
add_TA(posi_tion > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(posi_tion < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=colnames(pnl_s), 
       inset=0.05, bg="white", lty=1, lwd=6, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Function for \protect\emph{EWMA} Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The \emph{EWMA} strategy can be simulated by a single function, which allows the analysis of its performance depending on its parameters,
      \vskip1ex
      The function \texttt{simu\_ewma()} performs a simulation of the \emph{EWMA} strategy, given an \emph{OHLC} time series of prices, and a decay parameter $\lambda$, 
      \vskip1ex
      The function \texttt{simu\_ewma()} returns the \emph{EWMA} strategy positions and returns, in a two-column \emph{xts} time series, 
    \column{0.6\textwidth}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
simu_ewma <- function(oh_lc, lamb_da=0.01, wid_th=251, bid_offer=0.001, tre_nd=1) {
  # Calculate EWMA prices
  weight_s <- exp(-lamb_da*1:wid_th)
  weight_s <- weight_s/sum(weight_s)
  cl_ose <- quantmod::Cl(oh_lc)
  ew_ma <- stats::filter(as.numeric(cl_ose), filter=weight_s, sides=1)
  ew_ma[1:(wid_th-1)] <- ew_ma[wid_th]
  # Determine dates right after EWMA has crossed prices
  indica_tor <- tre_nd*xts::xts(sign(as.numeric(cl_ose) - ew_ma), order.by=index(oh_lc))
  indicator_lag <- rutils::lag_it(indica_tor)
  trade_dates <- (rutils::diff_it(indica_tor) != 0)
  trade_dates <- which(trade_dates) + 1
  trade_dates <- trade_dates[trade_dates<NROW(oh_lc)]
  # Calculate positions, either: -1, 0, or 1
  posi_tion <- rep(NA_integer_, NROW(oh_lc))
  posi_tion[1] <- 0
  posi_tion[trade_dates] <- indicator_lag[trade_dates]
  posi_tion <- na.locf(posi_tion)
  posi_tion <- xts(posi_tion, order.by=index(oh_lc))
  op_en <- quantmod::Op(oh_lc)
  close_lag <- rutils::lag_it(cl_ose)
  pos_lagged <- rutils::lag_it(posi_tion)
  # Calculate transaction costs
  cost_s <- 0.0*posi_tion
  cost_s[trade_dates] <- 0.5*bid_offer*abs(pos_lagged[trade_dates] - posi_tion[trade_dates])*op_en[trade_dates]
  # Calculate daily profits and losses
  re_turns <- pos_lagged*(cl_ose - close_lag)
  re_turns[trade_dates] <- pos_lagged[trade_dates] * (op_en[trade_dates] - close_lag[trade_dates]) + posi_tion[trade_dates] * (cl_ose[trade_dates] - op_en[trade_dates]) - cost_s
  # Calculate strategy returns
  out_put <- cbind(posi_tion, re_turns)
  colnames(out_put) <- c("positions", "returns")
  out_put
}  # end simu_ewma
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Multiple Trend-following \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{EWMA} strategies can be simulated by calling the function \texttt{simu\_ewma()} in a loop over a vector of $\lambda$ parameters.
      \vskip1ex
      But \texttt{simu\_ewma()} returns an \emph{xts} time series, and \texttt{sapply()} cannot merge \emph{xts} time series together.
      \vskip1ex
      So instead the loop is performed using \texttt{lapply()} which returns a list of \emph{xts}, and the list is merged into a single \emph{xts} using functions \texttt{rutils::do\_call()} and \texttt{cbind()}.
      <<echo=TRUE,eval=FALSE>>=
source("C:/Develop/R/lecture_slides/scripts/ewma_model.R")
lamb_das <- seq(0.0001, 0.05, 0.005)
# Perform lapply() loop over lamb_das
pnl_s <- lapply(lamb_das, function(lamb_da) {
  # simulate EWMA strategy and calculate re_turns
  star_t + cumsum(simu_ewma(oh_lc=oh_lc, 
    lamb_da=lamb_da, wid_th=wid_th)[, "returns"])
})  # end lapply
pnl_s <- rutils::do_call(cbind, pnl_s)
colnames(pnl_s) <- paste0("lambda=", lamb_das)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_trend_returns.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA strategies with custom line colors
column_s <- seq(1, NCOL(pnl_s), by=3)
plot_theme <- chart_theme()
plot_theme$col$line.col <- 
  colorRampPalette(c("blue", "red"))(NROW(column_s))
chart_Series(pnl_s[, column_s], 
  theme=plot_theme, name="Cumulative Returns of EWMA Strategies")
legend("topleft", legend=colnames(pnl_s[, column_s]), 
  inset=0.1, bg="white", cex=0.8, lwd=rep(6, NCOL(pnl_s)), 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating \protect\emph{EWMA} Strategies Using Parallel Computing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Simulating \emph{EWMA} strategies naturally lends itself to parallel computing, since the simulations are independent from each other.
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs apply loops under \emph{Windows}, using parallel computing on several CPU cores.
      \vskip1ex
      The resulting list of time series can then be collapsed into a single \emph{xts} series using the functions \texttt{rutils::do\_call()} and \texttt{cbind()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# initialize compute cluster under Windows
library(parallel)
clus_ter <- makeCluster(detectCores()-1)
clusterExport(clus_ter, 
  varlist=c("oh_lc", "wid_th", "simu_ewma"))
# Perform parallel loop over lamb_das under Windows
pnl_s <- parLapply(clus_ter, lamb_das, function(lamb_da) {
  library(quantmod)
  # simulate EWMA strategy and calculate re_turns
  star_t + cumsum(simu_ewma(oh_lc=oh_lc, 
    lamb_da=lamb_da, wid_th=wid_th)[, "returns"])
})  # end parLapply
# Perform parallel loop over lamb_das under Mac-OSX or Linux
re_turns <- mclapply(lamb_das, function(lamb_da) {
  library(quantmod)
  # simulate EWMA strategy and calculate re_turns
  star_t + cumsum(simu_ewma(oh_lc=oh_lc, 
    lamb_da=lamb_da, wid_th=wid_th)[, "returns"])
})  # end mclapply
stopCluster(clus_ter)  # stop R processes over cluster under Windows
pnl_s <- rutils::do_call(cbind, pnl_s)
colnames(pnl_s) <- paste0("lambda=", lamb_das)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of Trend-following \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sharpe} ratios of \emph{EWMA} strategies with different $\lambda$ parameters can be calculated by performing an \texttt{sapply()} loop over the \emph{columns} of returns.
      \vskip1ex
      \texttt{sapply()} treats the columns of \emph{xts} time series as list elements, and loops over the columns.
      \vskip1ex
      Performing loops in \texttt{R} over the \emph{columns} of returns is acceptable, but \texttt{R} loops over the \emph{rows} of returns should be avoided.
      \vskip1ex
      The performance of trend following \emph{EWMA} strategies depends on the $\lambda$ parameter, with larger $\lambda$ parameters performing worse than smaller ones.
      <<echo=TRUE,eval=FALSE>>=
sharpe_ratios <- sqrt(252)*sapply(pnl_s, function(x_ts) {
  # Calculate annualized Sharpe ratio of strategy returns
  x_ts <- rutils::diff_it(log(x_ts))
  sum(x_ts)/sd(x_ts)
})/NROW(pnl_s)  # end sapply
plot(x=lamb_das, y=sharpe_ratios, t="l", 
     main="Performance of EWMA trend following strategies 
     as function of the decay parameter lambda")
trend_returns <- rutils::diff_it(log(pnl_s))
trend_sharpe <- sharpe_ratios
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_trend_performance.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Trend-following \protect\emph{EWMA} Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The best performing trend following \emph{EWMA} strategy has a relatively small $\lambda$ parameter, corresponding to slower weight decay (giving more weight to past prices), and producing less frequent trading.
      <<echo=TRUE,eval=FALSE>>=
# Simulate best performing strategy
ewma_trend <- simu_ewma(oh_lc=oh_lc, 
  lamb_da=lamb_das[which.max(sharpe_ratios)], 
  wid_th=wid_th)
posi_tion <- ewma_trend[, "positions"]
pnl_s <- star_t + cumsum(ewma_trend[, "returns"])
pnl_s <- cbind(cl_ose, pnl_s)
colnames(pnl_s) <- c("VTI", "EWMA PnL")
# Plot EWMA PnL with position shading
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(pnl_s, theme=plot_theme, 
             name="Performance of Trend-following EWMA Strategy")
add_TA(posi_tion > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(posi_tion < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=colnames(pnl_s), 
  inset=0.05, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_trend_best.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Multiple Mean reverting \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{EWMA} strategies can be backtested by calling the function \texttt{simu\_ewma()} in a loop over a vector of $\lambda$ parameters.
      \vskip1ex
      But \texttt{simu\_ewma()} returns an \emph{xts} time series, and \texttt{sapply()} cannot merge \emph{xts} time series together.
      \vskip1ex
      So instead the loop is performed using \texttt{lapply()} which returns a list of \emph{xts}, and the list is merged into a single \emph{xts} using functions \texttt{rutils::do\_call()} and \texttt{cbind()}.
      <<echo=TRUE,eval=FALSE>>=
source("C:/Develop/R/lecture_slides/scripts/ewma_model.R")
lamb_das <- seq(0.05, 1.0, 0.05)
# Perform lapply() loop over lamb_das
pnl_s <- lapply(lamb_das, function(lamb_da) {
  # backtest EWMA strategy and calculate re_turns
  star_t + cumsum(simu_ewma(
    oh_lc=oh_lc, lamb_da=lamb_da, wid_th=wid_th, tre_nd=(-1))[, "returns"])
})  # end lapply
pnl_s <- rutils::do_call(cbind, pnl_s)
colnames(pnl_s) <- paste0("lambda=", lamb_das)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_revert_returns.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA strategies with custom line colors
column_s <- seq(1, NCOL(pnl_s), by=4)
plot_theme <- chart_theme()
plot_theme$col$line.col <- 
  colorRampPalette(c("blue", "red"))(NROW(column_s))
chart_Series(pnl_s[, column_s], 
  theme=plot_theme, name="Cumulative Returns of Mean reverting EWMA Strategies")
legend("topleft", legend=colnames(pnl_s[, column_s]), 
  inset=0.1, bg="white", cex=0.8, lwd=rep(6, NCOL(pnl_s)), 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of Mean reverting \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sharpe} ratios of \emph{EWMA} strategies with different $\lambda$ parameters can be calculated by performing an \texttt{sapply()} loop over the \emph{columns} of returns.
      \vskip1ex
      \texttt{sapply()} treats the columns of \emph{xts} time series as list elements, and loops over the columns.
      \vskip1ex
      Performing loops in \texttt{R} over the \emph{columns} of returns is acceptable, but \texttt{R} loops over the \emph{rows} of returns should be avoided.
      \vskip1ex
      The performance of mean reverting \emph{EWMA} strategies depends on the $\lambda$ parameter, with performance decreasing for very small or very large $\lambda$ parameters.
      \vskip1ex
      For too large $\lambda$ parameters, the trading frequency is too high, causing high transaction costs.
      \vskip1ex
      For too small $\lambda$ parameters, the trading frequency is too low, causing the strategy to miss profitable trades.
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_revert_performance.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
sharpe_ratios <- sqrt(252)*sapply(pnl_s, function(x_ts) {
  # Calculate annualized Sharpe ratio of strategy returns
  x_ts <- rutils::diff_it(log(x_ts))
  sum(x_ts)/sd(x_ts)
})/NROW(pnl_s)  # end sapply
plot(x=lamb_das, y=sharpe_ratios, t="l", 
     main="Performance of EWMA mean reverting strategies 
     as function of the decay parameter lambda")
revert_returns <- rutils::diff_it(log(pnl_s))
revert_sharpe <- sharpe_ratios
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Mean reverting \protect\emph{EWMA} Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Reverting the rules of the trend following \emph{EWMA} strategy creates a mean reverting strategy.
      \vskip1ex
      The best performing mean reverting \emph{EWMA} strategy has a relatively large $\lambda$ parameter, corresponding to faster weight decay (giving more weight to recent prices), and producing more frequent trading.
      \vskip1ex
      But a too large $\lambda$ parameter also causes very high trading frequency, and high transaction costs.
      <<echo=TRUE,eval=FALSE>>=
# backtest best performing strategy
ewma_revert <- simu_ewma(oh_lc=oh_lc, 
  lamb_da=lamb_das[which.max(sharpe_ratios)],
  wid_th=wid_th, tre_nd=(-1))
posi_tion <- ewma_revert[, "positions"]
pnl_s <- star_t + cumsum(ewma_revert[, "returns"])
pnl_s <- cbind(cl_ose, pnl_s)
colnames(pnl_s) <- c("VTI", "EWMA PnL")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_revert_best.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA PnL with position shading
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(pnl_s, theme=plot_theme, 
             name="Performance of Mean reverting EWMA Strategy")
add_TA(posi_tion > 0, on=-1,
       col="lightgreen", border="lightgreen")
add_TA(posi_tion < 0, on=-1,
       col="lightgrey", border="lightgrey")
legend("top", legend=colnames(pnl_s), 
  inset=0.05, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Combining Trend-following and Mean reverting Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The returns of trend following and mean reverting strategies are usually negatively correlated to each other, so combining them can achieve significant diversification of risk.
      <<echo=TRUE,eval=FALSE>>=
# Calculate correlation between trend following and mean reverting strategies
trend_ing <- ewma_trend[, "returns"]
colnames(trend_ing) <- "trend"
revert_ing <- ewma_revert[, "returns"]
colnames(revert_ing) <- "revert"
close_rets <- rutils::diff_it(log(cl_ose))
cor(cbind(trend_ing, revert_ing, close_rets))
# Calculate combined strategy
com_bined <- trend_ing + revert_ing
colnames(com_bined) <- "combined"
# Calculate annualized Sharpe ratio of strategy returns
re_turns <- cbind(close_rets, trend_ing, revert_ing, com_bined)
sqrt(252)*sapply(re_turns, function(x_ts) 
  sum(x_ts)/sd(x_ts))/NROW(com_bined)
pnl_s <- lapply(re_turns, function(x_ts) {star_t + cumsum(x_ts)})
pnl_s <- rutils::do_call(cbind, pnl_s)
colnames(pnl_s) <- c("VTI", "trending", "reverting", "EWMA combined PnL")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_combined.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue", "green", "magenta2")
chart_Series(pnl_s, theme=plot_theme, 
             name="Performance of Combined EWMA Strategies")
legend("topleft", legend=colnames(pnl_s),
       inset=0.05, bg="white", lty=1, lwd=6, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Ensemble of \protect\emph{EWMA} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Instead of selecting the best performing \emph{EWMA} strategy, one can choose a weighted average of strategies (ensemble), which corresponds to allocating positions according to the weights.
      \vskip1ex
      The weights can be chosen to be proportional to the Sharpe ratios of the \emph{EWMA} strategies.
      <<echo=TRUE,eval=FALSE>>=
sharpe_ratios <- c(trend_sharpe, revert_sharpe)
weight_s <- sharpe_ratios
weight_s[weight_s<0] <- 0
weight_s <- weight_s/sum(weight_s)
re_turns <- cbind(trend_returns, revert_returns)
avg_returns <- re_turns %*% weight_s
avg_returns <- xts(avg_returns, order.by=index(re_turns))
pnl_s <- (star_t + cumsum(avg_returns))
pnl_s <- cbind(cl_ose, pnl_s)
colnames(pnl_s) <- c("VTI", "EWMA PnL")
# Plot EWMA PnL without position shading
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(pnl_s, theme=plot_theme, 
  name="Performance of Ensemble EWMA Strategy")
legend("top", legend=colnames(pnl_s), 
  inset=0.05, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ewma_ensemble.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Backtesting Active Investment Strategies}


%%%%%%%%%%%%%%%
\subsection{Aggregations Over Look-back and Look-forward Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An example of data aggregations are the past cumulative returns of the \emph{EWMA} strategies calculated over a vector of \emph{end points}. 
      \vskip1ex
      Overlapping aggregations can be specified by a vector of \emph{look-back} intervals attached at \emph{end points}.
      \vskip1ex
      For example, we may specify aggregations at monthly \emph{end points}, over overlapping 12-month \emph{look-back} intervals.
      \vskip1ex
      The variable \texttt{look\_back} is equal to the number of \emph{end points} in the \emph{look-back} interval.
      \vskip1ex
      The \emph{start points} are the \emph{end points} lagged by the length of the \emph{look-back} interval.
      \vskip1ex
      The \emph{look-back} intervals are spanned by the vectors of \emph{start points} and \emph{end points}.
      \vskip1ex
      Non-overlapping aggregations can also be calculated over a list of \emph{look-forward} intervals (\texttt{look\_fwds}).
      \vskip1ex
      The \emph{look-back} intervals should not overlap with the \emph{look-forward} intervals, in order to avoid data snooping.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define end of month end_points
end_points <- rutils::calc_endpoints(re_turns, 
                inter_val="months")
n_rows <- NROW(end_points)
# Start_points equal end_points lagged by 12-month look-back interval
look_back <- 12
start_points <- c(rep_len(1, look_back-1), 
  end_points[1:(n_rows-look_back+1)])
# Calculate past performance over end_points 
perform_ance <-
  function(re_turns) sum(re_turns)/sd(re_turns)
past_perf <- sapply(1:(n_rows-1), function(it_er) {
  sapply(re_turns[start_points[it_er]:end_points[it_er]], perform_ance)
})  # end sapply
past_perf <- t(past_perf)
fut_rets <- sapply(1:(n_rows-1), function(it_er) {
  sapply(re_turns[(end_points[it_er]+1):end_points[it_er+1]], sum)
})  # end sapply
fut_rets <- t(fut_rets)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Momentum Portfolio Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In \emph{momentum} strategies, the portfolio weights are adjusted over time to be proportional to the past performance of the assets.
      \vskip1ex
      This way \emph{momentum} strategies switch their weights to the best performing assets.
      \vskip1ex
      The weights are scaled to limit the portfolio \emph{leverage} and its market \emph{beta}.
      \vskip1ex
      The portfolio weights of \emph{momentum} strategies can be scaled in several different ways.
      \vskip1ex
      To limit the portfolio leverage, the weights can be scaled so that the sum of their absolute values (or their squares) is equal to $1$: $\sum_{i=1}^n {w_i^2} = 1$
      \vskip1ex
      The weights can also be de-meaned, so that their sum is equal to zero, to create long-short portfolios with small betas.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate weight_s proportional to past_perf
weight_s <- past_perf
weight_s[weight_s<0] <- 0
# scale weight_s so their sum is equal to 1
weight_s <- weight_s/rowSums(weight_s)
# set NA values to zero
weight_s[is.na(weight_s)] <- 0
sum(is.na(weight_s))


# Calculate ETF prices and simple returns
sym_bols <- c("VTI", "IEF", "DBC")
price_s <- rutils::etf_env$price_s[, sym_bols]
price_s <- zoo::na.locf(price_s)
price_s <- na.omit(price_s)
re_turns <- rutils::diff_it(price_s)
# Define look-back and look-forward intervals
end_points <- rutils::calc_endpoints(re_turns, 
  inter_val="months")
n_cols <- NCOL(re_turns)
n_rows <- NROW(end_points)
look_back <- 12
start_points <- c(rep_len(1, look_back-1), 
  end_points[1:(n_rows-look_back+1)])
# Calculate past performance over end_points
perform_ance <- 
  function(re_turns) sum(re_turns)/sd(re_turns)
agg_s <- sapply(1:(n_rows-1), function(it_er) {
  c(past_perf=sapply(re_turns[start_points[it_er]:end_points[it_er]], perform_ance),
    fut_rets=sapply(re_turns[(end_points[it_er]+1):end_points[it_er+1]], sum))
})  # end sapply
agg_s <- t(agg_s)
# Select look-back and look-forward aggregations
past_perf <- agg_s[, 1:n_cols]
fut_rets <- agg_s[, n_cols+1:n_cols]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In \emph{momentum} strategies, the portfolio weights are adjusted over time to be proportional to the past performance of the assets.
      \vskip1ex
      This way \emph{momentum} strategies switch their weights to the best performing assets.
      \vskip1ex
      The weights are scaled to limit the portfolio \emph{leverage} and its market \emph{beta}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate weight_s proportional to past_perf
weight_s <- past_perf
weight_s[weight_s<0] <- 0
# scale weight_s so their sum is equal to 1
weight_s <- weight_s/rowSums(weight_s)
# set NA values to zero
weight_s[is.na(weight_s)] <- 0
sum(is.na(weight_s))
in_dex <- index(re_turns[end_points[-n_rows]])
trend_weights <- rowMeans(weight_s[, 1:NCOL(trend_returns)])
revert_weights <- rowMeans(weight_s[, -(1:NCOL(trend_returns))])
diff_weights <- xts(trend_weights-revert_weights, order.by=in_dex)
# Find best and worst EWMA Strategies in each period
bes_t <- apply(weight_s, 1, which.max)
wors_t <- apply(weight_s, 1, which.min)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_diff_weights.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot the mean weights of EWMA Strategies
zoo::plot.zoo(cbind(diff_weights, 
  cl_ose[end_points[-n_rows]]), 
  oma = c(3, 0, 3, 0), mar = c(0, 4, 0, 1), 
  xlab=NULL, ylab=c("diff weights", "VTI"), 
  main="Trend minus Revert Weights of EWMA strategies")
best_worst <- xts(cbind(bes_t, wors_t), order.by=in_dex)
zoo::plot.zoo(best_worst,
  oma = c(3, 0, 3, 0), mar = c(0, 4, 0, 1), 
  xlab=NULL, ylab=c("best EWMA", "worst EWMA"), 
  main="Best and Worst EWMA strategies")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting the \protect\emph{EWMA} \protect\emph{Momentum} Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Backtesting} is the testing of the accuracy of a forecasting model using simulation on historical data.
      \vskip1ex
      \emph{Backtesting} is a type of \emph{cross-validation} applied to time series data.
      \vskip1ex
      \emph{Backtesting} is performed by \emph{training} the model on past data and \emph{testing} it on future out-of-sample data.
      \vskip1ex
      The \emph{training} data is specified by the \emph{look-back} intervals (\texttt{past\_aggs}), and the model forecasts are applied to the future data defined by the \emph{look-forward} intervals (\texttt{fwd\_rets}).
      \vskip1ex
      The out-of-sample \emph{momentum} strategy returns can be calculated by multiplying the \texttt{fwd\_rets} by the forecast \emph{ETF} portfolio weights.
      <<echo=TRUE,eval=FALSE>>=
# Calculate backtest returns
pnl_s <- rowSums(weight_s*fut_rets)
pnl_s <- xts(pnl_s, order.by=in_dex)
colnames(pnl_s) <- "ewma momentum"
close_rets <- rutils::diff_it(cl_ose[in_dex])
cor(cbind(pnl_s, close_rets))
pnl_s <- star_t + cumsum(pnl_s)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/backtest_ewma.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot the backtest
chart_Series(x=cl_ose[end_points[-n_rows]], 
  name="backtest of EWMA strategies", col="orange")
add_TA(pnl_s, on=1, lwd=2, col="blue")
legend("top", legend=c("VTI", "EWMA"), 
       inset=0.1, bg="white", lty=1, lwd=6, 
       col=c("orange", "blue"), bty="n")
# shad_e <- xts(index(pnl_s) < as.Date("2008-01-31"), order.by=index(pnl_s))
# add_TA(shad_e, on=-1, col="lightgrey", border="lightgrey")
# text(x=7, y=0, labels="warmup period")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy for an \protect\emph{ETF} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{momentum} strategy can be \emph{backtested} for a portfolio of \emph{ETFs} or stocks over a series of \emph{end points} as follows:
      \setlength{\leftmargini}{1.0em}
      \begin{enumerate}
        \item Trade a single unit (dollar) of capital, 
        \item Allocate capital to the assets in proportion to their \emph{relative} past performance, 
        \item Calculate the portfolio weights as the number of units (shares) of each asset,
        \item At each \emph{end point} repeat the above and rebalance the asset allocations,
        \item Calculate the future out-of-sample portfolio returns in each period (without reinvestment).
        \item Calculate the transaction costs (as the bid-offer spread times the asset prices, times the change in weights), and subtract them from the returns.
      \end{enumerate}
      The above points \texttt{\#1, \#2} and \texttt{\#3} represent the \emph{momentum} strategy, while points \texttt{\#4, \#5} and \texttt{\#6} represent the \emph{backtest} procedures.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate ETF prices and simple returns
sym_bols <- c("VTI", "IEF", "DBC")
price_s <- rutils::etf_env$price_s[, sym_bols]
price_s <- na.omit(zoo::na.locf(price_s))
re_turns <- rutils::diff_it(price_s)
# Define look-back and look-forward intervals
end_points <- rutils::calc_endpoints(re_turns, 
  inter_val="months")
n_cols <- NCOL(re_turns)
n_rows <- NROW(end_points)
look_back <- 12
start_points <- c(rep_len(1, look_back-1), 
  end_points[1:(n_rows-look_back+1)])
# Calculate past performance over end_points
perform_ance <- 
  function(re_turns) sum(re_turns)/sd(re_turns)
agg_s <- sapply(1:(n_rows-1), function(it_er) {
  c(past_perf=sapply(re_turns[start_points[it_er]:end_points[it_er]], perform_ance),
    fut_rets=sapply(re_turns[(end_points[it_er]+1):end_points[it_er+1]], sum))
})  # end sapply
agg_s <- t(agg_s)
# Select look-back and look-forward aggregations
past_perf <- agg_s[, 1:n_cols]
fut_rets <- agg_s[, n_cols+1:n_cols]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting the \protect\emph{Momentum} Strategy for an \protect\emph{ETF} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The hypothetical \emph{momentum} strategy returns can be calculated by multiplying the forecast portfolio weights times the forward (future) out-of-sample returns.
      <<echo=TRUE,eval=FALSE>>=
# Calculate portfolio weights equal to number of shares
end_prices <- price_s[end_points[-n_rows]]
weight_s <- 
  past_perf/rowSums(abs(past_perf))/end_prices
weight_s[is.na(weight_s)] <- 0
colnames(weight_s) <- colnames(re_turns)
# Calculate profits and losses
pnl_s <- rowSums(weight_s*fut_rets)
pnl_s <- xts(pnl_s, index(end_prices))
colnames(pnl_s) <- "pnls"
# Calculate transaction costs
bid_offer <- 0.001
cost_s <- 
  0.5*bid_offer*end_prices*abs(rutils::diff_it(weight_s))
cost_s <- rowSums(cost_s)
pnl_s <- (pnl_s - cost_s)
pnl_s <- cumsum(pnl_s)
# Plot momentum strategy with VTI
cl_ose <- price_s[index(end_prices), "VTI"]
zoo::plot.zoo(cbind(cl_ose, pnl_s, weight_s), 
  oma = c(3, 1, 3, 0), mar = c(0, 4, 0, 1), nc=1,
  xlab=NULL, main="ETF Momentum Strategy")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_weights.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Functional for \protect\emph{Momentum} Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# Define backtest functional
backtest_ep <- function(re_turns, price_s, perform_ance=sum, 
    look_back=12, re_balance="months", bid_offer=0.001,
    end_points=rutils::calc_endpoints(re_turns, inter_val=re_balance), 
    with_weights=FALSE, ...) {
  stopifnot("package:quantmod" %in% search() || require("quantmod", quietly=TRUE))
  # Define look-back and look-forward intervals
  n_cols <- NCOL(re_turns)
  n_rows <- NROW(end_points)
  start_points <- c(rep_len(1, look_back-1), end_points[1:(n_rows-look_back+1)])
  # Calculate past performance over end_points
  agg_s <- sapply(1:(n_rows-1), function(it_er) {
    c(past_perf=sapply(re_turns[start_points[it_er]:end_points[it_er]], perform_ance, ...),  # end sapply
    fut_rets=sapply(re_turns[(end_points[it_er]+1):end_points[it_er+1]], sum))  # end sapply
  })  # end sapply
  agg_s <- t(agg_s)
  # Select look-back and look-forward aggregations
  past_perf <- agg_s[, 1:n_cols]
  fut_rets <- agg_s[, n_cols+1:n_cols]
  # Calculate portfolio weights equal to number of shares
  end_prices <- price_s[end_points[-n_rows]]
  weight_s <- past_perf/rowSums(abs(past_perf))/end_prices
  weight_s[is.na(weight_s)] <- 0
  colnames(weight_s) <- colnames(re_turns)
  # Calculate profits and losses
  pnl_s <- rowSums(weight_s*fut_rets)
  pnl_s <- xts(pnl_s, index(end_prices))
  colnames(pnl_s) <- "pnls"
  # Calculate transaction costs
  cost_s <- 0.5*bid_offer*end_prices*abs(rutils::diff_it(weight_s))
  cost_s <- rowSums(cost_s)
  pnl_s <- (pnl_s - cost_s)
  pnl_s <- cumsum(pnl_s)
  if (with_weights)
    cbind(pnl_s, weight_s)
  else
    pnl_s
}  # end backtest_ep
      @
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimization of \protect\emph{Momentum} Strategy Parameters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The performance of the \emph{momentum} strategy depends on the length of the \emph{look-back interval} used for calculating the past performance.
      \vskip1ex
      Performing a \emph{backtest} allows finding the optimal \emph{momentum} (trading) strategy parameters, such as the \emph{look-back interval}.
      \vskip1ex
      But using a different rebalancing frequency in the \emph{backtest} can produce different values for the optimal trading strategy parameters.
      \vskip1ex
      So \emph{backtesting} just redefines the problem of finding (tuning) the optimal trading strategy parameters, into the problem of finding the optimal \emph{backtest} (meta-model) parameters.
      \vskip1ex
      But the advantage of using the \emph{backtest} meta-model is that it can reduce the number of parameters that need to be optimized.
      \vskip1ex
      Performing many \emph{backtests} on multiple trading strategies risks identifying inherently unprofitable trading strategies as profitable, purely by chance (\emph{p-value hacking}).
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/look_back_profile.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
source("C:/Develop/R/lecture_slides/scripts/back_test.R")
look_backs <- seq(5, 60, by=5)
perform_ance <- function(re_turns) sum(re_turns)/sd(re_turns)
pro_files <- sapply(look_backs, function(x) {
  last(backtest_ep(re_turns=re_turns, price_s=price_s, 
    re_balance="weeks", look_back=x, perform_ance=perform_ance))
})  # end sapply
plot(x=look_backs, y=pro_files, t="l", 
  main="Strategy PnL as function of look_back", 
  xlab="look_back (weeks)", ylab="pnl")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy Performance}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The hypothetical out-of-sample \emph{momentum} strategy returns can be calculated by multiplying the \texttt{fwd\_rets} by the forecast \emph{ETF} portfolio weights.
      \vskip1ex
      The \emph{training} data is specified by the \emph{look-back} intervals (\texttt{back\_aggs}), and the forecasts are applied to the future data defined by the \emph{look-forward} intervals (\texttt{fwd\_rets}).
      <<echo=TRUE,eval=FALSE>>=
look_back <- look_backs[which.max(pro_files)]
pnl_s <- backtest_ep(re_turns=re_turns, price_s=price_s, 
  re_balance="weeks", look_back=look_back, perform_ance=perform_ance,
  with_weights=TRUE)
cl_ose <- Cl(rutils::etf_env$VTI[index(pnl_s)])
# bind model returns with VTI
da_ta <- star_t
da_ta <- cbind(cl_ose, da_ta*pnl_s[, 1]+da_ta)
colnames(da_ta) <- c("VTI", "momentum")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_backtest.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot momentum strategy with VTI
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue")
chart_Series(da_ta, theme=plot_theme, lwd=2, 
             name="Momentum PnL")
legend("topleft", legend=colnames(da_ta), 
  inset=0.1, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Combining the \protect\emph{Momentum} and Static Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{momentum} strategy has attractive returns compared to a static buy-and-hold strategy.
      \vskip1ex
      But the \emph{momentum} strategy suffers from draw-downs called \emph{momentum crashes}, especially after the market rallies from a sharp-sell-off.
      \vskip1ex
      This suggests that combining the \emph{momentum} strategy with a static buy-and-hold strategy can achieve significant diversification of risk.
      <<echo=TRUE,eval=FALSE>>=
# combine momentum strategy with static
da_ta <- cbind(da_ta, 0.5* (da_ta[, "VTI"] + da_ta[, "momentum"]))
colnames(da_ta) <- c("VTI", "momentum", "combined")
# Calculate strategy annualized Sharpe ratios
sapply(da_ta, function(cumu_lative) {
  x_ts <- na.omit(diff(log(cumu_lative)))
  sqrt(52)*sum(x_ts)/sd(x_ts)/NROW(x_ts)
})  # end sapply
# Calculate strategy correlations
cor(na.omit(diff(log(da_ta))))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_combined.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot momentum strategy combined with VTI
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue", "green")
chart_Series(da_ta, theme=plot_theme, 
             name="Momentum strategy combined with VTI")
legend("topleft", legend=colnames(da_ta), 
  inset=0.1, bg="white", lty=1, lwd=6, 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy Versus the \protect\emph{All-Weather} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{All-Weather} portfolio is a static portfolio of stocks (30\%), bonds (55\%), and commodities and precious metals (15\%) (approximately), and was designed by Bridgewater Associates, the largest hedge fund in the world:\\
      \url{https://www.bridgewater.com/research-library/the-all-weather-strategy/}
      \url{http://www.nasdaq.com/article/remember-the-allweather-portfolio-its-having-a-killer-year-cm685511}
      \vskip1ex
      The three different asset classes (stocks, bonds, commodities) provide positive returns under different economic conditions (recession, expansion, inflation).
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define all-weather symbols and weights
weight_s <- c(0.30, 0.55, 0.15)
all_weather <- (re_turns / price_s) %*% weight_s
all_weather <- cumsum(all_weather)
all_weather <- xts(all_weather, index(re_turns))[index(pnl_s)]
all_weather <- star_t*all_weather + 
  star_t
colnames(all_weather) <- "all_weather"
# combine momentum strategy with all-weather
da_ta <- cbind(da_ta, all_weather)
# Calculate strategy annualized Sharpe ratios
sapply(da_ta, function(cumu_lative) {
  x_ts <- na.omit(diff(log(cumu_lative)))
  sqrt(52)*sum(x_ts)/sd(x_ts)/NROW(x_ts)
})  # end sapply
# Calculate strategy correlations
cor(na.omit(diff(log(da_ta))))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_all_weather.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot momentum strategy, combined, and all-weather
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue", "green", "violet")
chart_Series(da_ta, theme=plot_theme, lwd=2, name="Momentum PnL")
legend("topleft", legend=colnames(da_ta),
  inset=0.1, bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
      \vspace{-1em}
      The combination of bonds, stocks, and commodities in the \emph{All-Weather} portfolio is designed to provide positive returns under most economic conditions, without the costs of trading.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy Market Beta}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{momentum} strategy market beta can be calculated by multiplying the \emph{ETF} betas by the \emph{ETF} portfolio weights.
      <<echo=TRUE,eval=FALSE>>=
# Calculate betas
beta_s <- c(1, rutils::etf_env$capm_stats[
  match(sym_bols[-1], 
        rownames(rutils::etf_env$capm_stats)), 
  "Beta"])
names(beta_s)[1] <- sym_bols[1]
# weights times betas
weight_s <- price_s[index(pnl_s)]*pnl_s[, -1]
beta_s <- weight_s %*% beta_s
beta_s <- xts(beta_s, order.by=index(weight_s))
colnames(beta_s) <- "portf_beta"
zoo::plot.zoo(cbind(beta_s, cl_ose),
  oma = c(3, 1, 3, 0), mar = c(0, 4, 0, 1), 
  main="betas & VTI", xlab="")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_betas.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{market timing} skill can be measured by performing a \emph{linear regression} of a strategy's returns against a strategy with perfect \emph{market timing} skill.
      \vskip1ex
      The \emph{Merton-Henriksson} market timing test uses a linear \emph{market timing} term:
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + \gamma \max{(0, R_m - R_f)} + {\varepsilon}
      \end{displaymath}
      Where $R$ are the strategy returns, $R_m$ are the market returns, and $R_f$ are the risk-free returns.
      \vskip1ex
      If the coefficient $\gamma$ is statistically significant, then it's very likely due to \emph{market timing} skill.
      \vskip1ex
      The \emph{market timing} regression is a generalization of the \emph{Capital Asset Pricing Model}.
      \vskip1ex
      The \emph{Treynor-Mazuy} test uses a quadratic term, which makes it more sensitive to the magnitude of returns:
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + \gamma (R_m - R_f)^2 + {\varepsilon}
      \end{displaymath}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_timing.png}
    \vspace{-2em}
      <<echo=(-(1:4)),eval=FALSE>>=
# open x11 for plotting
x11(width=6, height=4)
# set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 3, 1), oma=c(0, 0, 0, 0))
momentum_rets <- as.numeric(rutils::diff_it(pnl_s[, 1]))
vti_rets <- as.numeric(rutils::diff_it(cl_ose)/100)
# Merton-Henriksson test
vti_b <- cbind(VTI=vti_rets, skill=0.5*(vti_rets+abs(vti_rets)))
mod_el <- lm(momentum_rets ~ vti_b); summary(mod_el)
# Treynor-Mazuy test
vti_b <- cbind(VTI=vti_rets, skill=vti_rets^2)
mod_el <- lm(momentum_rets ~ vti_b); summary(mod_el)
# Plot scatterplot
plot(x=vti_rets, y=momentum_rets, xlab="VTI", ylab="momentum")
title(main="Treynor-Mazuy market timing test\n for Momentum vs VTI", line=0.5)
# Plot fitted (predicted) response values
points(x=vti_rets, y=mod_el$fitted.values, pch=16, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy Returns Skew}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The distribution of returns of the \emph{momentum} strategy has about one half of the negative skew compared to \emph{VTI}.
      \vskip1ex
      The \emph{momentum} strategy indicates the existence of a market anomaly (outsized profits), because it has a smaller negative skew than the market, while having comparable returns to the market.
      <<echo=TRUE,eval=FALSE>>=
# Normalize the returns
momentum_rets <- 
  (momentum_rets-mean(momentum_rets))
momentum_rets <- 
  sd(vti_rets)*momentum_rets/sd(momentum_rets)
vti_rets <- (vti_rets-mean(vti_rets))
# Calculate ratios of moments
sapply(2:4, FUN=moments::moment, x=vti_rets)/
  sapply(2:4, FUN=moments::moment, x=momentum_rets)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/momentum_distr.png}
    \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot histogram
x_lim <- 4*sd(momentum_rets)
hist(momentum_rets, breaks=30, 
  main="Momentum and VTI Return Distributions", 
  xlim=c(-x_lim, x_lim), 
  xlab="", ylab="", freq=FALSE)
# draw kernel density of histogram
lines(density(momentum_rets), col='red', lwd=2)
lines(density(vti_rets), col='blue', lwd=2)
# add legend
legend("topright", inset=0.05, cex=0.8, title=NULL,
       leg=c("Momentum", "VTI"),
       lwd=6, bg="white", col=c("red", "blue"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{rolling portfolio optimization} strategy consists of rebalancing a portfolio over a vector of end points: 
      \setlength{\leftmargini}{1.0em}
      \begin{enumerate}
        \item Calculate the maximum Sharpe ratio portfolio weights at each end point.
        \item Apply the weights in the next interval and calculate the out-of-sample portfolio returns.
      \end{enumerate}
      The parameters of this strategy are:
      \begin{enumerate}
        \item Rebalancing frequency (annual, monthly, etc.)
        \item Length of look-back interval (sliding or expanding).
        \item Scaling of weights (sum or sum-of-squares).
      \end{enumerate}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# sym_bols contains all the symbols in rutils::etf_env$re_turns except "VXX" and "SVXY"
sym_bols <- colnames(rutils::etf_env$re_turns)
sym_bols <- sym_bols[!((sym_bols == "VXX")|(sym_bols == "SVXY"))]
# Extract columns of rutils::etf_env$re_turns and remove NA values
re_turns <- rutils::etf_env$re_turns[, sym_bols]
re_turns <- na.omit(zoo::na.locf(re_turns))
# Calculate vector of monthly end points and start points
look_back <- 12
end_points <- rutils::calc_endpoints(re_turns, inter_val="months")
end_points[end_points<2*NCOL(re_turns)] <- 2*NCOL(re_turns)
      @
    \column{0.5\textwidth}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
n_rows <- NROW(end_points)
# sliding window
start_points <- c(rep_len(1, look_back-1), end_points[1:(n_rows-look_back+1)])
# OR expanding window
# start_points <- rep_len(1, NROW(end_points))
# risk_free is the daily risk-free rate
risk_free <- 0.03/252
# Calculate daily excess returns 
ex_cess <- re_turns - risk_free
# Perform loop over end_points
portf_rets <- lapply(2:NROW(end_points),
  function(i) {
    # Subset the ex_cess returns
    ex_cess <- ex_cess[start_points[i-1]:end_points[i-1], ]
    in_verse <- solve(cov(ex_cess))
    # Calculate the maximum Sharpe ratio portfolio weights
    weight_s <- in_verse %*% colMeans(ex_cess)
    weight_s <- drop(weight_s/sqrt(sum(weight_s^2)))
    # Subset the re_turns
    re_turns <- re_turns[(end_points[i-1]+1):end_points[i], ]
    # Calculate the out-of-sample portfolio returns
    xts(re_turns %*% weight_s, index(re_turns))
  }  # end anonymous function
)  # end lapply
portf_rets <- rutils::do_call(rbind, portf_rets)
colnames(portf_rets) <- "portf_rets"
# Calculate compounded cumulative portfolio returns
portf_rets <- cumsum(portf_rets)
quantmod::chart_Series(portf_rets,
  name="Cumulative Returns of Max Sharpe Portfolio Strategy")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} Stock Index Constituent Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{S\&P500} stock index constituent data is of poor quality before \texttt{2000}, so we'll mostly use the data after \texttt{2000}.
      <<echo=TRUE,eval=FALSE>>=
# Load S&P500 constituent stock prices
load("C:/Develop/R/lecture_slides/data/sp500.RData")
price_s <- eapply(env_sp500, quantmod::Cl)
price_s <- rutils::do_call(cbind, price_s)
# carry forward and backward non-NA prices
price_s <- zoo::na.locf(price_s)
price_s <- zoo::na.locf(price_s, fromLast=TRUE)
colnames(price_s) <- sapply(colnames(price_s),
  function(col_name) strsplit(col_name, split="[.]")[[1]][1])
# Calculate percentage returns of the S&P500 constituent stocks
re_turns <- rutils::diff_it(log(price_s))
returns_100 <- re_turns[, sample(NCOL(re_turns), s=100, replace=FALSE)]
save(price_s, re_turns, returns_100, 
  file="C:/Develop/R/lecture_slides/data/sp500_prices.RData")
# Calculate number of constituents without prices
da_ta <- rowSums(rutils::roll_sum(re_turns, 4)==0)
da_ta <- xts::xts(da_ta, order.by=index(re_turns))
dygraphs::dygraph(da_ta, main="Number of S&P 500 Constituents Without Prices") %>%
  dyAxis("y", valueRange=c(0, 300))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/sp500_without_prices.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} Stock Portfolio Index}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The price-weighted index of \emph{S\&P500} constituents closely follows the VTI \emph{ETF}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate price weighted index of constituent
n_cols <- NCOL(price_s)
in_dex <- xts(rowSums(price_s)/n_cols, index(price_s))
colnames(in_dex) <- "index"
# Combine index with VTI
da_ta <- cbind(in_dex[index(etf_env$VTI)], etf_env$VTI[, 4])
col_names <- c("index", "VTI")
colnames(da_ta) <- col_names
# Plot index with VTI
dygraphs::dygraph(da_ta, 
  main="S&P 500 Price-weighted Index and VTI") %>%
  dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
  dySeries(name=col_names[1], axis="y", col="red") %>%
  dySeries(name=col_names[2], axis="y2", col="blue")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/sp500_portfolio_index.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Momentum} Strategy for \protect\emph{S\&P500} Stock Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A very simple \emph{momentum} strategy for the \emph{S\&P500}, is to go long constituents with positive recent performance, and short constituents with negative performance.
      \vskip1ex
      This \emph{momentum} strategy does not perform well and suffers from \emph{momentum crashes} when the market rebounds sharply from a recent lows.
      <<echo=TRUE,eval=FALSE>>=
# Calculate rolling variance of S&P500 portfolio
wid_th <- 252
vari_ance <- roll::roll_var(re_turns, width=wid_th)
vari_ance <- zoo::na.locf(vari_ance)
vari_ance[is.na(vari_ance)] <- 0
# Calculate rolling Sharpe of S&P500 portfolio
returns_width <- rutils::diff_it(log(price_s), lagg=wid_th)
weight_s <- returns_width/sqrt(wid_th*vari_ance)
weight_s[vari_ance==0] <- 0
weight_s[1:wid_th, ] <- 1
weight_s[is.na(weight_s)] <- 0
weight_s <- weight_s/sqrt(rowSums(weight_s^2))
weight_s[is.na(weight_s)] <- 0
weight_s <- rutils::lag_it(weight_s)
sum(is.na(weight_s))
# Calculate portfolio profits and losses
pnl_s <- weight_s*re_turns
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/sp500_momentum.png}
    \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate transaction costs
bid_offer <- 0.001
cost_s <- 0.5*bid_offer*abs(rutils::diff_it(weight_s))
pnl_s <- (pnl_s - cost_s)
pnl_s <- exp(cumsum(pnl_s))
pnl_s <- rowMeans(pnl_s)
pnl_s <- xts(pnl_s, order.by=index(price_s))
pnl_s <- cbind(rutils::etf_env$VTI[, 4], pnl_s)
pnl_s <- na.omit(pnl_s)
colnames(pnl_s) <- c("VTI", "momentum")
col_names <- colnames(pnl_s)
# Plot momentum and VTI
dygraphs::dygraph(pnl_s, main=paste(col_names, collapse=" and ")) %>%
  dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
  dySeries(name=col_names[1], axis="y", col="blue") %>%
  dySeries(name=col_names[2], axis="y2", col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Functional for \protect\emph{S\&P500} Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# Define backtest functional
backtest_rolling <- function(re_turns, price_s, wid_th=252, bid_offer=0.001, tre_nd=1, ...) {
  stopifnot("package:quantmod" %in% search() || require("quantmod", quietly=TRUE))
  # Define look-back and look-forward intervals
  n_cols <- NCOL(re_turns)
  vari_ance <- roll::roll_var(re_turns, width=wid_th)
  vari_ance <- zoo::na.locf(vari_ance)
  vari_ance[is.na(vari_ance)] <- 0
  # Calculate rolling Sharpe of S&P500 portfolio
  returns_width <- rutils::diff_it(log(price_s), lagg=wid_th)
  weight_s <- tre_nd*returns_width/sqrt(wid_th*vari_ance)
  weight_s[vari_ance==0] <- 0
  weight_s[1:wid_th, ] <- 1
  weight_s[is.na(weight_s)] <- 0
  weight_s <- weight_s/sqrt(rowSums(weight_s^2))
  weight_s[is.na(weight_s)] <- 0
  weight_s <- rutils::lag_it(weight_s)
  sum(is.na(weight_s))
  # Calculate portfolio profits and losses
  pnl_s <- weight_s*re_turns
  # Calculate transaction costs
  bid_offer <- 0.001
  cost_s <- 0.5*bid_offer*abs(rutils::diff_it(weight_s))
  pnl_s <- (pnl_s - cost_s)
  pnl_s <- exp(cumsum(pnl_s))
  pnl_s <- rowMeans(pnl_s)
  pnl_s
}  # end backtest_rolling
@
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Multiple \protect\emph{S\&P500} \protect\emph{Momentum} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{S\&P500} \emph{momentum} strategies can be backtested by calling the function \texttt{backtest\_rolling()} in a loop over a vector of \emph{width} parameters.
      \vskip1ex
      The \emph{momentum} strategies do not perform well, especially the ones with a small \emph{width} parameter.
      <<echo=TRUE,eval=FALSE>>=
source("C:/Develop/R/lecture_slides/scripts/back_test.R")
pnl_s <- backtest_rolling(wid_th=252, re_turns=re_turns, 
  price_s=price_s, bid_offer=bid_offer)
width_s <- seq(50, 300, by=50)
# Perform sapply loop over lamb_das
pro_files <- sapply(width_s, backtest_rolling, re_turns=re_turns, 
  price_s=price_s, bid_offer=bid_offer)
colnames(pro_files) <- paste0("width=", width_s)
pro_files <- xts(pro_files, index(price_s))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/sp500_momentum_mult.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- 
  colorRampPalette(c("blue", "red"))(NCOL(pro_files))
chart_Series(pro_files, 
  theme=plot_theme, name="Cumulative Returns of S&P500 Momentum Strategies")
legend("bottomleft", legend=colnames(pro_files), 
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(re_turns)), 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Multiple \protect\emph{S\&P500} \protect\emph{Mean reverting} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{S\&P500} \emph{mean reverting} strategies can be backtested by calling the function \texttt{backtest\_rolling()} in a loop over a vector of \emph{width} parameters.
      \vskip1ex
      The \emph{mean reverting} strategies for the \emph{S\&P500} constituents perform the best for short \emph{width} parameters.
      <<echo=TRUE,eval=FALSE>>=
width_s <- seq(5, 50, by=5)
# Perform sapply loop over lamb_das
pro_files <- sapply(width_s, backtest_rolling, re_turns=re_turns, 
  price_s=price_s, bid_offer=bid_offer, tre_nd=(-1))
colnames(pro_files) <- paste0("width=", width_s)
pro_files <- xts(pro_files, index(price_s))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/sp500_revert_mult.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- 
  colorRampPalette(c("blue", "red"))(NCOL(pro_files))
chart_Series(pro_files, 
  theme=plot_theme, name="Cumulative Returns of S&P500 Mean reverting Strategies")
legend("topleft", legend=colnames(pro_files), 
  inset=0.05, bg="white", cex=0.7, lwd=rep(6, NCOL(re_turns)), 
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization Strategy for \protect\emph{S\&P500}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a rolling portfolio optimization strategy the portfolio weights are adjusted to their optimal values at every end point. 
      \vskip1ex
      A portfolio optimization is performed using past data, and the optimal portfolio weights are applied out-of-sample in the next interval. 
      \vskip1ex
      The weights are scaled to match the volatility of the equally weighted portfolio, and are kept constant until the next end point.
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
load("C:/Develop/R/lecture_slides/data/sp500_prices.RData")
n_cols <- NCOL(re_turns) ; date_s <- index(re_turns)
# Calculate returns on equal weight portfolio
in_dex <- xts(cumsum(re_turns %*% rep(1/n_cols, n_cols)), index(re_turns))
# Define monthly end points
end_points <- rutils::calc_endpoints(re_turns, inter_val="months")
end_points <- end_points[end_points > (n_cols+1)]
n_rows <- NROW(end_points) ; look_back <- 12
start_points <- c(rep_len(1, look_back-1), end_points[1:(n_rows-look_back+1)])
# Perform backtest
al_pha <- 0.01 ; max_eigen <- 3
pnl_s <- HighFreq::back_test(ex_cess=re_turns, 
                             re_turns=re_turns,
                             start_points=start_points-1,
                             end_points=end_points-1,
                             al_pha=al_pha,
                             max_eigen=max_eigen)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/backtest_sharpe_monthly.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot strategy in log scale
pnl_s <- cumsum(pnl_s)
pnl_s <- cbind(pnl_s, in_dex, (pnl_s+in_dex)/2)
col_names <- c("Strategy", "Index", "Average")
colnames(pnl_s) <- col_names
dygraphs::dygraph(pnl_s[end_points], main="Rolling Portfolio Optimization Strategy") %>%
  dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
  dySeries(name=col_names[1], axis="y", col="red", strokeWidth=1) %>%
  dySeries(name=col_names[2], axis="y2", col="blue", strokeWidth=1) %>%
  dySeries(name=col_names[3], axis="y2", col="green", strokeWidth=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Backtesting Framework with Overlapping Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An interval aggregation can be specified by a vector of look-back \emph{intervals} attached at \emph{end points} spanning fixed time \emph{intervals}, 
      \vskip1ex
      For example, we may wish to perform aggregations at weekly \emph{end points}, over overlapping 40-week look-back \emph{intervals}, 
      \vskip1ex
      The variable \texttt{look\_back} is equal to the number of end points in the look-back \emph{interval}, while (\texttt{look\_back - 1}) is equal to the number of intervals in the look-back, 
      \vskip1ex
      The \emph{startpoints} are the \emph{end points} lagged by the number of interval intervals (number of intervals in the interval), 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Define time interval for end points
re_balance <- "weeks"
# look-back interval is multiple of re_balance
look_back <- 41
# Create index of rebalancing period end points
end_points <- xts::endpoints(rutils::etf_env$re_turns, 
                             on=re_balance)
# start_points are multi-period lag of end_points
n_rows <- NROW(end_points)
start_points <- c(rep_len(1, look_back-1), 
  end_points[1:(n_rows-look_back+1)])
# Create list of look-back intervals
look_backs <- lapply(2:n_rows, 
    function(it_er) {
      start_points[it_er]:end_points[it_er]
  })  # end lapply
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Performing Overlapping Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An interval aggregation can be specified by a vector of look-back \emph{intervals} attached at \emph{end points} spanning fixed time \emph{intervals}, 
      \vskip1ex
      For example, we may wish to perform aggregations at weekly \emph{end points}, over overlapping 40-week look-back \emph{intervals}, 
      \vskip1ex
      The variable \texttt{look\_back} is equal to the number of end points in the look-back \emph{interval}, while (\texttt{look\_back-1}) is equal to the number of intervals in that interval, 
      \vskip1ex
      The \emph{startpoints} are the \emph{end points} lagged by the number of interval intervals (number of intervals in the interval), 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Create vector of symbols for model
sym_bols <- c("VTI", "IEF", "DBC")

# Calculate risk&ret stats for some symbols, over a range of dates
# Perform lapply() loop over look_backs
risk_stats <- lapply(look_backs, 
  function(look_back) {
    x_ts <- 
      rutils::etf_env$re_turns[look_back, sym_bols]
    t(sapply(x_ts, 
      function(col_umn)
        c(return=mean(col_umn), risk=mad(col_umn))
      ))  # end sapply
    })  # end lapply
# rbind list into single xts or matrix
# risk_stats <- rutils::do_call_rbind(risk_stats)
# head(risk_stats)
# Calculate non-overlapping returns in interval
re_turns <-sapply(2:n_rows, 
    function(it_er) {
    sapply(rutils::etf_env$re_turns[
      (end_points[it_er-1]+1):end_points[it_er], 
      sym_bols], sum)
  })  # end sapply
re_turns <- t(re_turns)
      @
  \end{columns}
\end{block}

\end{frame}


\end{document}
