% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='scriptsize', fig.width=4, fig.height=4)
options(digits=3)
options(width=60, dev='pdf')
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% tikz package for plotting and tables
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{array}
\usepackage{multirow}
% mathtools package for math symbols
\usepackage{mathtools}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \usepackage{bookmark}
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[Probability and Statistics]{Probability and Statistics}
\subtitle{FRE6871 \& FRE7241, Fall 2019}
% \subject{Getting Started With R}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \author[Jerzy Pawlowski]{Jerzy Pawlowski \texorpdfstring{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}
% \email{jp3900@poly.edu}
% \date{January 27, 2014}
\date{\today}
% \pgfdeclareimage[height=0.5cm]{university-logo}{engineering_long_white}
% \logo{\pgfuseimage{engineering_long_white}}


%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Random Numbers and Estimators}


%%%%%%%%%%%%%%%
\subsection{Pseudo-Random Numbers}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Pseudo-random numbers are deterministic sequences of numbers which have some of the properties of random numbers, but they are not truly random numbers,
      \vskip1ex
      Pseudo-random number generators depend on a \emph{seed} value, and produce the same sequence of numbers for a given \emph{seed} value,
      \vskip1ex
      The function \texttt{set.seed()} initializes the random number generator by specifying the \emph{seed} value,
      \vskip1ex
      The choice of \emph{seed} value isn't important, and a given value is just good as any other one,
      \vskip1ex
      The function \texttt{runif()} produces random numbers from the \emph{uniform} distribution,
      \vskip1ex
      The function \texttt{rnorm()} produces random numbers from the \emph{normal} distribution,
      \vskip1ex
      The function \texttt{dnorm()} calculates the normal probability density,
      \vskip1ex
      The function \texttt{pnorm()} calculates the cumulative \emph{normal} distribution,
      \vskip1ex
      The function \texttt{qnorm()} calculates the inverse cumulative \emph{normal} distribution,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # Reset random number generator
runif(3)  # three numbers from uniform distribution
runif(3)  # Produce another three numbers
set.seed(1121)  # Reset random number generator
runif(3)  # Produce another three numbers

# Produce random number from standard normal distribution
rnorm(1)
# Produce five random numbers from standard normal distribution
rnorm(5)
# Produce five random numbers from the normal distribution
rnorm(n=5, mean=1, sd=2)  # Match arguments by name
# Calculate cumulative standard normal distribution
c(pnorm(-2), pnorm(2))
# Calculate inverse cumulative standard normal distribution
c(qnorm(0.75), qnorm(0.25))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Logistic Map}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{logistic map} is a recurrence relation which produces a deterministic sequence of numbers:
      \begin{displaymath}
        x_{n+1} = r x_n (1 - x_n)
      \end{displaymath}
      If the \emph{seed} value $x_0$ is in the interval $(0, 1)$ and if $r=4$, then the sequence $x_n$ is also contained in the interval $(0, 1)$,
      \vskip1ex
      The function \texttt{curve()} plots a function defined by its name,
      <<echo=TRUE,eval=FALSE>>=
# Define logistic map function
log_map <- function(x, r=4) r*x*(1-x)
log_map(0.25, 4)
# Plot logistic map
x11(width=6, height=5)
curve(expr=log_map, type="l", xlim=c(0, 1),
      xlab="x[n-1]", ylab="x[n]", lwd=2, col="blue",
      main="logistic map")
lines(x=c(0, 0.25), y=c(0.75, 0.75), lwd=2, col="orange")
lines(x=c(0.25, 0.25), y=c(0, 0.75), lwd=2, col="orange")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/logistic_map.png}\\
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generating Pseudo-Random Numbers Using Logistic Map}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{logistic map} can be used to calculate sequences of pseudo-random numbers,
      \vskip1ex
      For most \emph{seed} values $x_0$ and $r=4$, the \emph{logistic map} produces a pseudo-random sequence, but it's not uniformly distributed,
      \vskip1ex
      The inverse cosine function \texttt{acos()} transforms a \emph{logistic map} sequence into a uniformly distributed sequence,
      \begin{displaymath}
        u_n = \arccos(1 - 2 x_n) / \pi
      \end{displaymath}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate uniformly distributed pseudo-random
# Sequence using logistic map function
uni_form <- function(see_d, n_rows=10) {
  # Pre-allocate vector instead of "growing" it
  out_put <- numeric(n_rows)
  # initialize
  out_put[1] <- see_d
  # Perform loop
  for (i in 2:n_rows) {
    out_put[i] <- 4*out_put[i-1]*(1-out_put[i-1])
  }  # end for
  acos(1-2*out_put)/pi
}  # end uni_form
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/logistic_map_density.png}\\
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
uni_form(see_d=0.1, n_rows=15)
plot(
  density(uni_form(see_d=runif(1), n_rows=1e5)),
  xlab="", ylab="", lwd=2, col="blue",
  main="uniform pseudo-random number density")
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generating Binomial Random Numbers}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      A \emph{binomial} trial is a coin flip, that results in either a success or failure,
      \vskip1ex
      The \emph{binomial} distribution specifies the probability of obtaining a certain number of successes in a sequence of independent \emph{binomial} trials,
      \vskip1ex
      Let $p$ be the probability of obtaining a success in a \emph{binomial} trial, and let $(1-p)$ be the probability of failure,
      \vskip1ex
      $p = 0.5$ corresponds to flipping an unbiased coin,
      \vskip1ex
      The probability of obtaining $k$ successes in $n$ independent \emph{binomial} trials is equal to:
      \begin{displaymath}
        {n \choose k} p^k (1-p)^{(n-k)}
      \end{displaymath}
      The function \texttt{rbinom()} produces random numbers from the \emph{binomial} distribution,
    \column{0.6\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Flip unbiased coin once, 20 times
rbinom(n=20, size=1, 0.5)
# Number of heads after flipping twice, 20 times
rbinom(n=20, size=2, 0.5)
# Number of heads after flipping thrice, 20 times
rbinom(n=20, size=3, 0.5)
# Number of heads after flipping biased coin thrice, 20 times
rbinom(n=20, size=3, 0.8)
# Number of heads after flipping biased coin thrice, 20 times
rbinom(n=20, size=3, 0.2)
# Flip unbiased coin once, 20 times
sample(x=0:1, size=20, replace=TRUE)  # Fast
as.numeric(runif(20) < 0.5)  # Slower
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generating Random Samples and Permutations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{sample} is a subset of elements taken from a set of data elements,
      \vskip1ex
      The function \texttt{sample()} produces a random sample form a vector of data elements,
      \vskip1ex
      By default the \emph{size} of the sample (the \texttt{size} argument) is equal to the number of elements in the data vector,
      \vskip1ex
      So the call \texttt{sample(da\_ta)} produces a random permutation of all the elements of \texttt{da\_ta},
      \vskip1ex
      If \texttt{replace=TRUE}, then \texttt{sample()} produces samples with replacement,
      \vskip1ex
      \emph{Monte Carlo} simulation consists of generating random samples from a given probability distribution,
      \vskip1ex
      The \emph{Monte Carlo} data samples can then used to calculate different parameters of the probability distribution (moments, quantiles, etc.), and its functionals,
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Permutation of five numbers
sample(x=5)
# Permutation of four strings
sample(x=c("apple", "grape", "orange", "peach"))
# Sample of size three
sample(x=5, size=3)
# Sample with replacement
sample(x=5, replace=TRUE)
sample(  # Sample of strings
  x=c("apple", "grape", "orange", "peach"),
  size=12,
  replace=TRUE)
# Binomial sample: flip coin once, 20 times
sample(x=0:1, size=20, replace=TRUE)
# Flip unbiased coin once, 20 times
as.numeric(runif(20) > 0.5)  # Slower
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Statistical Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A data \emph{sample} is a set of observations of a \emph{random variable},
      \vskip1ex
      Let $\{x_1,\ldots ,x_n\}$ be a data \emph{sample} of a \emph{random variable} \texttt{x},
      \vskip1ex
      Let \texttt{x} follow a probability distribution with population \emph{mean} equal to $\mu$ and population \emph{standard deviation} equal to $\sigma$,
      \vskip1ex
      A \emph{statistic} is a function of a data \emph{sample}:  $f( x_1,\ldots ,x_n )$,
      \vskip1ex
      A \emph{statistic} is itself a \emph{random variable},
      \vskip1ex
      A statistical \emph{estimator} is a \emph{statistic} that provides an estimate of a \emph{distribution} parameter,
      \vskip1ex
      For example:
      \begin{displaymath}
        \bar{x} = \frac{1}{n}{\sum_{i=1}^n x_i}
      \end{displaymath}
      Is an \emph{estimator} of the \emph{mean} of the \emph{distribution},
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:2))>>=
rm(list=ls())
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
da_ta <- rnorm(1000)

mean(da_ta)  # Sample mean

median(da_ta)  # Sample median

sd(da_ta)  # Sample standard deviation
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimators of Higher Moments}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimators of moments of a probability distribution, based on a \emph{sample} of data, are given by:
      \vskip1ex
      Sample mean: $\bar{x}=\frac{1}{n} \sum_{i=1}^n x_i$
      \vskip1ex
      Sample variance: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$
      \vskip1ex
      With their expected values equal to the population mean and standard deviation:\\
      $\mathbb{E}[\bar{x}] = \mu$ \hskip0.5em and \hskip0.5em $\mathbb{E}[\hat\sigma] = \sigma\mathbb{E}[\hat\sigma] = \sigma\mathbb{E}[\hat\sigma] = \sigma\mathbb{E}[\hat\sigma] = \sigma\mathbb{E}[\hat\sigma] = \sigma\mathbb{E}[\hat\sigma] = \sigma\mathbb{E}[\hat\sigma] = \sigma\mathbb{E}[\hat\sigma] = \sigma$
      \vskip1ex
      The sample skewness (third moment) is equal to:
      \begin{displaymath}
        \hat{s}=\frac{n}{(n-1)(n-2)} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^3
      \end{displaymath}
      The sample kurtosis (fourth moment) is equal to
      \begin{displaymath}
        \hat{k}=\frac{n}{(n-1)^2} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^4
      \end{displaymath}
      The normal distribution has zero skewness and kurtosis equal to 3.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-(1:1)),eval=FALSE>>=
rm(list=ls())
# DAX returns
re_turns <- diff(log(EuStockMarkets[, 1]))
# Number of observations
n_rows <- NROW(re_turns)
# Mean of DAX returns
mea_n <- mean(re_turns)
# Standard deviation of DAX returns
s_d <- sd(re_turns)
# Normalize returns
re_turns <- (re_turns - mea_n)/s_d
# Skewness of DAX returns
skew(re_turns)
# Or
n_rows/((n_rows-1)*(n_rows-2))*sum(re_turns^3)
# Kurtosis of DAX returns
kurt(re_turns)
# Or
n_rows/(n_rows-1)^2*sum(re_turns^4)
# Random normal returns
re_turns <- rnorm(n_rows, sd=2)
# Mean and standard deviation of random normal returns
mean(re_turns); sd(re_turns)
# Skewness and kurtosis of random normal returns
skew(re_turns); kurt(re_turns)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimators of Quantiles}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{quantile} of a probability distribution is the value of the \emph{random variable} \texttt{x}, such that the probability of values less than \texttt{x} is equal to the given \emph{probability} $p$,
      \vskip1ex
      The \emph{quantile} of a data sample can be calculated by first sorting the sample, and then finding the value corresponding closest to the given \emph{probability} $p$,
      \vskip1ex
      The function \texttt{quantile()} calculates the sample quantiles, but it's quite slow,
      \vskip1ex
      The function \texttt{sort()} returns a vector sorted into ascending order,
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1000
da_ta <- rnorm(n_rows)
# Sample mean - MC estimate
mean(da_ta)
# Sample standard deviation - MC estimate
sd(da_ta)
# Monte Carlo estimate of cumulative probability
da_ta <- sort(da_ta)
pnorm(1)
sum(da_ta<1)/n_rows
# Monte Carlo estimate of quantile
conf_level <- 0.99
qnorm(conf_level)
cut_off <- conf_level*n_rows
da_ta[cut_off]
quantile(da_ta, probs=conf_level)
# Analyze the source code of quantile()
stats:::quantile.default
# microbenchmark quantile
library(microbenchmark)
summary(microbenchmark(
  monte_carlo=da_ta[cut_off],
  quan_tile=quantile(da_ta, probs=conf_level),
  times=100))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Statistical estimators are functions of samples (which are random variables), and therefore are themselves \emph{random variables},
      \vskip1ex
      The \emph{standard error} (SE) of an estimator is defined as its \emph{standard deviation} (not to be confused with the \emph{population standard deviation} of the underlying random variable),
      \vskip1ex
      For example, the \emph{standard error} of the estimator of the mean is equal to:
      \begin{displaymath}
        \sigma_\mu = \frac{\sigma}{\sqrt{n}}
      \end{displaymath}
      Where $\sigma$ is the \emph{population standard deviation} (which is usually unkown),
      \vskip1ex
      The \emph{estimator} of this \emph{standard error} is equal to:
      \begin{displaymath}
        SE_{\mu} = \frac{\hat\sigma}{\sqrt{n}}
      \end{displaymath}
      where: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$ is the sample standard deviation (the estimator of the population standard deviation),
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1000
da_ta <- rnorm(n_rows)
# Sample mean
mean(da_ta)
# Sample standard deviation
sd(da_ta)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Probability Distributions}


%%%%%%%%%%%%%%%
\subsection{The Characteristic Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{characteristic function} $\hat{f}(t)$ is equal to the \emph{Fourier transform} of the \emph{probability density function} $f(x)$:
      \begin{displaymath}
        \hat{f}(t) = \mathbb{E}[e^{i t x}] = \int_{-\infty}^{\infty} {f(x) \, e^{i t x} \, \mathrm{d}x}
      \end{displaymath}
      The \emph{normal} probability density function:
      \begin{displaymath}
        \phi(x) = \frac{e^{-(x-\mu)^2/2\sigma^2}}{\sigma\sqrt{2 \pi}}
      \end{displaymath}
      Has the \emph{characteristic function} $\hat\phi(t)$ equal to:
      \begin{displaymath}
        \hat\phi(t) = e^{i \mu t} e^{-(\sigma t)^2/2}
      \end{displaymath}
      The \emph{probability function} $f(x)$ is equal to the \emph{inverse Fourier transform} of the \emph{characteristic function} $\hat{f}(t)$:
      \begin{displaymath}
        f(x) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} {\hat{f}(t) \, e^{- i t x} \, \mathrm{d}t}
      \end{displaymath}
      
    \column{0.5\textwidth}
      The \emph{characteristic function} of the first derivative of $f(x)$ is equal to $(-i t) \hat{f}(t)$, the \emph{characteristic function} multiplied by $(-i t)$:
      \begin{displaymath}
        \frac{d f(x)}{d x} = \frac{1}{2 \pi} \int_{-\infty}^{\infty} { -i t \, \hat{f}(t) \, e^{- i t x} \, \mathrm{d}t}
      \end{displaymath}
      The \emph{characteristic function} of the \emph{n}-th derivative of $f(x)$ is equal to $(-i t)^n \hat{f}(t)$, the \emph{characteristic function} multiplied by $(-i t)^n$:
      \begin{displaymath}
        \frac{d^n f(x)}{d x^n} = \frac{1}{2 \pi} \int_{-\infty}^{\infty} { (-i t)^n \, \hat{f}(t) \, e^{- i t x} \, \mathrm{d}t}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Moment Generating Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{moment generating function} $M_X(t)$ of a random variable \texttt{x} with the probability density function $f(x)$ is equal to:
      \begin{displaymath}
        M_X(t) = \mathbb{E}[e^{t x}] = \int_{-\infty}^{\infty} {f(x) \, e^{t x} \, \mathrm{d}x}
      \end{displaymath}
      The \emph{n}-th derivative of $M_X(t)$ with respect to \texttt{t}, at $t = 0$ is equal to the \emph{n}-th \emph{moment} $\mu_n$:
      \begin{align*}
        \mu_n &= \frac{d^n M_X(t)}{d t^n} |_{t = 0} = \frac{d^n \mathbb{E}[e^{t x}]}{d t^n} |_{t = 0} &\\
        &= \mathbb{E}[x^n e^{t x}] |_{t = 0} = \mathbb{E}[x^n] &
      \end{align*}
      The \emph{moments} $\mu_n$ are related to the \emph{central moments} $\mathbb{E}[(x - \mu)^n]$ but they are not equal to them.
    \column{0.5\textwidth}
      The \emph{moment generating function} can be expressed as a series of its \emph{moments}:
      \begin{displaymath}
        M_X(t) = \sum_{n=0}^{\infty} {\frac{\mu_n t^n}{n!}}
      \end{displaymath}
      The moment generating function for the \emph{normal} distribution is equal to:
      \begin{displaymath}
        M_X(t) = \exp(\mu t + \frac{1}{2} {\sigma^2 t^2})
      \end{displaymath}
      The \emph{characteristic function} $\hat{f}(t)$ is equal to the \emph{moment generating function} with a purely \emph{imaginary} argument:
      \begin{displaymath}
        \hat{f}(t) = \mathbb{E}[e^{i t x}] = M_X(it)
        \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Cumulants of Probability Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{cumulant generating function} $K_X(t)$ is equal to the logarithm of the \emph{moment generating function}: 
      \begin{displaymath}
        K_X(t) = \log{M_X(t)}
      \end{displaymath}
      The \emph{n}-th derivative of $K_X(t)$ with respect to \texttt{t}, at $t = 0$ is equal to the \emph{n}-th \emph{cumulant} $\kappa_n$:
      \begin{displaymath}
        \kappa_n = \frac{d^n K_X(t)}{d t^n} |_{t = 0}
      \end{displaymath}
      The \emph{cumulants} are related to the \emph{moments} of a distribution:
      with the first three cumulants being equal to the \emph{central moments} (mean, variance, and skewness), while the higher order \emph{cumulants} are polynomials of the \emph{moments}.
      \vskip1ex
      The \emph{cumulant generating function} $K_X(t)$ can be expanded into a power series of the \emph{cumulants}: 
      \begin{align*}
        K_X(t) = \sum_{n=1}^{\infty} {\frac{\kappa_n t^n}{n!}} = \mu t + \sigma^2 \frac{t^2}{2} + \sigma^3 s^3 \frac{t^3}{6} + \dots
      \end{align*}
      
    \column{0.5\textwidth}
      The cumulant generating function for the \emph{normal} distribution is equal to:
      \begin{displaymath}
        K_X(t) = \mu t + \frac{1}{2} {\sigma^2 t^2}
      \end{displaymath}
      So that its first two \emph{cumulants} are equal to the \emph{mean} $\mu$ and the \emph{variance} $\sigma^2$, and the \emph{cumulants} of order \texttt{3} and higher are all equal to zero.
      \vskip1ex
      The advantage of \emph{cumulants} over the \emph{moments} is that the \emph{cumulants} of the sum of independent random variables are equal to the sum of their \emph{cumulants}:
      \begin{flalign*}
        K_{(X+Y)}(t) = \log{\mathbb{E}[e^{t (X+Y)}]} = \log{(\mathbb{E}[e^{t X}] \mathbb{E}[e^{t Y}])} \\
        = \log{\mathbb{E}[e^{t X}]} \log{\mathbb{E}[e^{t Y}]} = K_X(t) + K_Y(t)
      \end{flalign*}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Hermite Polynomials}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{n}-th derivative of the \emph{standard normal} distribution $\phi(x)$ is given by Rodrigues' formula:
      \begin{displaymath}
          \frac{d^n \phi(x)}{d x^n} = \frac{d^n}{d x^n} \frac{e^{-x^2/2}}{\sqrt{2 \pi}} = (-1)^n H_n(x) \phi(x)
      \end{displaymath}
      Where $H_n$ are the \emph{Hermite polynomials}.
      \vskip1ex
      The first four \emph{Hermite polynomials} are equal to:
      \begin{align*}
        H_0(x) &= 1 ; H_1(x) = x \\
        H_2(x) &= x^2 - 1 ; H_3(x) = x^3 - 3x
      \end{align*}
      The even order polynomials are \emph{symmetric} $H_{2n}(-x) = H_{2n}(x)$ while the odd order are \emph{antisymmetric} $H_{2n-1}(-x) = -H_{2n-1}(x)$.
      <<echo=TRUE,eval=FALSE>>=
# Define Hermite polynomials
her_mite <- function(x, n) {
    switch(n+1, 1, x, (x^2 - 1), (x^3 - 3*x), 0)
}  # end her_mite
      @
    \column{0.5\textwidth}
      \vspace{-1em}
    \includegraphics[width=0.5\paperwidth]{figure/hermite_poly.png}
      \vspace{-2em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)  # Plot in window
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
col_ors <- c("red", "blue", "green")
for (in_dex in 1:3) {  # Plot three curves
  curve(expr=her_mite(x, in_dex),
        xlim=c(-3, 3), ylim=c(-2.5, 2.5),
        xlab="", ylab="", lwd=4, col=col_ors[in_dex],
        add=as.logical(in_dex-1))
}  # end for
# Add title and legend
title(main="Hermite Polynomials", line=0.5)
lab_els <- paste("Order", 1:3, sep=" = ")
legend("top", inset=0.0, bty="n",
       title=NULL, lab_els, cex=0.8, lwd=6, lty=1, 
       col=col_ors)
      @
    
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Hermite Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Hermite functions} $\psi_n(x)$ are equal to:
      \begin{displaymath}
        \psi_n(x) = \frac{1}{\sqrt{n! \sqrt{2 \pi}}} e^{-x^2/4} H_n(x)
      \end{displaymath}
      The \emph{Hermite functions} form an orthonormal set:
      \begin{displaymath}
        \int_{-\infty}^{\infty} {\psi_n(x) \, \psi_m(x) \, \mathrm{d}x} =
        \begin{cases}
          1 & \text{if } n = m\\
          0 & \text{if } n \neq m
        \end{cases}
      \end{displaymath}
      The \emph{Hermite functions} of increasing order oscillate more frequently, with the function of order $n$ crossing zero $n$ times.
      <<echo=TRUE,eval=FALSE>>=
# Define Hermite functions
hermite_fun <- function(x, n) 
  exp(-x^2/4)*her_mite(x, n)/(2*pi)^(0.25)/sqrt(factorial(n))
# Integrate Hermite functions
integrate(function(x, n, m) 
  hermite_fun(x, n)*hermite_fun(x, m),
  lower=(-Inf), upper=Inf, n=2, m=3)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
    \includegraphics[width=0.5\paperwidth]{figure/hermite_func.png}
      \vspace{-2em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)  # Plot in window
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
col_ors <- c("red", "blue", "green")
for (in_dex in 1:3) {  # Plot three curves
  curve(expr=hermite_fun(x, in_dex),
        xlim=c(-6, 6), ylim=c(-0.6, 0.6),
        xlab="", ylab="", lwd=4, col=col_ors[in_dex],
        add=as.logical(in_dex-1))
}  # end for
# Add title and legend
title(main="Hermite Functions", line=0.5)
lab_els <- paste("Order", 1:3, sep=" = ")
legend("topright", inset=0.0, bty="n",
       title=NULL, lab_els, cex=0.8, lwd=6, lty=1, 
       col=col_ors)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Hermite Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
% wippp
      To-do: expand a probability distribution into a series of \emph{Hermite functions}.
      \vskip1ex
      The \emph{Hermite functions} $\psi_i(x)$ form an orthonormal basis that can be used to expand a given probability distribution $f(x)$ into a series:
      \begin{displaymath}
        f(x) = \sum_{i=0}^n {f_i \, \psi_i(x)}
      \end{displaymath}
      The coefficients $f_i$ are equal to:
      \begin{displaymath}
        f_i = \int_{-\infty}^{\infty} {f(x) \, \psi_i(x) \, \mathrm{d}x}
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
    \includegraphics[width=0.5\paperwidth]{figure/hermite_poly.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Integrate Hermite functions
integrate(her_mite, lower=(-Inf), upper=Inf, n=2)
integrate(function(x, n, m) her_mite(x, n)*her_mite(x, m), 
          lower=(-Inf), upper=Inf, n=2, m=3)
integrate(function(x, n, m) her_mite(x, n)*her_mite(x, m), 
          lower=(-Inf), upper=Inf, n=2, m=2)
      @
      \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)  # Plot in window
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
col_ors <- c("red", "blue", "green")
for (in_dex in 1:3) {  # Plot three curves
  curve(expr=her_mite(x, in_dex),
        xlim=c(-4, 4), ylim=c(-0.6, 0.6),
        xlab="", ylab="", lwd=3, col=col_ors,
        add=as.logical(in_dex-1))
}  # end for
# Add title and legend
title(main="Hermite Functions", line=0.5)
lab_els <- paste("Order", 1:3, sep=" = ")
legend("topright", inset=0.05, bty="n",
       title=NULL, lab_els, cex=0.8, lwd=6, lty=1, 
       col=col_ors)
      @
    
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Bell polynomials}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Bell polynomials} $B_n$ are the coefficients in the expansion of the exponent of a series:
      \begin{displaymath}
        \exp( \sum_{n=1}^{\infty} { \kappa_n \frac{t^n}{n!} } ) = \sum_{n=0}^{\infty} { B_n(\kappa_1, \ldots, \kappa_n) \frac{t^n}{n!} }
      \end{displaymath}
      The first four \emph{Bell polynomials} are equal to:
      \begin{align*}
        B_0 &= 1 \\
        B_1(\kappa_1) &= \kappa_1 \\
        B_2(\kappa_1, \kappa_2) &= \kappa_1^2 + \kappa_2 \\
        B_3(\kappa_1, \kappa_2, \kappa_3) &= \kappa_1^3 + 3 \kappa_1 \kappa_2 + \kappa_3 \\
      \end{align*}

    \column{0.5\textwidth}

  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Gram-Charlier Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Gram-Charlier series} expresses the \emph{density function} $f(x)$ in terms of its \emph{cumulants} and a \emph{basis function} $\phi(x)$, with \emph{characteristic} $\hat{\phi}(t)$.
      \vskip1ex
      The \emph{characteristic functions} $\hat{f}(t)$ and $\hat{\phi}(t)$ can be expressed in terms of their \emph{cumulants} as:
      \begin{align*}
        \hat{f}(t) &= e^{K_X(i t)} = \exp(\sum_{n=1}^{\infty} {\kappa_n \frac{(i t)^n}{n!}}) \\
        \hat{\phi}(t) &= \exp(\sum_{n=1}^{\infty} {\phi_n \frac{(i t)^n}{n!}})
      \end{align*}
      Then $\hat{f}(t)$ can be expressed in terms of $\hat{\phi}(t)$ as:
      \begin{displaymath}
        \hat{f}(t) = \exp[\sum_{n=1}^{\infty} {(\kappa_n - \phi_n) \frac{(i t)^n}{n!}}] \, \hat{\phi}(t)
      \end{displaymath}
      The \emph{basis function} $\phi(x)$ can be chosen to be a \emph{normal distribution}, with \emph{mean} and \emph{standard deviation} equal to that of $f(x)$, and with all its \emph{normal cumulants} $\phi_n$ of order \texttt{3} and higher equal to zero.
      
    \column{0.5\textwidth}
      Then we get a series starting at \texttt{n=3}:
      \begin{displaymath}
        \hat{f}(t) = \exp[\sum_{n=3}^{\infty} {\kappa_n \frac{(i t)^n}{n!}}] \, \hat{\phi}(t)
      \end{displaymath}
      We can expand the exponent and collect terms with the same power of $t$ using the \emph{Bell polynomials} $B_n$:
      \begin{displaymath}
        \hat{f}(t) = \sum_{n=0}^{\infty} { B_n(0, 0, \kappa_3, \ldots, \kappa_n) \, \frac{(i t)^n}{n!} \, \hat{\phi}(t) }
      \end{displaymath}
      The \emph{inverse Fourier transform} of the above equation gives the \emph{probability function} $f(x)$:
      \begin{displaymath}
        f(x) = \sum_{n=0}^{\infty} { B_n(0, 0, \kappa_3, \ldots, \kappa_n) \frac{(-1)^n}{n!} \frac{d^n \phi(x)}{d x^n} }
      \end{displaymath}
      The derivatives of the \emph{normal} distribution $\phi(x)$ can be expressed using the \emph{Hermite polynomials} $H_n$ so that the \emph{Gram-Charlier series} becomes:
      \begin{displaymath}
        f(x) = \phi(x) \sum_{n=0}^{\infty} { \frac{B_n(0, 0, \kappa_3, \ldots, \kappa_n)}{n! \sigma^n} H_n(\frac{x-\mu}{\sigma}) }
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Edgeworth Expansion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
% wippp
      The \emph{Edgeworth expansion} expresses the \emph{probability density function} $f(x)$ as a series of its \emph{cumulants} and a \emph{basis function} $\phi(x)$ (with \emph{characteristic function} $\hat{\phi}(t)$).
      \vskip1ex
      The \emph{characteristic functions} $\hat{f}(t)$ and $\hat{\phi}(t)$ can be expressed in terms of their corresponding \emph{cumulants} as:
      \begin{align*}
        \hat{f}(t) &= e^{K_X(i t)} = \exp(\sum_{n=1}^{\infty} {\kappa_n \frac{(i t)^n}{n!}}) \\
        \hat{\phi}(t) &= \exp(\sum_{n=1}^{\infty} {\phi_n \frac{(i t)^n}{n!}})
      \end{align*}
      Then $\hat{f}(t)$ can be expressed in terms of $\hat{\phi}(t)$ as:
      \begin{displaymath}
        \hat{f}(t) = \exp[\sum_{n=1}^{\infty} {(\kappa_n - \phi_n) \frac{(i t)^n}{n!}}] \, \hat{\phi}(t)
      \end{displaymath}
      
    \column{0.5\textwidth}
      If the \emph{basis function} $\phi(x)$ is chosen to be the \emph{normal distribution}, with \emph{mean} and \emph{standard deviation} equal to that of $f(x)$, and since the \emph{normal cumulants} of order \texttt{3} and higher are all equal to zero, then we get:
      \begin{displaymath}
        \hat{f}(t) = \exp[\sum_{n=3}^{\infty} {\kappa_n (-1)^n \frac{(i t)^n}{n!}}] \, \hat{\phi}(t)
      \end{displaymath}
      The \emph{inverse Fourier transform} of the above equation gives the \emph{probability function} $f(x)$:
      \begin{displaymath}
        f(x) = \exp[\sum_{n=3}^{\infty} {\kappa_n (-1)^n \frac{d^n}{d x^n}}] \, \phi(x)
      \end{displaymath}
      Now expand the exponent in a series and collect terms with the same order of the derivative to obtain:
      \begin{displaymath}
        f(x) = \sum_{n=0}^{\infty} { {B_n(0, 0, \kappa_3, \ldots, \kappa_n) (-1)^n \frac{d^n \phi(x)}{d x^n}} }
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


% wippp
%%%%%%%%%%%%%%%
\subsection{draft: The Cornish-Fisher Expansion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Cornish-Fisher expansion} expresses the quantiles of a distribution as a series of its \emph{moments}.
      \vskip1ex
      The Edgeworth Expansion
      The \emph{cumulant generating function} $K_X(t)$ is equal to the logarithm of the \emph{moment generating function}: 
      \begin{displaymath}
        K_X(t) = \log{M_X(t)}
      \end{displaymath}
      The \emph{n}-th derivative of $K_X(t)$ with respect to \texttt{t}, at $t = 0$ is equal to the \emph{n}-th \emph{cumulant} $\kappa_n$:
      \begin{displaymath}
        \kappa_n = \frac{d^n K_X(t)}{d t^n} |_{t = 0}
      \end{displaymath}
      The \emph{cumulants} are related to the \emph{moments} of the distribution:
      the first three cumulants are equal to the \emph{central moments} (mean, variance, and skewness), while the higher order \emph{cumulants} can be expressed as polynomials of the \emph{central moments}.
      \vskip1ex
      The \emph{cumulant generating function} $K_X(t)$ can be expanded into a power series of the \emph{cumulants}: 
      \begin{displaymath}
        K_X(t) = \sum_{n=1}^n {\frac{\kappa_n t^n}{n!}} = \mu t + \sigma^2 t^2 / 2 + 
      \end{displaymath}

      The \emph{n}-th \emph{moment} $\mu_n$ is not equal to the \emph{central moment} $\mathbb{E}[(x - \mu)^n]$.
      
      The \emph{moment generating function} $M_X(t)$ of a random variable \texttt{x} with the probability function $f(x)$ is equal to:
      \begin{displaymath}
        M_X(t) = \mathbb{E}[e^{t x}] = \int_{-\infty}^{\infty} {f(x) \, e^{t x} \, \mathrm{d}x}
      \end{displaymath}
      
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1000
da_ta <- rnorm(n_rows)
# Sample mean
mean(da_ta)
# Sample standard deviation
sd(da_ta)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Hypothesis Testing}


%%%%%%%%%%%%%%%
\subsection{Hypothesis Testing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Hypothesis Tests} are designed to test the validity of \emph{null hypotheses}, and they consist of:
      \begin{itemize}
        \item A \emph{null hypothesis},
        \item A test \emph{statistic} derived from the data sample,
        \item A \emph{p}-value: the conditional probability of observing the test statistic value, assuming the \emph{null hypothesis} is \texttt{TRUE},
        \item A \emph{significance level} $\alpha$ corresponding to a \emph{critical value}.
      \end{itemize}
      The \emph{p}-value is compared to the \emph{significance level} and if the \emph{p}-value is less than the \emph{significance level} $\alpha$, then the \emph{null hypothesis} is rejected.
      \vskip1ex
      It's possible for the \emph{null hypothesis} to be \texttt{TRUE}, but to obtain a very small \emph{p}-value purely by chance.
      \vskip1ex
      The \emph{p}-value is the probability of erroneously rejecting a \texttt{TRUE} \emph{null hypothesis}, due to the randomness of the data sample.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
### Perform two-tailed test that sample is
### from Standard Normal Distribution (mean=0, SD=1)
# generate vector of samples and store in data frame
test_frame <- data.frame(samples=rnorm(1e4))
# get p-values for all the samples
test_frame$p_values <- sapply(test_frame$samples,
              function(x) 2*pnorm(-abs(x)))
# Significance level, two-tailed test, critical value=2*SD
signif_level <- 2*(1-pnorm(2))
# Compare p_values to significance level
test_frame$result <-
  test_frame$p_values > signif_level
# Number of null rejections
sum(!test_frame$result) / NROW(test_frame)
# Show null rejections
head(test_frame[!test_frame$result, ])
      @
      The \emph{p}-value is a conditional probability, and is not equal to the un-conditional probability of the hypothesis being \texttt{TRUE}.
      \vskip1ex
      In statistics we cannot \emph{prove} that a hypothesis is \texttt{TRUE} or not, but we can attempt to invalidate it, and conclude that it's unlikely to be \texttt{TRUE}, given the test statistic value and its \emph{p}-value.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Two-tailed Hypothesis Tests}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In two-tailed hypothesis tests, both tails of the probability distribution contribute to the \emph{p}-value.
      \vskip1ex
      Two-tailed hypothesis tests are applied for testing if the absolute value of a sample  exceeds the critical value.
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
# Plot the Normal probability distribution
curve(expr=dnorm(x, sd=1), type="l", xlim=c(-4, 4),
      xlab="", ylab="", lwd=3, col="blue")
title(main="Two-tailed Test", line=0.5)
# Plot tails of the distribution using polygons
star_t <- 2; e_nd <- 4
# Plot right tail using polygon
x_var <- seq(star_t, e_nd, length=100)
y_var <- dnorm(x_var, sd=1)
y_var[1] <- (-1)
y_var[NROW(y_var)] <- (-1)
polygon(x=x_var, y=y_var, col="red")
# Plot left tail using polygon
y_var <- dnorm(-x_var, sd=1)
y_var[1] <- (-1)
y_var[NROW(y_var)] <- (-1)
polygon(x=(-x_var), y=y_var, col="red")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/hyp_test_tow_tail.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Visualizing Hypothesis Testing Using Package \protect\emph{ggplot2}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In two-tailed hypothesis tests, both tails of the probability distribution contribute to the \emph{p}-value.
      <<hyp_test_ggp2,echo=(-(1:2)),eval=FALSE,fig.show='hide'>>=
rm(list=ls())
par(oma=c(1, 1, 1, 1), mgp=c(2, 0.5, 0), mar=c(5, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
library(ggplot2)  # Load ggplot2

qplot(  # Simple ggplot2
    main="Standard Normal Distribution",
    c(-4, 4),
    stat="function",
    fun=dnorm,
    geom="line",
    xlab=NULL, ylab=NULL
    ) +  # end qplot

theme(  # Modify plot theme
    plot.title=element_text(vjust=-1.0),
    plot.background=element_blank()
    ) +  # end theme

geom_vline(  # Add vertical line
  aes(xintercept=c(-2.0, 2.0)),
  colour="red",
  linetype="dashed"
  )  # end geom_vline
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/hyp_test_ggp2-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Visualizing Hypothesis Testing Using \protect\emph{ggplot2} (cont.)}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In two-tailed hypothesis tests, both tails of the probability distribution contribute to the \emph{p}-value.
      <<hyp_test_ggp2_2,echo=(-(1:2)),eval=FALSE,fig.show='hide'>>=
rm(list=ls())
par(oma=c(1, 1, 1, 1), mgp=c(2, 0.5, 0), mar=c(5, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
### Create ggplot2 with shaded area
x_var <- -400:400/100
norm_frame <- data.frame(x_var=x_var,
                       d.norm=dnorm(x_var))
norm_frame$shade <- ifelse(
                  abs(norm_frame$x_var) >= 2,
                  norm_frame$d.norm, NA)
ggplot(  # Main function
  data=norm_frame,
  mapping=aes(x=x_var, y=d.norm)
  ) +  # end ggplot
# Plot line
  geom_line() +
# Plot shaded area
  geom_ribbon(aes(ymin=0, ymax=shade), fill="red") +
# No axis labels
  xlab("") + ylab("") +
# Add title
  ggtitle("Standard Normal Distribution") +
# Modify plot theme
  theme(
        plot.title=element_text(vjust=-1.0),
        plot.background=element_blank()
  )  # end theme
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/hyp_test_ggp2_2-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Student's t-test} for the Distribution Mean}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Student's t-test} is designed to test the \emph{null hypothesis} that a sample: $\{x_1, \ldots , x_n\}$ was obtained from a normal distribution with a \emph{mean} equal to $\mu$.
      \vskip1ex
      The test statistic is equal to the \emph{t-ratio}:
      \begin{displaymath}
        t = \frac{\bar{x} - \mu}{\hat\sigma / \sqrt{n}}
      \end{displaymath}
      Where $\bar{x}=\frac{1}{n} \sum_{i=1}^n x_i$ is the sample mean and $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$ is the sample variance.
      \vskip1ex
      Under the \emph{null hypothesis} the \emph{t-ratio} follows the \emph{t-distribution} with $n$ degrees of freedom, with the probability density function:
      \begin{displaymath}
        f(x) = \frac{\Gamma((n+1)/2)}{\sqrt{\pi n} \, \Gamma(n/2)} \, (1 + x^2/n)^{-(n+1)/2}
      \end{displaymath}
      \emph{Student's t-test} can also be used to test if two different normally distributed samples have equal \emph{population means}.
      \vskip1ex
      \emph{Student's t-test} is not valid for random variables that do not follow the normal distribution.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/t_dist_norm.png}\\
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# t-test for single sample
t.test(rnorm(100))
# t-test for two samples
t.test(rnorm(100),
       rnorm(100, mean=1))
# Plot the normal and t-distribution densities
x11(width=6, height=5)
par(mar=c(3, 3, 3, 1), oma=c(0, 0, 0, 0))
curve(expr=dnorm, xlim=c(-4, 4),
      xlab="", ylab="", lwd=3)
curve(expr=dt(x, df=3),
      xlab="", ylab="", lwd=3,
      col="red", add=TRUE)
# Add title
title(main="Normal and t-distribution densities", line=0.5)
# Add legend
legend("topright", inset=0.05, bty="n",
       title=NULL, c("normal", "t-dist"),
       cex=0.8, lwd=6, lty=1,
       col=c("black", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Analysis of Variance (\protect\emph{ANOVA})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Analysis of Variance (\emph{ANOVA}) to test if the sub-samples of the data have the same mean.
      \vskip1ex
      ANOVA provides a statistical test of whether two or more population means are equal, and therefore generalizes the t-test beyond two means.
      \emph{Student's t-test}
      is designed to test the \emph{null hypothesis} that a sample: $\{x_1, \ldots , x_n\}$ was obtained from a normal distribution with a \emph{mean} equal to $\mu$.
      \vskip1ex
      For example, \emph{ANOVA} is widely used to study the effect of medical treatments, with the \emph{null hypothesis} being that the treatments have no effect and that differences are due to random chance.
      \vskip1ex
      The test statistic is equal to the \emph{t-ratio}:
      \begin{displaymath}
        t = \frac{\bar{x} - \mu}{\hat\sigma / \sqrt{n}}
      \end{displaymath}
      Where $\bar{x}=\frac{1}{n} \sum_{i=1}^n x_i$ is the sample mean and $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$ is the sample variance,
      \vskip1ex
      Under the \emph{null hypothesis} the \emph{t-ratio} follows the \emph{t-distribution} with $n$ degrees of freedom, with the probability density function:
      \begin{displaymath}
        f(x) = \frac{\Gamma((n+1)/2)}{\sqrt{\pi n} \, \Gamma(n/2)} \, (1 + x^2/n)^{-(n+1)/2}
      \end{displaymath}
      \emph{Student's t-test} can also be used to test if two different normally distributed samples have equal \emph{population means},
      \vskip1ex
      \emph{Student's t-test} is not valid for random variables that do not follow the normal distribution.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/t_dist_norm.png}\\
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# t-test for single sample
t.test(rnorm(100))
# t-test for two samples
t.test(rnorm(100),
       rnorm(100, mean=1))
# Plot the normal and t-distribution densities
x11(width=6, height=5)
par(mar=c(3, 3, 3, 1), oma=c(0, 0, 0, 0))
curve(expr=dnorm, xlim=c(-4, 4),
      xlab="", ylab="", lwd=3)
curve(expr=dt(x, df=3),
      xlab="", ylab="", lwd=3,
      col="red", add=TRUE)
# Add title
title(main="Normal and t-distribution densities", line=0.5)
# Add legend
legend("topright", inset=0.05, bty="n",
       title=NULL, c("normal", "t-dist"),
       cex=0.8, lwd=6, lty=1,
       col=c("black", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Kolmogorov-Smirnov} Test for Probability Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kolmogorov-Smirnov} test is designed to test the \emph{null hypothesis} that two samples: $\{x_1, \ldots , x_n\}$ and $\{y_1, \ldots , y_n\}$ were obtained from the same probability distribution.
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} statistic is the maximum difference between two empirical cumulative distribution functions (cumulative frequencies):
      \begin{displaymath}
        D = \sup_i | P(x_i) - P(y_i) |
      \end{displaymath}
      The function \texttt{ks.test()} calculates the \emph{Kolmogorov-Smirnov} statistic and its \emph{p}-value.
      \vskip1ex
      The second argument is either a \texttt{numeric} vector of data values, or a name of a cumulative distribution function.
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} test can be used as a \emph{goodness of fit} test, to test if a set of observations fits a given probability distribution.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# KS-test for normal distribution
ks.test(rnorm(100), pnorm)
# KS-test for uniform distribution
ks.test(runif(100), pnorm)
# KS-test for two similar normal distributions
ks.test(rnorm(100), rnorm(100, mean=0.1))
# KS-test for two different normal distributions
ks.test(rnorm(100), rnorm(100, mean=1.0))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Shapiro-Wilk} Test of Normality}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Shapiro-Wilk} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1,\ldots ,x_n\}$ is from a normally distributed population.
      \vskip1ex
      The test statistic is equal to:
      \begin{displaymath}
        W = \frac {(\sum_{i=1}^n a_i x_{(i)})^2} {\sum_{i=1}^n (x_i-\bar{x})^2}
      \end{displaymath}
      Where the: $\{a_1,\ldots ,a_n\}$ are proportional to the \emph{order statistics} of random variables from the normal distribution.
      \vskip1ex
      $x_{(k)}$ is the \emph{k}-th \emph{order statistic}, and is equal to the \emph{k}-th smallest value in the sample: $\{x_1,\ldots ,x_n\}$.
      \vskip1ex
      The \emph{Shapiro-Wilk} statistic follows its own distribution, and is less than or equal to $1$.
      \vskip1ex
      The \emph{Shapiro-Wilk} statistic is close to one for samples from normal distributions.
      \vskip1ex
      The \emph{p}-value for DAX returns is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and the DAX returns are not from a normally distributed population.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
# Calculate DAX percentage returns
dax_rets <- diff(log(EuStockMarkets[, 1]))
# Shapiro-Wilk test for normal distribution
shapiro.test(rnorm(NROW(dax_rets)))
# Shapiro-Wilk test for DAX returns
shapiro.test(dax_rets)
# Shapiro-Wilk test for uniform distribution
shapiro.test(runif(NROW(dax_rets)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Jarque-Bera} Test of Normality}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Jarque-Bera} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1,\ldots ,x_n\}$ is from a normally distributed population.
      \vskip1ex
      The test statistic is equal to:
      \begin{displaymath}
        JB= \frac{n}{6} (\hat{s}^2 + \frac{1}{4} (\hat{k} - 3)^2)
      \end{displaymath}
      Where the skewness and kurtosis are defined as:
      \begin{align*}
        \hat{s} = \frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^3
      &&
        \hat{k} = \frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^4
      \end{align*}
      The \emph{Jarque-Bera} statistic asymptotically follows the \emph{chi-squared} distribution with two degrees of freedom.
      \vskip1ex
      The \emph{Jarque-Bera} statistic is small for samples from normal distributions.
      \vskip1ex
      The \emph{p}-value for DAX returns is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and the DAX returns are not from a normally distributed population.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
library(tseries)  # Load package tseries
# Jarque-Bera test for normal distribution
jarque.bera.test(rnorm(NROW(dax_rets)))
# Jarque-Bera test for DAX returns
jarque.bera.test(dax_rets)
# Jarque-Bera test for uniform distribution
jarque.bera.test(runif(NROW(dax_rets)))
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Robust Estimators}


%%%%%%%%%%%%%%%
\subsection{Sorting and Ranking Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{sort()} returns a vector sorted into ascending order, from smallest to largest.
      \vskip1ex
      A permutation is a re-ordering of the elements of a vector.
      \vskip1ex
      The permutation index specifies how the elements are re-ordered in a permutation.
      \vskip1ex
      The function \texttt{order()} calculates the permutation index to sort a given vector into ascending order.
      \vskip1ex
      Applying the function \texttt{order()} twice: \texttt{order(order())}, calculates the permutation index to sort the vector from ascending order into its original unsorted order.
      \vskip1ex
      The permutation index produced by: \texttt{order(order())} is the reverse of the permutation index produced by: \texttt{order()}.
      \vskip1ex
      The function \texttt{rank()} calculates the ranks of the elements, according to their magnitude, from smallest to largest.
      \vskip1ex
      The ranks of the elements are equal to the reverse permutation index.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
# Sort a vector into ascending order
da_ta <- round(runif(7), 2)
sort_ed <- sort(da_ta)
da_ta  # original data
sort_ed  # sorted data
# Calculate index to sort into ascending order
in_dex <- order(da_ta)
in_dex  # permutation index to sort
all.equal(sort_ed, da_ta[in_dex])
# Sort the ordered vector back to its original unsorted order
in_dex <- order(order(da_ta))
in_dex  # permutation index to unsort
all.equal(da_ta, sort_ed[in_dex])
# Calculate ranks of the vector elements
rank(da_ta)
all.equal(rank(da_ta), in_dex)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Mean and Median Estimators of Location}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For symmetric distributions, the expected values of the sample \emph{mean} and the \emph{median} are equal to each other.
      \vskip1ex
      For normally distributed data, the \emph{mean} has a smaller standard error than the median.
      \vskip1ex
      But for distributions with very large kurtosis (fat tails), the \emph{median} may have a smaller standard error than the \emph{mean}, because it's less sensitive to outliers.
      \vskip1ex
      The \emph{median} is always well defined, even for data that has infinite variance.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
n_rows <- 1e3
da_ta <- rnorm(n_rows)
mean(da_ta)
median(da_ta)

# wipp
# Bootstrap of sd and mad estimators
boot_data <- sapply(1:10000, function(x) {
  boot_sample <-
    da_ta[sample.int(n_rows, replace=TRUE)]
  c(sd=sd(boot_sample), mad=mad(boot_sample))
})  # end sapply
boot_data <- t(boot_data)
# Analyze bootstrapped variance
head(boot_data)
sum(is.na(boot_data))
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2,
      function(x) c(mean=mean(x), std_error=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # initialize compute cluster
boot_data <- parLapply(clus_ter, 1:10000,
  function(x, da_ta) {
    boot_sample <-
      da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(boot_sample), mad=mad(boot_sample))
  }, da_ta=da_ta)  # end parLapply
# Parallel bootstrap under Mac-OSX or Linux
boot_data <- mclapply(1:10000,
  function(x) {
    boot_sample <-
      da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(boot_sample), mad=mad(boot_sample))
  }, mc.cores=n_cores)  # end mclapply
stopCluster(clus_ter)  # Stop R processes over cluster
boot_data <- rutils::do_call(rbind, boot_data)
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2,
      function(x) c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Robust Estimators of Location}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability), defined using the \emph{median} instead of the mean:
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(x_i - \operatorname{median}(\mathbf{x})))
      \end{displaymath}
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance.
      \vskip1ex
      Explain breakdown point of estimators
      \href{https://en.wikipedia.org/wiki/Robust_statistics}{breakdown point of estimators}

      \vskip1ex

      For normally distributed data, the \emph{MAD} has a larger standard error than the standard deviation.
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}.
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$.
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
n_rows <- 1e3
da_ta <- rnorm(n_rows)
sd(da_ta)
mad(da_ta)
median(abs(da_ta - median(da_ta)))
median(abs(da_ta - median(da_ta)))/qnorm(0.75)
# Bootstrap of sd and mad estimators
boot_data <- sapply(1:10000, function(x) {
  boot_sample <-
    da_ta[sample.int(n_rows, replace=TRUE)]
  c(sd=sd(boot_sample), mad=mad(boot_sample))
})  # end sapply
boot_data <- t(boot_data)
# Analyze bootstrapped variance
head(boot_data)
sum(is.na(boot_data))
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2,
      function(x) c(mean=mean(x), std_error=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # initialize compute cluster
boot_data <- parLapply(clus_ter, 1:10000,
  function(x, da_ta) {
    boot_sample <-
      da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(boot_sample), mad=mad(boot_sample))
  }, da_ta=da_ta)  # end parLapply
# Parallel bootstrap under Mac-OSX or Linux
boot_data <- mclapply(1:10000,
  function(x) {
    boot_sample <-
      da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(boot_sample), mad=mad(boot_sample))
  }, mc.cores=n_cores)  # end mclapply
stopCluster(clus_ter)  # Stop R processes over cluster
boot_data <- rutils::do_call(rbind, boot_data)
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2,
      function(x) c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The \protect\emph{Hodges-Lehmann} Estimator of Location}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability), defined using the \emph{median} instead of the mean:
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(x_i - \operatorname{median}(\mathbf{x})))
      \end{displaymath}
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance.
      \vskip1ex
      Explain breakdown point of estimators
      \href{https://en.wikipedia.org/wiki/Robust_statistics}{breakdown point of estimators}

      \vskip1ex

      For normally distributed data, the \emph{MAD} has a larger standard error than the standard deviation.
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}.
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$.
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
bar <- wilcox.test(foo, conf.int=TRUE)
bar$estimate


n_rows <- 1e3
da_ta <- rnorm(n_rows)
sd(da_ta)
mad(da_ta)
median(abs(da_ta - median(da_ta)))
median(abs(da_ta - median(da_ta)))/qnorm(0.75)
# Bootstrap of sd and mad estimators
boot_data <- sapply(1:10000, function(x) {
  boot_sample <-
    da_ta[sample.int(n_rows, replace=TRUE)]
  c(sd=sd(boot_sample), mad=mad(boot_sample))
})  # end sapply
boot_data <- t(boot_data)
# Analyze bootstrapped variance
head(boot_data)
sum(is.na(boot_data))
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2,
      function(x) c(mean=mean(x), std_error=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # initialize compute cluster
boot_data <- parLapply(clus_ter, 1:10000,
  function(x, da_ta) {
    boot_sample <-
      da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(boot_sample), mad=mad(boot_sample))
  }, da_ta=da_ta)  # end parLapply
# Parallel bootstrap under Mac-OSX or Linux
boot_data <- mclapply(1:10000,
  function(x) {
    boot_sample <-
      da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(boot_sample), mad=mad(boot_sample))
  }, mc.cores=n_cores)  # end mclapply
stopCluster(clus_ter)  # Stop R processes over cluster
boot_data <- rutils::do_call(rbind, boot_data)
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2,
      function(x) c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Robust Estimators and Influence Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The influence function measures the sensitivity of an estimator to changes in the values of individual data points.
      \vskip1ex
      But for distributions with very large kurtosis (fat tails), the \emph{median} may have a smaller standard error than the mean, because it's less sensitive to outliers.
      \vskip1ex
      Statistical estimators are functions of samples (which are random variables), and therefore are themselves \emph{random variables}.
      \vskip1ex
      The \emph{standard error} (SE) of an estimator is defined as its \emph{standard deviation} (not to be confused with the \emph{population standard deviation} of the underlying random variable).
      \vskip1ex
      For example, the \emph{standard error} of the estimator of the mean is equal to:
      \begin{displaymath}
        \sigma_{\mu} = \frac{\sigma}{\sqrt{n}}
      \end{displaymath}
      Where $\sigma$ is the \emph{population standard deviation} (which is usually unkown).
      \vskip1ex
      The \emph{estimator} of this \emph{standard error} is equal to:
      \begin{displaymath}
        SE_{\mu} = \frac{\hat\sigma}{\sqrt{n}}
      \end{displaymath}
      where: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$ is the sample standard deviation (the estimator of the population standard deviation).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1e3
da_ta <- rnorm(n_rows)
# Sample mean
mean(da_ta)
# Sample standard deviation
sd(da_ta)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Efficiency and Bias of Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      bias-variance tradeoff
      Statistical estimators are functions of samples (which are random variables), and therefore are themselves \emph{random variables}.
      \vskip1ex
      The \emph{standard error} (SE) of an estimator is defined as its \emph{standard deviation} (not to be confused with the \emph{population standard deviation} of the underlying random variable).
      \vskip1ex
      For example, the \emph{standard error} of the estimator of the mean is equal to:
      \begin{displaymath}
        \sigma_{\mu} = \frac{\sigma}{\sqrt{n}}
      \end{displaymath}
      Where $\sigma$ is the \emph{population standard deviation} (which is usually unkown).
      \vskip1ex
      The \emph{estimator} of this \emph{standard error} is equal to:
      \begin{displaymath}
        SE_{\mu} = \frac{\hat\sigma}{\sqrt{n}}
      \end{displaymath}
      where: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$ is the sample standard deviation (the estimator of the population standard deviation).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1e3
da_ta <- rnorm(n_rows)
# Sample mean
mean(da_ta)
# Sample standard deviation
sd(da_ta)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Wilcoxon} Test for the Distribution Mean}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Wilcoxon} test is designed to test the \emph{null hypothesis} that two samples: $\{x_1, \ldots , x_n\}$ and $\{y_1, \ldots , y_n\}$ were obtained from two probability distributions with equal \emph{population means}.
      \vskip1ex
      The function \texttt{wilcox.test()} calculates the \emph{Wilcoxon} statistic and its \emph{p}-value.
      \vskip1ex
      If a single sample is passed into \texttt{wilcox.test()} then it tests if the data was produced by a probability distribution with zero mean.
      \vskip1ex
      For many distributions, the \emph{Wilcoxon} test has greater \emph{sensitivity} than the \emph{Student's t-test}.
      \vskip1ex
      The \emph{sensitivity} of a statistical test is the ability to correctly identify \emph{true positive} cases (when the null hypothesis is \texttt{FALSE}).
      \vskip1ex
      The \emph{specificity} of a statistical test is the ability to correctly identify \emph{true negative} cases (when the null hypothesis is \texttt{TRUE}).
      \vskip1ex
      The \emph{Wilcoxon} test is also more \emph{robust} with respect to data outliers, i.e. it reports fewer \emph{false positive} cases when there are outliers in the data.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Wilcoxon test for normal distribution
wilcox.test(rnorm(100))
# Wilcoxon test for two normal distributions
sample1 <- rnorm(100)
sample2 <- rnorm(100, mean=0.1)
wilcox.test(sample1, sample2)$p.value
t.test(sample1, sample2)$p.value
# Wilcoxon test with data outliers
sample2 <- sample1
sample2[1:11] <- sample2[1:11] + 5
wilcox.test(sample1, sample2)$p.value
t.test(sample1, sample2)$p.value
# Wilcoxon test for two normal distributions
wilcox.test(rnorm(100), rnorm(100, mean=1.0))
# Wilcoxon test for a uniform versus normal distribution
wilcox.test(runif(100)-0.5, rnorm(100))
# Wilcoxon test for a uniform versus normal distribution
wilcox.test(runif(100), rnorm(100))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{W} Statistic of the \protect\emph{Wilcoxon} Test}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Wilcoxon} test statistic \emph{W} is equal to the sum of the ranks $r_i = \operatorname{rank}(|x_i - y_i|)$ of the absolute differences weighted by their signs:
      \begin{displaymath}
        W = \sum_{i=1}^n \operatorname{sgn}(x_i - y_i) r_i
      \end{displaymath}
      The statistic \emph{W} follows a distribution without a simple formula, which converges to the normal distribution for large sample size $n$, with an expected value equal to $0$ and a variance equal to $\frac{n(n+1)(2n+1)}{6}$.
      \vskip1ex
      The \emph{Wilcoxon} test is \emph{nonparametric} because it doesn't assume any type of sample distribution, unlike the \emph{Student's t-test} which assumes that the sample is taken from the \emph{normal} distribution.
      \vskip1ex
      The \emph{Wilcoxon} test is more \emph{robust} with respect to data outliers because it only depends on the ranks of the sample differences $(x_i - y_i)$, not the differences themselves.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Wilcoxon test for random data around 0
n_rows <- 1e3
da_ta <- (runif(n_rows) - 0.5)
wil_cox <- wilcox.test(da_ta)
# Calculate V statistic of Wilcoxon test
wil_cox$statistic
sum(rank(abs(da_ta))[da_ta>0])
# Calculate W statistic of Wilcoxon test
sum(sign(da_ta)*rank(abs(da_ta)))
# Calculate distributon of Wilcoxon W statistic
wilcox_w <- sapply(1:1e3, function(x) {
  da_ta <- (runif(n_rows) - 0.5)
  sum(sign(da_ta)*rank(abs(da_ta)))
})  # end sapply
wilcox_w <- wilcox_w/sqrt(n_rows*(n_rows+1)*(2*n_rows+1)/6)
var(wilcox_w)
hist(wilcox_w)
      @
      \vspace{-1em}
      The function \texttt{wilcox.test()} returns the \emph{V} statistic, not the the \emph{W} statistic:       \begin{displaymath}
        V = \sum_{i=1}^n \operatorname{H}(x_i - y_i) r_i
      \end{displaymath}
      Where $\operatorname{H}(x) = 1$ if $x > 0$, and $0$ otherwise.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Time Series Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Bootstrapping the regression of asset returns shows that the actual standard errors can be over twice as large as those reported by the function \texttt{lm()}.
      \vskip1ex
      This is because the function \texttt{lm()} assumes that the data is normally distributed, while in reality asset returns have very large skewness and kurtosis.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load time series of ETF percentage returns
re_turns <- rutils::etf_env$re_turns[, c("XLF", "XLE")]
re_turns <- na.omit(re_turns)
n_rows <- NROW(re_turns)
head(re_turns)
# Define regression formula
for_mula <- paste(colnames(re_turns)[1],
  paste(colnames(re_turns)[-1], collapse="+"),
  sep=" ~ ")
# Standard regression
mod_el <- lm(for_mula, data=re_turns)
model_sum <- summary(mod_el)
# Bootstrap of regression
set.seed(1121)  # initialize random number generator
boot_data <- sapply(1:100, function(x) {
  boot_sample <- sample.int(n_rows, replace=TRUE)
  mod_el <- lm(for_mula,
               data=re_turns[boot_sample, ])
  mod_el$coefficients
})  # end sapply
# Means and standard errors from regression
model_sum$coefficients
# Means and standard errors from bootstrap
dim(boot_data)
t(apply(boot_data, MARGIN=1,
      function(x) c(mean=mean(x), std_error=sd(x))))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Kruskal-Wallis} Test for the Distribution Similarity}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kruskal-Wallis} test is designed to test the \emph{null hypothesis} that sub-samples of the data corresponding to different categories follow similar distributions.
      \vskip1ex
      The \emph{Kruskal-Wallis} test can be used as a type of \emph{nonparametric ANOVA} test, to test if the sub-samples of the data have the same mean.
      \vskip1ex
      For example, given the heights of several different species of trees, the \emph{Kruskal-Wallis} test can test if all the species have the same height.
      \vskip1ex
      The \emph{Kruskal-Wallis} test can also test if samples have different \emph{skewness}, even if they have the same \emph{means}.
      \vskip1ex
      The function \texttt{kruskal.test()} accepts a vector of sample data and a factor specifying the categories, and calculates the \emph{Kruskal-Wallis} statistic and its \emph{p}-value.
      \vskip1ex
      The function \texttt{kruskal.test()} can also accept the data as a formula combined with a matrix or data frame.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# iris data frame
aggregate(Sepal.Length ~ Species, data=iris,
          FUN=function(x) c(mean=mean(x), sd=sd(x)))
# Kruskal-Wallis test for iris data
k_test <- kruskal.test(Sepal.Length ~ Species, data=iris)
str(k_test)
k_test$statistic
# Kruskal-Wallis test for independent normal distributions
sample1 <- rnorm(1e3)
sample2 <- rnorm(1e3)
fac_tor <- c(rep(TRUE, 1e3), rep(FALSE, 1e3))
kruskal.test(x=c(sample1, sample2), g=fac_tor)
# Kruskal-Wallis test for shifted normal distributions
kruskal.test(x=c(sample1+1, sample2), g=fac_tor)
# Kruskal-Wallis test for beta distributions
sample1 <- rbeta(1e3, 2, 8) + 0.3
sample2 <- rbeta(1e3, 8, 2) - 0.3
mean(sample1); mean(sample2)
kruskal.test(x=c(sample1, sample2), g=fac_tor)
# Plot the beta distributions
x11()
plot(density(sample1), col="blue", lwd=3,
     xlim=range(c(sample1, sample2)), xlab="samples",
     main="Two samples from beta distributions with equal means")
lines(density(sample2), col="red", lwd=3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Kruskal-Wallis} Test Statistic}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Given a data sample $x_i$ with $n$ elements, and a factor of $k$ categories, the sample can be divided into $k$ sub-samples $x^j_i$.
      \vskip1ex
      Let $r_i = \operatorname{rank}(x_i)$ be the ranks of the sample, and $r^j_i$ be the ranks of the sub-samples.
      \vskip1ex
      The \emph{Kruskal-Wallis} test statistic \emph{H} is proportional to the sum of squared differences between the average rank of the sample $\bar{r} = \frac{n+1}{2}$, minus the average ranks of the sub-samples $\bar{r}_j$:
      \begin{displaymath}
        H = \frac{12}{n(n+1)} \sum_{j=1}^k (\frac{n+1}{2} - \bar{r}_j)^2 n_j
      \end{displaymath}
      Where the sum is over all the $k$ categories, and $n_j$ is the number of elements in sub-sample $j$.
      \vskip1ex
      The \emph{H} statistic follows a distribution without a simple formula, which is approximately equal to the \emph{chi-squared} distribution with $k-1$ degrees of freedom.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Kruskal-Wallis test for iris data
k_test <- kruskal.test(Sepal.Length ~ Species, data=iris)
# Calculate Kruskal-Wallis test Statistic
n_rows <- NROW(iris)
iris_data <- data.frame(rank_s=rank(iris$Sepal.Length),
                        spe_cies=iris$Species)
kruskal_stat <- (12/n_rows/(n_rows+1))*sum(
  aggregate(rank_s ~ spe_cies,
            data=iris_data,
            FUN=function(x) {
              NROW(x)*((n_rows+1)/2 - mean(x))^2
            })[, 2])
c(k_test=unname(k_test$statistic),
  k_stat=kruskal_stat)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Kruskal-Wallis} Test with Data Outliers}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kruskal-Wallis} test is \emph{nonparametric} because it doesn't assume any type of sample distribution.
      \vskip1ex
      The \emph{Kruskal-Wallis} test is also \emph{robust} with respect to data outliers, since it only depends on the ranks of the sample.
      \vskip1ex
      When a few data outliers are added to the data, \emph{Student's t-test} rejects the \emph{null hypothesis} that the means are equal, but the \emph{Kruskal-Wallis} test still accepts the \emph{null hypothesis} that the distributions are similar.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Kruskal-Wallis test with data outliers
sample1 <- rnorm(1e3)
sample2 <- rnorm(1e3)
sample2[1:11] <- sample2[1:11] + 50
fac_tor <- c(rep(TRUE, 1e3), rep(FALSE, 1e3))
kruskal.test(x=c(sample1, sample2), g=fac_tor)$p.value
t.test(sample1, sample2)$p.value
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Robust Estimators of Dispersion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability), defined using the \emph{median} instead of the mean:
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(x_i - \operatorname{median}(\mathbf{x})))
      \end{displaymath}
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance.
      \vskip1ex
      For normally distributed data, the \emph{MAD} has a larger standard error than the standard deviation.
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}.
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$.
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
n_rows <- 1e3
da_ta <- rnorm(n_rows)
sd(da_ta)
mad(da_ta)
median(abs(da_ta - median(da_ta)))
median(abs(da_ta - median(da_ta)))/qnorm(0.75)
# Bootstrap of sd and mad estimators
boot_data <- sapply(1:10000, function(x) {
  boot_sample <-
    da_ta[sample.int(n_rows, replace=TRUE)]
  c(sd=sd(boot_sample), mad=mad(boot_sample))
})  # end sapply
boot_data <- t(boot_data)
# Analyze bootstrapped variance
head(boot_data)
sum(is.na(boot_data))
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2,
      function(x) c(mean=mean(x), std_error=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # initialize compute cluster
boot_data <- parLapply(clus_ter, 1:10000,
  function(x, da_ta) {
    boot_sample <-
      da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(boot_sample), mad=mad(boot_sample))
  }, da_ta=da_ta)  # end parLapply
# Parallel bootstrap under Mac-OSX or Linux
boot_data <- mclapply(1:10000,
  function(x) {
    boot_sample <-
      da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(boot_sample), mad=mad(boot_sample))
  }, mc.cores=n_cores)  # end mclapply
stopCluster(clus_ter)  # Stop R processes over cluster
boot_data <- rutils::do_call(rbind, boot_data)
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2,
      function(x) c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Robust Estimators of Skewness}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability), defined using the \emph{median} instead of the mean:
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(x_i - \operatorname{median}(\mathbf{x})))
      \end{displaymath}
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance.
      \vskip1ex
      For normally distributed data, the \emph{MAD} has a larger standard error than the standard deviation.
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}.
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$.
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
n_rows <- 1e3
da_ta <- rnorm(n_rows)
sd(da_ta)
mad(da_ta)
median(abs(da_ta - median(da_ta)))
median(abs(da_ta - median(da_ta)))/qnorm(0.75)
# Bootstrap of sd and mad estimators
boot_data <- sapply(1:10000, function(x) {
  boot_sample <-
    da_ta[sample.int(n_rows, replace=TRUE)]
  c(sd=sd(boot_sample), mad=mad(boot_sample))
})  # end sapply
boot_data <- t(boot_data)
# Analyze bootstrapped variance
head(boot_data)
sum(is.na(boot_data))
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2,
      function(x) c(mean=mean(x), std_error=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # initialize compute cluster
boot_data <- parLapply(clus_ter, 1:10000,
  function(x, da_ta) {
    boot_sample <-
      da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(boot_sample), mad=mad(boot_sample))
  }, da_ta=da_ta)  # end parLapply
# Parallel bootstrap under Mac-OSX or Linux
boot_data <- mclapply(1:10000,
  function(x) {
    boot_sample <-
      da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(boot_sample), mad=mad(boot_sample))
  }, mc.cores=n_cores)  # end mclapply
stopCluster(clus_ter)  # Stop R processes over cluster
boot_data <- rutils::do_call(rbind, boot_data)
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2,
      function(x) c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Pearson} Correlation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{covariance} $\sigma_{xy}$ between two sets of data, $x_i$ and $y_i$, is defined as:
      \begin{displaymath}
        \sigma_{xy} = \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{n-1}
      \end{displaymath}
      Where $\bar{x} = \frac{1}{n}{\sum_{i=1}^n x_i}$ and $\bar{y} = \frac{1}{n}{\sum_{i=1}^n y_i}$ are the \emph{mean} values.
      \vskip1ex
      The function \texttt{cov()} calculates the covariance between two numeric vectors.
      \vskip1ex
      The \emph{Pearson} correlation $\rho_P$ is equal to the \emph{covariance} divided by the standard deviations $\sigma_x$ and $\sigma_y$:
      \begin{displaymath}
        \rho_P = \frac{\sigma_{xy}}{\sigma_x \sigma_y}
      \end{displaymath}
      The function \texttt{cor()} calculates the correlation between two numeric vectors.
      \vskip1ex
      Depending on the argument \texttt{"method"}, it calculates either the \emph{Pearson} (default), \emph{Spearman}, or \emph{Kendall} correlations.
      \vskip1ex
      The function \texttt{cor.test()} performs a test of the statistical significance of the correlation coefficient.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/cor_scatter_plot.png}
      \vspace{-2em}
        <<echo=(-(1:1)),eval=FALSE>>=
set.seed(1121)  # initialize random number generator
# Define variables and calculate correlation
n_rows <- 100
x_var <- runif(n_rows); y_var <- runif(n_rows)
cor(x_var, y_var)
# Test statistical significance of correlation
cor.test(x_var, y_var)
# Correlate the variables and calculate correlation
rh_o <- 0.5
y_var <- rh_o*x_var + (1-rh_o)*y_var
# Test statistical significance of correlation
cor.test(x_var, y_var)
# Plot in x11 window
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0.5, 0.5, 0, 0))
# Plot scatterplot and exact regression line
plot(x_var, y_var, xlab="x_var", ylab="y_var")
title(main="Correlated Variables", line=0.5)
abline(a=0.25, b=rh_o, lwd=3, col="blue")
# Calculate regression
summary(lm(y_var ~ x_var))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Spearman} Rank Correlation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Spearman} correlation $\rho_S$ is equal to the \emph{Pearson} correlation between the ranks ${rx}_i$ and ${ry}_i$ of the variables $x_i$ and $y_i$:
      \begin{displaymath}
        \rho_S = \frac{\sum_{i=1}^n ({rx}_i - \bar{rx}) ({ry}_i - \bar{ry})}{(n-1) \sigma_{rx} \sigma_{ry}}
      \end{displaymath}
      If the ranks are all distinct integers, then the \emph{Spearman} correlation $\rho_S$ can be expressed as:
      \begin{displaymath}
        \rho_S = 1 - \frac{6 \sum_{i=1}^n {dr}_i^2}{n(n^2-1)}
      \end{displaymath}
      Where ${dr}_i = {rx}_i - {ry}_i$ are the differences between the ranks.
      \vskip1ex
      The \emph{Spearman} correlation is a \emph{robust} measure of association because it depends on the ranks, so it's not sensitive to the extreme values of the variables $x_i$ and $y_i$.
      \vskip1ex
      The \emph{Spearman} correlation is considered a \emph{nonparametric} estimator because it does not depend on the joint probability distribution of the variables $x_i$ and $y_i$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate correlations
cor(x_var, y_var, method="pearson")
cor(x_var, y_var, method="spearman")
# Test statistical significance of correlations
cor.test(x_var, y_var, method="pearson")
cor.test(x_var, y_var, method="spearman")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Kendall's} $\tau$ Correlation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The pair of observations $\left\{ x_i, y_i \right\}$ is concordant with the pair $\left\{ x_j, y_j \right\}$ if the signs of the differences $({rx}_i-{rx}_j)$ and $({ry}_i-{ry}_j)$ are the same, i.e. if their ranks follow the same order.
      \vskip1ex
      The \emph{Kendall} correlation $\tau_K$ (\emph{Kendall's} $\tau$) is equal to the difference between the number of concordant pairs of observations, minus the number of discordant pairs:
      \begin{displaymath}
        \tau_K = \frac{2}{n(n-1)} \sum_{i<j}^n {\operatorname{sgn}({rx}_i-{rx}_j) \operatorname{sgn}({ry}_i-{ry}_j)}
      \end{displaymath}
      The \emph{Kendall} correlation $\tau_K$ is also a \emph{robust} and \emph{nonparametric} estimator of association, because it only depends on the ranks, so it's not sensitive to the extreme values of the variables $x_i$ and $y_i$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate correlations
cor(x_var, y_var, method="pearson")
cor(x_var, y_var, method="kendall")
# Test statistical significance of correlations
cor.test(x_var, y_var, method="pearson")
cor.test(x_var, y_var, method="kendall")
      @
  \end{columns}
\end{block}

\end{frame}


\end{document}
